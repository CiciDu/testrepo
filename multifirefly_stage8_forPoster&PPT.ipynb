{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CiciDu/testrepo/blob/main/multifirefly_stage8_forPoster%26PPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qr9aTanPaUB"
      },
      "source": [
        "In this stage, I decide to speed up some of the codes and improve the codes' readability by eliminating unnecessary cells and adding comments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzM2JV06prsz"
      },
      "source": [
        "## General setups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxu1E8rbUkNa"
      },
      "source": [
        "note: on_off_lines have the maximum distance of 400, while connect_ff_path has the maximum distance of 250"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxDasmGp3e43"
      },
      "outputs": [],
      "source": [
        "# for monkey\n",
        "NEW_DATASET = False\n",
        "MONKEY_DATA = True\n",
        "NO_PLOT_NEEDED = True\n",
        "data_folder_name = \"0219\"\n",
        "data_num = 19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-T4kmuT3Wzq"
      },
      "outputs": [],
      "source": [
        "# for agent\n",
        "NEW_DATASET = True\n",
        "MONKEY_DATA = False\n",
        "NO_PLOT_NEEDED = True\n",
        "data_folder_name = \"LSTM_July_29\"\n",
        "data_num = 721\n",
        "trial_total_num = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHVeDCEw3Jsk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(7777)\n",
        "#rng = np.random.default_rng(2021) \n",
        "NEW_DATASET = True\n",
        "MONKEY_DATA = True\n",
        "SHOW_INTERESTING = False\n",
        "trial_total_num = 20\n",
        "list_of_colors = [\"navy\", \"magenta\", \"white\", \"gray\", \"brown\", \"black\"] # For plotting ff clusters\n",
        "point_index_array = np.arange(1,100,10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3mHeHV2py2i"
      },
      "source": [
        "### Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp_L08hSPCG_"
      },
      "outputs": [],
      "source": [
        "! pip install google.colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHpWYCpxUfGv"
      },
      "source": [
        "### unzip data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "id": "XMwO0LOgH7mc",
        "outputId": "22f40053-14aa-4f3e-964f-58a36e4ddbdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting neo\n",
            "  Downloading neo-0.10.2-py3-none-any.whl (639 kB)\n",
            "\u001b[K     |████████████████████████████████| 639 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting quantities>=0.12.1\n",
            "  Downloading quantities-0.13.0.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 20.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.1 in ./opt/anaconda3/lib/python3.8/site-packages (from neo) (1.20.1)\n",
            "Building wheels for collected packages: quantities\n",
            "  Building wheel for quantities (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Created wheel for quantities: filename=quantities-0.13.0-py3-none-any.whl size=77858 sha256=518040d62252c17712a36d5cfd9f4d4bbfc86a46c2a7f8db97c0a9979fb19a10\n",
            "  Stored in directory: /Users/dusiyi/Library/Caches/pip/wheels/a2/54/36/12af6e58a292a912f4bcd62d146d593fadcd414a258a970cf2\n",
            "Successfully built quantities\n",
            "Installing collected packages: quantities, neo\n",
            "Successfully installed neo-0.10.2 quantities-0.13.0\n",
            "unzip:  cannot find or open gdrive/MyDrive/fireflies_data/0219.zip, gdrive/MyDrive/fireflies_data/0219.zip.zip or gdrive/MyDrive/fireflies_data/0219.zip.ZIP.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-705084060f45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdata_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#seg_reader = neo.io.Spike2IO(filename=\"/content/behavioural_data/m51s26.smr\").read_segment()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mseg_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpike2IO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/0219/m51c0936.smr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_segment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/neo/io/spike2io.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, **kargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mSpike2RawIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mBaseFromRaw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/neo/io/basefromrawio.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mBaseIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     def read_block(self, block_index=0, lazy=False,\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/neo/rawio/baserawio.py\u001b[0m in \u001b[0;36mparse_header\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_stream_signal_channel_characteristics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/neo/rawio/spike2rawio.py\u001b[0m in \u001b[0;36m_parse_header\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# get header info and channel_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_global_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_as_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaderDescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_global_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/0219/m51c0936.smr'"
          ]
        }
      ],
      "source": [
        "!pip install neo\n",
        "import neo\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "#!unzip gdrive/MyDrive/behavioural_data.zip\n",
        "!unzip gdrive/MyDrive/fireflies_data/0219.zip\n",
        "#!unzip gdrive/MyDrive/fireflies_data/0220.zip\n",
        "#!unzip gdrive/MyDrive/fireflies_data/0221.zip\n",
        "data_folder_name = \"0219\"\n",
        "data_num = 19\n",
        "#seg_reader = neo.io.Spike2IO(filename=\"/content/behavioural_data/m51s26.smr\").read_segment()\n",
        "seg_reader = neo.io.Spike2IO(filename=\"/content/0219/m51c0936.smr\").read_segment()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yoGmwF_pxf4"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iqs1g4OroJwW"
      },
      "outputs": [],
      "source": [
        "import os.path\n",
        "from os import path\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "\n",
        "\n",
        "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from matplotlib.colors import ListedColormap\n",
        "from IPython.display import HTML\n",
        "import pandas as pd\n",
        "import math\n",
        "import collections\n",
        "import re\n",
        "import os, sys\n",
        "from contextlib import contextmanager\n",
        "from scipy.signal import decimate\n",
        "from matplotlib import rc, cm\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import torch\n",
        "from math import pi\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "np.set_printoptions(suppress=True)\n",
        "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
        "import seaborn as sns\n",
        "\n",
        "!pip install matplotlib-scalebar\n",
        "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
        "from matplotlib_scalebar.scalebar import ScaleBar\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "from numpy import linalg as LA\n",
        "import csv \n",
        "from scipy.cluster.hierarchy import linkage\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import plotly.express as px\n",
        "from random import randint\n",
        "\n",
        "\n",
        "class HiddenPrints:\n",
        "    def __enter__(self):\n",
        "        self._original_stdout = sys.stdout\n",
        "        sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout.close()\n",
        "        sys.stdout = self._original_stdout\n",
        "\n",
        "\n",
        "from matplotlib.figure import Figure\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import matplotlib.ticker as mtick"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NunM2pwESpPm"
      },
      "source": [
        "### Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXjd1b8noJwd"
      },
      "outputs": [],
      "source": [
        "def plt_config(title=None, xlim=None, ylim=None, xlabel=None, ylabel=None, colorbar=False, sci=False):\n",
        "    for field in ['title', 'xlim', 'ylim', 'xlabel', 'ylabel']:\n",
        "        if eval(field) != None: getattr(plt, field)(eval(field))\n",
        "    if isinstance(sci, str): plt.ticklabel_format(style='sci', axis=sci, scilimits=(0,0))\n",
        "    if isinstance(colorbar,str): plt.colorbar(label=colorbar)\n",
        "    elif colorbar: plt.colorbar(label = '$Number\\ of\\ Entries$')\n",
        "\n",
        "@contextmanager\n",
        "def initiate_plot(dimx=24, dimy=9, dpi=100, fontweight='normal'):\n",
        "    plt.rcParams['figure.figsize'] = (dimx, dimy)\n",
        "    plt.rcParams['font.weight'] = fontweight\n",
        "    plt.rcParams['mathtext.default'] = 'regular'\n",
        "    plt.rcParams[\"font.family\"] = 'Arial'\n",
        "    global fig; fig = plt.figure(dpi=dpi)\n",
        "    yield\n",
        "    plt.show() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6RhvWObqE93"
      },
      "source": [
        "### Animation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "242FrPYS4I-f"
      },
      "outputs": [],
      "source": [
        "rc('animation', html='jshtml')\n",
        "matplotlib.rcParams['animation.embed_limit'] = 2**128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uutNr0RhILiv"
      },
      "source": [
        "### smr_extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePt4bJvwo8ZC"
      },
      "outputs": [],
      "source": [
        "class smr_extractor(object):   \n",
        "    def __init__(self):\n",
        "        self.folder_path = data_folder_name\n",
        "        self.files_names = [file for file in os.listdir(self.folder_path) if 'smr' in file]\n",
        "        self.full_path_file_names = [os.path.join(self.folder_path,self.files_names[i]) \n",
        "                                     for i,value in enumerate(self.files_names)] # a list contains 2 file path in total\n",
        "    def extract_data(self):\n",
        "        Channel_signal_output = []\n",
        "        marker_list = []\n",
        "        \n",
        "        for index, file_name in enumerate(self.full_path_file_names):  # loop 2 files one by one\n",
        "            seg_reader = neo.io.Spike2IO(filename=file_name).read_segment() # read file\n",
        "            \n",
        "            if index == 0: # get sampling rate, only need to get it once\n",
        "                smr_sampling_rate = seg_reader.analogsignals[0].sampling_rate \n",
        "                \n",
        "            analog_length = min([i.size for i in seg_reader.analogsignals]) # in case analog channels have different shape\n",
        "            \n",
        "            Channel_index = [] # create an empty list to store channel names\n",
        "\n",
        "            for C_index, C_data in enumerate(seg_reader.analogsignals[:-1]): # -1 indicates we disgard 'Raw' channel   \n",
        "                shape = seg_reader.analogsignals[C_index].shape[1] # See how many channels are contained in each element of the list\n",
        "                if C_index==0:\n",
        "                  Channel_signal = C_data.as_array()[:analog_length,];\n",
        "                else:\n",
        "                  Channel_signal = np.append(Channel_signal, C_data.as_array()[:analog_length,], axis=1)\n",
        "                for i in range(shape):\n",
        "                  Channel_index.append(seg_reader.analogsignals[C_index].name) # get channel name one by one and put in Channel_index\n",
        "\n",
        "            Channel_signal = np.append(Channel_signal, np.asarray(seg_reader.analogsignals[0].times[:analog_length,]).reshape(analog_length,1), axis=1)# get time stamps and put in Channel_signal\n",
        "            Channel_index.append('Time') \n",
        "\n",
        "            Channel_signal_output.append(pd.DataFrame(Channel_signal,columns=Channel_index))\n",
        "\n",
        "            marker_channel_index = [index for index,value in enumerate(seg_reader.events) if value.name == 'marker'][0] #find 'marker' channel\n",
        "            marker_labels = seg_reader.events[marker_channel_index].get_labels().astype('int') # get 'marker' labels\n",
        "            marker_values = seg_reader.events[marker_channel_index].as_array() # get 'marker' values\n",
        "            marker = {'labels': marker_labels, 'values': marker_values} # arrange labels and values in a dict\n",
        "            marker_list.append(marker)\n",
        "            \n",
        "        return Channel_signal_output, marker_list, smr_sampling_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aUfFxMEIPNw"
      },
      "source": [
        "### log_extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91Iln6AMr-fV"
      },
      "outputs": [],
      "source": [
        "# Read log file here\n",
        "class log_extractor(object):\n",
        "    def __init__(self):\n",
        "        self.folder_path = data_folder_name\n",
        "        #self.files_names = [file for file in os.listdir(self.folder_path) if 'txt' and '0' in file]\n",
        "        self.files_names = (\"m51s936.txt\",)\n",
        "        self.full_path_file_names = os.path.join(self.folder_path,self.files_names[0])\n",
        "        \n",
        "    def extract_data(self):\n",
        "        ffLinenumberList = []\n",
        "        ff_information = []\n",
        "        ffname_index = 0\n",
        "        \n",
        "        with open(self.full_path_file_names,'r',encoding='UTF-8') as content:\n",
        "            log_content = content.readlines()\n",
        "         \n",
        "        for LineNumber, line in enumerate(log_content):\n",
        "            key_ff = re.search('Firefly', line)\n",
        "            key_monkey = re.search('Monkey', line)\n",
        "            if key_ff is not None:\n",
        "                ffLinenumberList.append(LineNumber)\n",
        "            if key_monkey is not None:\n",
        "                monkeyLineNum = LineNumber\n",
        "\n",
        "        # get ff data\n",
        "        for index, LineNumber in enumerate(ffLinenumberList):\n",
        "            FF_Catched_T = []\n",
        "            FF_Position = []\n",
        "            FF_believed_position = []\n",
        "            FF_flash_T = []\n",
        "            \n",
        "            if index == len(ffLinenumberList)-1:\n",
        "                log_content_block = log_content[ffLinenumberList[index]+1:monkeyLineNum]\n",
        "            else:\n",
        "                log_content_block = log_content[ffLinenumberList[index]+1:ffLinenumberList[index+1]]\n",
        "            \n",
        "            for line in log_content_block:\n",
        "                if len(line.split(' ')) == 5:\n",
        "                    if 'inf' not in line:\n",
        "                        FF_Catched_T.append(float(line.split(' ')[0]))\n",
        "                        FF_Position.append([float(line.split(' ')[1]),float(line.split(' ')[2])])\n",
        "                        FF_believed_position.append([float(line.split(' ')[3]),float(line.split(' ')[4])])\n",
        "                else:\n",
        "                    try:\n",
        "                        FF_flash_T.append([float(line.split(' ')[0]),float(line.split(' ')[1])])\n",
        "                    except:\n",
        "                        1\n",
        "                        \n",
        "            FF_flash_T = np.array(FF_flash_T)\n",
        "            seperated_ff = np.digitize(FF_flash_T.T[0],FF_Catched_T)\n",
        "            for j in np.unique(seperated_ff)[:-1]:\n",
        "                \n",
        "                ff_information.append({'ff_index':ffname_index,'ff_catched_T':FF_Catched_T[j],'ff_real_position': np.array(FF_Position[j]),\n",
        "                                  'ff_believed_position': np.array(FF_believed_position[j]),\n",
        "                                      'ff_flash_T': FF_flash_T[seperated_ff==j]})  \n",
        "                \n",
        "                ffname_index = ffname_index+1    \n",
        "                \n",
        "        #get monkey data\n",
        "        Monkey_X = []\n",
        "        Monkey_Y = []\n",
        "        Monkey_Position_T = []\n",
        "        \n",
        "        for line in log_content[monkeyLineNum+1:]:\n",
        "            Monkey_X.append(float(line.split(' ')[0]))\n",
        "            Monkey_Y.append(float(line.split(' ')[1]))\n",
        "            Monkey_Position_T.append(float(line.split(' ')[2]))\n",
        "            \n",
        "        monkey_information = {'monkey_x': np.array(Monkey_X), 'monkey_y': np.array(Monkey_Y), 'monkey_t': np.array(Monkey_Position_T)}\n",
        "        \n",
        "        return ff_information, monkey_information\n",
        "        #self.ff_information = ff_information\n",
        "        #self.monkey_information = monkey_information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC_HUAJI6Att"
      },
      "source": [
        "### monkey_smr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSNxOr5RMbvt"
      },
      "outputs": [],
      "source": [
        "Channel_signal_output,marker_list,smr_sampling_rate = smr_extractor().extract_data()\n",
        "#Considering the first smr file, use marker_list[0], Channel_signal_output[0]\n",
        "juice_timestamp = marker_list[0]['values'][marker_list[0]['labels']==4]\n",
        "Channel_signal_smr1 = Channel_signal_output[0]\n",
        "Channel_signal_smr1['section'] = np.digitize(Channel_signal_smr1.Time,juice_timestamp) # seperate analog signal by juice timestamps\n",
        "# Remove tail of analog data\n",
        "Channel_signal_smr1 = Channel_signal_smr1[Channel_signal_smr1['section']<Channel_signal_smr1['section'].unique()[-1]]\n",
        "Channel_signal_smr1['Time'].iloc[-1] = juice_timestamp[-1]\n",
        "# Remove head of analog data\n",
        "Channel_signal_smr1 = Channel_signal_smr1[Channel_signal_smr1['Time']>marker_list[0]['values'][marker_list[0]['labels']==1][0]]\n",
        "\n",
        "\n",
        "monkey_smr_dataframe = Channel_signal_smr1[[\"Time\", \"Signal stream 1\", \"Signal stream 2\", \"Signal stream 3\", \"Signal stream 10\"]].reset_index(drop=True)\n",
        "monkey_smr_dataframe.columns = ['monkey_t', 'monkey_x', 'monkey_y', 'monkey_speed', 'AngularV']\n",
        "monkey_smr = dict(zip(monkey_smr_dataframe.columns.tolist(), np.array(monkey_smr_dataframe.values.T.tolist())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtYgM4njsgf1"
      },
      "source": [
        "### Data from monkey\n",
        "\n",
        "ff & monkey information\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjv6DI-jqgGW"
      },
      "source": [
        "speed of monkey = delta_position/t\n",
        "delta_position = sqrt(delta_x^2 + delta_y^2)\n",
        "\n",
        "Here we can only assume that the trajectory between two time points \n",
        "is straight.\n",
        "\n",
        "To calculate speed_i, we use the information at t_i and t_i+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FunlWT3cizGl"
      },
      "outputs": [],
      "source": [
        "# for monkey\n",
        "NEW_DATASET = False\n",
        "MONKEY_DATA = True\n",
        "NO_PLOT_NEEDED = True\n",
        "data_folder_name = \"0219\"\n",
        "data_num = 19\n",
        "\n",
        "ff_information,monkey_information = log_extractor().extract_data()\n",
        "#sort ff by catched time\n",
        "ff_index = []\n",
        "ff_catched_T = []\n",
        "ff_real_position = []\n",
        "ff_believed_position = []\n",
        "ff_life = []\n",
        "ff_flash = []\n",
        "ff_flash_end = []  # This is the time that the firefly last stops flash\n",
        "for item in ff_information:\n",
        "    item['Life'] = np.array([item['ff_flash_T'][0][0],item['ff_catched_T']])\n",
        "    ff_index = np.hstack((ff_index,item['ff_index']))\n",
        "    ff_catched_T = np.hstack((ff_catched_T,item['ff_catched_T']))\n",
        "    ff_real_position.append(item['ff_real_position'])\n",
        "    ff_believed_position.append(item['ff_believed_position'])\n",
        "    ff_life.append(item['Life'])\n",
        "    ff_flash.append(item['ff_flash_T'])\n",
        "    ff_flash_end.append(item['ff_flash_T'][-1][-1])\n",
        "sort_index = np.argsort(ff_catched_T)\n",
        "ff_index_sorted = ff_index[sort_index]\n",
        "ff_catched_T_sorted = ff_catched_T[sort_index]\n",
        "ff_real_position_sorted = np.array(ff_real_position)[sort_index]\n",
        "ff_believed_position_sorted = np.array(ff_believed_position)[sort_index]\n",
        "ff_life_sorted = np.array(ff_life)[sort_index]\n",
        "ff_flash_sorted  = np.array(ff_flash, dtype=object)[sort_index].tolist()\n",
        "ff_flash_end_sorted = np.array(ff_flash_end)[sort_index]\n",
        "\n",
        "# Use accurate juice timestamps, ff_catched_T_sorted for smr1 (so that the time frame is correct)\n",
        "ff_catched_T_sorted = ff_catched_T_sorted[:np.where(ff_catched_T_sorted<=Channel_signal_smr1.Time.values[-1])[0][-1]+1]\n",
        "\n",
        "catched_ff_num = len(ff_catched_T_sorted) - 200  \n",
        "total_ff_num = len(ff_life_sorted)\n",
        "M_catched_ff_num = len(ff_catched_T_sorted) - 200\n",
        "M_total_ff_num = len(ff_life_sorted)\n",
        "\n",
        "\n",
        "M_ff_catched_T_sorted = ff_catched_T_sorted.copy()\n",
        "M_ff_real_position_sorted = ff_real_position_sorted.copy()\n",
        "M_ff_life_sorted = ff_life_sorted.copy()\n",
        "M_ff_flash_sorted = ff_flash_sorted.copy()\n",
        "M_ff_believed_position_sorted = ff_believed_position_sorted.copy()\n",
        "\n",
        "#################################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "delta_time = np.diff(monkey_information['monkey_t'])\n",
        "delta_x = np.diff(monkey_information['monkey_x'])\n",
        "delta_y = np.diff(monkey_information['monkey_y'])\n",
        "delta_position = np.sqrt(np.square(delta_x)+np.square(delta_y))\n",
        "monkey_speed = np.divide(delta_position, delta_time)\n",
        "monkey_speed = np.append(np.array([0]), monkey_speed)\n",
        "\n",
        "# If the monkey's speed at one point exceeds 200 \n",
        "#(this can happen when the monkey reaches the boundary and comes out at another place)\n",
        "# we replace it with the previous speed.\n",
        "while np.where(monkey_speed>=200)[0].size > 0:\n",
        "  index = np.where(monkey_speed>=200)[0]\n",
        "  monkey_speed1 = np.append(np.array([0]), monkey_speed)\n",
        "  monkey_speed[index] = monkey_speed1[index]\n",
        "monkey_information['monkey_speed'] = monkey_speed\n",
        "monkey_information['monkey_speeddummy'] = (monkey_information['monkey_speed']> 0.01).astype(int) \n",
        "monkey_speeddummy = (monkey_speed>0.1).astype(int) \n",
        "\n",
        "\n",
        "# Add angle of the monkey\n",
        "\n",
        "angle = [90]  # The monkey is at 90 degree angle at the beginning\n",
        "current_angle = 90 # This keeps track of the current angle during the iterations\n",
        "# Find the time in the data that is closest (right before) the time where we wan to know the monkey's angular position.\n",
        "for i in range(1, len(monkey_information['monkey_t'])):\n",
        "  # If the monkey basically stopped at this moment, we keep the previous angle\n",
        "  if monkey_information['monkey_speed'][i-1] < 1:  # use i-1 because 'monkey_speed' has 1 less element\n",
        "    angle.append(current_angle)   \n",
        "  else:\n",
        "    myradians = math.atan2(monkey_information['monkey_y'][i]-monkey_information['monkey_y'][i-1], monkey_information['monkey_x'][i]-monkey_information['monkey_x'][i-1])\n",
        "    mydegrees = math.degrees(myradians)\n",
        "    # Compare the new angle with the previous angle\n",
        "    # If the different is too large, then the monkey might be going backward, and we will just subtract 180 from the angle\n",
        "    if 170 < abs(mydegrees-current_angle)%360 < 190 :\n",
        "      current_angle = current_angle - 180\n",
        "      if current_angle < -180:\n",
        "        current_angle += 360\n",
        "      angle.append(current_angle)\n",
        "    else: #else, we keep the new angle\n",
        "      current_angle = mydegrees\n",
        "      angle.append(current_angle)\n",
        "angle = np.array(angle)\n",
        "monkey_information['monkey_angle'] = angle*pi/180\n",
        "\n",
        "delta_time = np.delete(monkey_information['monkey_t'], 0) - np.delete(monkey_information['monkey_t'], -1)\n",
        "delta_angle = np.delete(monkey_information['monkey_angle'], 0) - np.delete(monkey_information['monkey_angle'], -1)\n",
        "monkey_dw = np.append(np.array([0]), np.divide(delta_angle, delta_time))\n",
        "monkey_information['monkey_dw'] = monkey_dw\n",
        "\n",
        "monkey_dw = np.divide(np.diff(np.array(monkey_information['monkey_angle']), prepend=monkey_information['monkey_angle'][0]), np.append(delta_time[0], delta_time))\n",
        "monkey_information['monkey_dw'] = monkey_dw\n",
        "\n",
        "M_monkey_information = monkey_information.copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nylIDSZZRCxD"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "safydmQyWM0U"
      },
      "source": [
        "### find_intersection\n",
        "source: https://codereview.stackexchange.com/questions/203468/find-the-intervals-which-have-a-non-empty-intersection-with-a-given-interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFxkY1CJWLIv"
      },
      "outputs": [],
      "source": [
        "def find_intersection(intervals, query):\n",
        "    \"\"\"Find intersections between intervals.\n",
        "    Intervals are open and are represented as pairs (lower bound,\n",
        "    upper bound).\n",
        "\n",
        "    Arguments:\n",
        "    intervals: array_like, shape=(N, 2) -- Array of intervals.\n",
        "    query: array_like, shape=(2,) -- Interval to query.\n",
        "\n",
        "    Returns:\n",
        "    Array of indexes of intervals that overlap with query.\n",
        "\n",
        "    \"\"\"\n",
        "    intervals = np.asarray(intervals)\n",
        "    lower, upper = query\n",
        "    return np.where((lower < intervals[:, 1]) & (intervals[:, 0] < upper))[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqK53Ucnl-wg"
      },
      "source": [
        "### flashing_ff (based on trial)\n",
        "Find the indices of the fireflies that have flashed during the trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXYVvQzicptF"
      },
      "outputs": [],
      "source": [
        "def flashing_ff(ff_flash_sorted, duration):\n",
        "  ## Find the index of the fireflies that have flashed during the trial\n",
        "  ## Input: ff_flash_sorted contains the time that each firefly flashes on and off\n",
        "  ##        duration is the duration of the trial\n",
        "  ## Output: flash_index contains the indices of the fireflies that have flashed during the trial (among all fireflies)\n",
        "  flash_index = []\n",
        "  for index in range(total_ff_num):\n",
        "    ff = ff_flash_sorted[index]\n",
        "    if len(find_intersection(ff, duration)) > 0:\n",
        "      flash_index.append(index)\n",
        "  return flash_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJDdv-ixmfZp"
      },
      "source": [
        "###flash_on_ff (dict, based on points)\n",
        "Find the fireflies that are visible at each time point (for animation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0W9qbbuyekU"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary of {time: [indices of fireflies that are visible], ...}\n",
        "def flash_on_ff(anim_t, currentTrial, num_trials, ff_flash_sorted):\n",
        "  alive_ff_during_this_trial = np.where((ff_life_sorted[:,1] > ff_catched_T_sorted[currentTrial-num_trials])\\\n",
        "                                        & (ff_life_sorted[:,0] < ff_catched_T_sorted[currentTrial]))[0]\n",
        "  flash_on_ff_dict={}\n",
        "  for time in anim_t:\n",
        "    visible_ff_indices = [index for index in alive_ff_during_this_trial \\\n",
        "     if len(np.where(np.logical_and(ff_flash_sorted[index][:,0] <= time, \\\n",
        "                                       ff_flash_sorted[index][:,1] >= time))[0]) > 0]\n",
        "    flash_on_ff_dict[time] = visible_ff_indices\n",
        "  return flash_on_ff_dict\n",
        "\n",
        "# To call:\n",
        "# flash_on_ff_dict = flash_on_ff(anim_t, currentTrial, num_trials, ff_flash_sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZEn1XY4RK6z"
      },
      "source": [
        "### flash_starttime\n",
        "Find the earliest time that each firefly begins to flash during that trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eX5Fhtb8YR7t"
      },
      "outputs": [],
      "source": [
        "def flash_starttime(flash_index, duration):\n",
        "  # Find the earliest time that each firefly begins to flash during that trial\n",
        "  firefly_on = []\n",
        "  for index in flash_index:\n",
        "    ff = ff_flash_sorted[index]\n",
        "    overlapped_intervals = find_intersection(ff, duration)\n",
        "    first_start_time = ff[overlapped_intervals].flatten()[0]\n",
        "    if first_start_time >= duration[0]:\n",
        "      firefly_on.append(first_start_time)\n",
        "    else:\n",
        "      firefly_on.append(duration[0])\n",
        "  return firefly_on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4phjdAoRIlb"
      },
      "outputs": [],
      "source": [
        "# Another method\n",
        "'''\n",
        "def flash_starttime(flash_index, duration):\n",
        "  # Find the earliest time that each firefly begins to flash during that trial\n",
        "  firefly_on = []\n",
        "  for index in flash_index:\n",
        "    ff = ff_flash_sorted[index]\n",
        "    ff_all_starttime = ff[:,0]\n",
        "    start_time_index = np.argmax(ff_all_starttime >= duration[0])\n",
        "    start_time = ff_all_starttime[start_time_index]\n",
        "    # Consider extreme cases\n",
        "    if start_time >= duration[1]:\n",
        "      start_time = max(ff_all_starttime[start_time_index-1], duration[0])\n",
        "    elif ff[start_time_index-1][1] >= duration[0]:\n",
        "      start_time = duration[0]\n",
        "    firefly_on.append(start_time)\n",
        "  firefly_on = np.array(firefly_on)\n",
        "  return firefly_on\n",
        "  '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwVBYsNM8FuH"
      },
      "source": [
        "###believed_ff (updated version)\n",
        "Match the believed positions of the fireflies to the time when they are captured (for animation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mO_bXb-Q5Zhl"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary of {time: [[believed_ff_position], [believed_ff_position2], ...], ...}\n",
        "def believed_ff(anim_t, currentTrial, num_trials, ff_believed_position_sorted, ff_catched_T_sorted):\n",
        "  believed_ff_dict={}\n",
        "  # For each time point:\n",
        "  for index in range(len(anim_t)):\n",
        "    time = anim_t[index]\n",
        "    believed_ff_indices = [ff_believed_position_sorted[ff] for ff in range(currentTrial, currentTrial+num_trials) if time > ff_catched_T_sorted[ff]]\n",
        "    believed_ff_dict[index] = believed_ff_indices\n",
        "\n",
        "  # # The last point\n",
        "  # believed_ff_indices = [(ff_believed_position_sorted[ff]) for ff in range(currentTrial-num_trials+1, currentTrial+1)]\n",
        "  # believed_ff_dict[len(anim_t)-1] = believed_ff_indices\n",
        "\n",
        "  return believed_ff_dict\n",
        "\n",
        "# To call:\n",
        "# believed_ff_dict = believed_ff(anim_t, currentTrial, num_trials, ff_believed_position_sorted, ff_catched_T_sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0y2XWr3c_rF"
      },
      "source": [
        "### distance_traveled\n",
        "Find the length of the trajectory run by the monkey in this duration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K74cOdq-FYAP"
      },
      "outputs": [],
      "source": [
        "def distance_traveled(currentTrial):\n",
        "  duration = [ff_catched_T_sorted[currentTrial-1], ff_catched_T_sorted[currentTrial]]\n",
        "  cum_indices = np.where((monkey_information['monkey_t'] >= duration[0]) & (monkey_information['monkey_t'] <= duration[1]))[0]\n",
        "  if len(cum_indices) > 5:\n",
        "    cum_t, cum_angle = monkey_information['monkey_t'][cum_indices],  monkey_information['monkey_angle'][cum_indices]\n",
        "    cum_mx, cum_my = monkey_information['monkey_x'][cum_indices], monkey_information['monkey_y'][cum_indices] \n",
        "    cum_speed = monkey_information['monkey_speed'][cum_indices]\n",
        "    distance = np.sum((cum_t[1:] - cum_t[:-1])*cum_speed)\n",
        "  return distance\n",
        "\n",
        "# To Run:\n",
        "# distance = distance_traveled(currentTrial)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3JTUI5LfjtY"
      },
      "source": [
        "###abs_displacement\n",
        "Find the absolute displacement between the target for the currentTrial and the target for currentTrial.\n",
        "Return 9999 if the monkey has hit the border at one point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3Johk2giCPa"
      },
      "outputs": [],
      "source": [
        "def abs_displacement(currentTrial):\n",
        "  duration = [ff_catched_T_sorted[currentTrial-1], ff_catched_T_sorted[currentTrial]]\n",
        "  displacement = 0\n",
        "  cum_indices = np.where((monkey_information['monkey_t'] >= duration[0]) & (monkey_information['monkey_t'] <= duration[1]))[0]\n",
        "  if len(cum_indices) > 5:\n",
        "    cum_t, cum_angle = monkey_information['monkey_t'][cum_indices],  monkey_information['monkey_angle'][cum_indices]\n",
        "    cum_mx, cum_my = monkey_information['monkey_x'][cum_indices], monkey_information['monkey_y'][cum_indices] \n",
        "    flag = True\n",
        "    # If the monkey has hit the boundary\n",
        "    if np.any(cum_mx[1:]-cum_mx[:-1] > 10) or np.any(cum_my[1:]-cum_my[:-1] > 10):\n",
        "      displacement = 9999\n",
        "    else:\n",
        "      displacement = LA.norm(ff_believed_position_sorted[currentTrial]-ff_believed_position_sorted[currentTrial-1])\n",
        "  return displacement\n",
        "\n",
        "# To call:\n",
        "# displacement = abs_displacement(currentTrial):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6xhWRzpos7N"
      },
      "source": [
        "### num_of_stops\n",
        "Find the stops the monkey made between currentTrial and currentTrial + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_LT4m56ozz7"
      },
      "outputs": [],
      "source": [
        "if MONKEY_DATA == True:\n",
        "  def num_of_stops(currentTrial):\n",
        "    duration = [ff_catched_T_sorted[currentTrial-1], ff_catched_T_sorted[currentTrial]]\n",
        "    cum_indices = np.where((monkey_information['monkey_t'] > duration[0]) & (monkey_information['monkey_t'] <= duration[1]))[0]\n",
        "    if len(cum_indices) > 5:\n",
        "      cum_t, cum_angle = monkey_information['monkey_t'][cum_indices],  monkey_information['monkey_angle'][cum_indices]\n",
        "      cum_mx, cum_my = monkey_information['monkey_x'][cum_indices], monkey_information['monkey_y'][cum_indices] \n",
        "      cum_speeddummy = monkey_information['monkey_speeddummy'][cum_indices]\n",
        "      zerospeed_index = np.where(cum_speeddummy==0)[0]\n",
        "      if len(zerospeed_index) > 0 :\n",
        "        zerospeedx, zerospeedy = cum_mx[zerospeed_index], cum_my[zerospeed_index]\n",
        "        zerospeedindex = cum_indices[zerospeed_index]\n",
        "        stop0 = np.array(list(zip(zerospeedx,zerospeedy)))\n",
        "        _, stops_index = np.unique(stop0, axis=0, return_index=True)\n",
        "        stops = stop0[stops_index[np.argsort(stops_index)]]\n",
        "        stop_indices = zerospeedindex[stops_index[np.argsort(stops_index)]]\n",
        "        distinct_stops = [stops[0]] + [stops[i+1] for i in range(len(stops)-1) if LA.norm(np.array((stops[i+1][0]-stops[i][0], stops[i+1][1]-stops[i][1]))) > 0.6]\n",
        "      else:\n",
        "        distinct_stops = []\n",
        "    else:\n",
        "      distinct_stops = []\n",
        "    return distinct_stops\n",
        "else:\n",
        "  def num_of_stops(currentTrial):\n",
        "    duration = [ff_catched_T_sorted[currentTrial-1], ff_catched_T_sorted[currentTrial]]\n",
        "    cum_indices = np.where((monkey_information['monkey_t'] > duration[0]) & (monkey_information['monkey_t'] <= duration[1]))[0]\n",
        "    if len(cum_indices) > 5:\n",
        "      cum_mx, cum_my = monkey_information['monkey_x'][cum_indices], monkey_information['monkey_y'][cum_indices] \n",
        "      cum_speeddummy = monkey_information['monkey_speeddummy'][cum_indices]\n",
        "      zerospeed_index = np.where(cum_speeddummy==0)[0]\n",
        "      if len(zerospeed_index) > 0 :\n",
        "        zerospeedx, zerospeedy = cum_mx[zerospeed_index], cum_my[zerospeed_index]\n",
        "        zerospeedindex = cum_indices[zerospeed_index]\n",
        "        stop0 = np.array(list(zip(zerospeedx,zerospeedy)))\n",
        "        _, stops_index = np.unique(stop0, axis=0, return_index=True)\n",
        "        stops = stop0[stops_index[np.argsort(stops_index)]]\n",
        "        stop_indices = zerospeedindex[stops_index[np.argsort(stops_index)]]\n",
        "        distinct_stops = stops\n",
        "      else:\n",
        "        distinct_stops = []\n",
        "    else:\n",
        "      distinct_stops = []\n",
        "    return distinct_stops\n",
        "\n",
        "# To call:\n",
        "# distinct_stops = num_of_stops(currentTrial)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04P_sVdiaAYc"
      },
      "source": [
        "### num_of_stops_indices\n",
        "Find the stops the monkey made between currentTrial and currentTrial + 1 and return the indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSEp45QgBRfD"
      },
      "outputs": [],
      "source": [
        "if MONKEY_DATA == True:\n",
        "  def num_of_stops_indices(currentTrial):\n",
        "    duration = [ff_catched_T_sorted[currentTrial-1], ff_catched_T_sorted[currentTrial]]\n",
        "    distinct_stops_indices = []\n",
        "    cum_indices = np.where((monkey_information['monkey_t'] > duration[0]) & \n",
        "                              (monkey_information['monkey_t'] <= duration[1]))[0]\n",
        "    if len(cum_indices) > 5:\n",
        "      cum_t, cum_angle = monkey_information['monkey_t'][cum_indices],  monkey_information['monkey_angle'][cum_indices]\n",
        "      cum_mx, cum_my = monkey_information['monkey_x'][cum_indices], monkey_information['monkey_y'][cum_indices] \n",
        "      cum_speeddummy = monkey_information['monkey_speeddummy'][cum_indices]\n",
        "      zerospeed_index = np.where(cum_speeddummy==0)[0]\n",
        "      if len(zerospeed_index) > 0 :\n",
        "        zerospeedx, zerospeedy = cum_mx[zerospeed_index], cum_my[zerospeed_index]\n",
        "        zerospeedindex = cum_indices[zerospeed_index]\n",
        "        stop0 = np.array(list(zip(zerospeedx,zerospeedy)))\n",
        "        _, stops_index = np.unique(stop0, axis=0, return_index=True)\n",
        "        stops = stop0[stops_index[np.argsort(stops_index)]]\n",
        "        stop_indices = zerospeedindex[stops_index[np.argsort(stops_index)]]\n",
        "        if len(stops) > 1: \n",
        "          distinct_stops_indices = [stop_indices[0]] + [stop_indices[i+1] for i in range(len(stops)-1) if LA.norm(np.array((stops[i+1][0]-stops[i][0], stops[i+1][1]-stops[i][1]))) > 0.6]\n",
        "    return distinct_stops_indices\n",
        "else:\n",
        "  def num_of_stops_indices(currentTrial):\n",
        "    duration = [ff_catched_T_sorted[currentTrial-1], ff_catched_T_sorted[currentTrial]]\n",
        "    distinct_stops_indices = []\n",
        "    cum_indices = np.where((monkey_information['monkey_t'] > duration[0]) & \n",
        "                              (monkey_information['monkey_t'] <= duration[1]))[0]\n",
        "    if len(cum_indices) > 5:\n",
        "      cum_mx, cum_my = monkey_information['monkey_x'][cum_indices], monkey_information['monkey_y'][cum_indices] \n",
        "      cum_speeddummy = monkey_information['monkey_speeddummy'][cum_indices]\n",
        "      zerospeed_index = np.where(cum_speeddummy==0)[0]\n",
        "      if len(zerospeed_index) > 0 :\n",
        "        zerospeedx, zerospeedy = cum_mx[zerospeed_index], cum_my[zerospeed_index]\n",
        "        zerospeedindex = cum_indices[zerospeed_index]\n",
        "        stop0 = np.array(list(zip(zerospeedx,zerospeedy)))\n",
        "        _, stops_index = np.unique(stop0, axis=0, return_index=True)\n",
        "        stops = stop0[stops_index[np.argsort(stops_index)]]\n",
        "        stop_indices = zerospeedindex[stops_index[np.argsort(stops_index)]]\n",
        "      return stop_indices\n",
        "# To call:\n",
        "# distinct_stops = num_of_stops(currentTrial)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN8dUzJcpNnn"
      },
      "source": [
        "### num_of_stops_since_target_last_seen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq1Z3fI-auvB"
      },
      "outputs": [],
      "source": [
        "def num_of_stops_since_target_last_seen(currentTrial):\n",
        "  duration = [ff_catched_T_sorted[currentTrial]-t_last_visible[currentTrial-1], ff_catched_T_sorted[currentTrial]]\n",
        "  if t_last_visible[currentTrial-1] > 50:\n",
        "    distinct_stops = []\n",
        "    return distinct_stops\n",
        "  cum_indices = np.where((monkey_information['monkey_t'] > duration[0]) & (monkey_information['monkey_t'] <= duration[1]))[0]\n",
        "  if len(cum_indices) > 5:\n",
        "    cum_t = monkey_information['monkey_t'][cum_indices]\n",
        "    cum_mx, cum_my = monkey_information['monkey_x'][cum_indices], monkey_information['monkey_y'][cum_indices] \n",
        "    cum_speeddummy = monkey_information['monkey_speeddummy'][cum_indices]\n",
        "    zerospeed_index = np.where(cum_speeddummy==0)[0]\n",
        "    if len(zerospeed_index) > 0 :\n",
        "      zerospeedx, zerospeedy = cum_mx[zerospeed_index], cum_my[zerospeed_index]\n",
        "      zerospeedindex = cum_indices[zerospeed_index]\n",
        "      stop0 = np.array(list(zip(zerospeedx,zerospeedy)))\n",
        "      _, stops_index = np.unique(stop0, axis=0, return_index=True)\n",
        "      stops = stop0[stops_index[np.argsort(stops_index)]]\n",
        "      stop_indices = zerospeedindex[stops_index[np.argsort(stops_index)]]\n",
        "      distinct_stops = [stops[0]] + [stops[i+1] for i in range(len(stops)-1) if LA.norm(np.array((stops[i+1][0]-stops[i][0], stops[i+1][1]-stops[i][1]))) > 0.6]\n",
        "    else:\n",
        "      distinct_stops = []\n",
        "  else:\n",
        "    distinct_stops = []\n",
        "  return distinct_stops\n",
        "\n",
        "# To call:\n",
        "# distinct_stops = num_of_stops(currentTrial)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo8sZGZ2lUkq"
      },
      "source": [
        "### find_clusters\n",
        "Assign each stop with a number that indicates which cluster it belongs to\n",
        "\n",
        "Note: the algorithm is slightly different from previous notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keU_3wQzh8jK"
      },
      "outputs": [],
      "source": [
        "def find_clusters(currentTrial, distance_between_points):\n",
        "  distinct_stops = num_of_stops(currentTrial)\n",
        "  if len(distinct_stops) == 0: \n",
        "    clusters = []\n",
        "  else:\n",
        "    distinct_stops2 = np.array([[stop[0], stop[1]] for stop in distinct_stops])\n",
        "    current_cluster = 1\n",
        "    clusters = [1]\n",
        "    for i in range(1, len(distinct_stops2)):\n",
        "      flag = False\n",
        "      if LA.norm(distinct_stops2[i]-distinct_stops2[i-1]) < distance_between_points:\n",
        "          clusters.append(current_cluster)\n",
        "      else: # Create a new cluster\n",
        "        current_cluster = current_cluster+1\n",
        "        clusters.append(current_cluster)\n",
        "  return clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Odz5iw9yxWtv"
      },
      "source": [
        "### angle2ff\n",
        "Calculate the angle from the monkey to the firefly (from the monkey's perspective) at a given time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcF70BfAxe_x"
      },
      "outputs": [],
      "source": [
        "def angle2ff(firefly_xy, time):\n",
        "  num_i = np.where(monkey_information['monkey_t'] == time)[0]\n",
        "  if len(num_i) == 0:\n",
        "    time_before = np.where(monkey_information['monkey_t'] <= time)[0]\n",
        "    if len(time_before) > 0:\n",
        "      num_i = time_before[-1]\n",
        "    else:\n",
        "      num_i = 0\n",
        "\n",
        "  # Calculate the angle from the monkey to the firefly (overhead view)\n",
        "  ffradians = math.atan2(firefly_xy[1]-monkey_information['monkey_y'][num_i], firefly_xy[0]-monkey_information['monkey_x'][num_i])\n",
        "  ffdegrees = math.degrees(ffradians)\n",
        "\n",
        "  # Find the angle of the firefly from the monkey's perspective\n",
        "  ff_angle = ffdegrees-monkey_information['monkey_angle'][num_i]\n",
        "  while abs(ff_angle) > 180:\n",
        "    if ff_angle > 180:\n",
        "      ff_angle = ff_angle-360\n",
        "    elif ff_angle < -180:\n",
        "      ff_angle = ff_angle + 360\n",
        "  return ff_angle\n",
        "# to call:\n",
        "# ff_angle = angle2ff(firefly_xy, time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWs93tKLuPLx"
      },
      "source": [
        "### connect_path_ff\n",
        "Find the lines to be drawn between the nearby fireflies and the path based on the starting time to flash (excluding the target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YACIy1mym9o"
      },
      "outputs": [],
      "source": [
        "def connect_path_ff(cum_t, cum_mx, cum_my, cum_angle, currentTrial, max_distance, ff_flash_sorted, ff_real_position_sorted, total_ff_num):\n",
        "  x = []\n",
        "  y = []\n",
        "\n",
        "  if len(cum_t) > 5: # When cum_t is not empty\n",
        "    # For each firefly\n",
        "    total_ff_array = np.arange(total_ff_num)\n",
        "    non_targets_ff_array = np.delete(total_ff_array, currentTrial)\n",
        "    for i in non_targets_ff_array:\n",
        "      # For each duration of being visible\n",
        "      overlapped_intervals = ff_flash_sorted[i][find_intersection(ff_flash_sorted[i], [cum_t[0], cum_t[-1]])]\n",
        "      for interval in overlapped_intervals:\n",
        "        overlapped_indices = np.where((cum_t >= interval[0]) & (cum_t <= interval[1]))[0]\n",
        "        distances_to_monkey = LA.norm(ff_real_position_sorted[i] - np.stack([cum_mx[overlapped_indices], cum_my[overlapped_indices]], axis=1), axis=1)\n",
        "        valid_distance_indices = overlapped_indices[np.where(distances_to_monkey < max_distance)[0]]\n",
        "        \n",
        "        if len(valid_distance_indices) > 0:\n",
        "          angles_to_monkey = np.arctan2(ff_real_position_sorted[i,1]-cum_my[valid_distance_indices], \\\n",
        "                                        ff_real_position_sorted[i,0]-cum_mx[valid_distance_indices])-cum_angle[valid_distance_indices]\n",
        "          ## The following lines turn out to be unnecessary because of the range of output for np.arctan2\n",
        "          angles_to_monkey[angles_to_monkey > pi] = angles_to_monkey[angles_to_monkey > pi] - 2*pi\n",
        "          angles_to_monkey[angles_to_monkey < -pi] = angles_to_monkey[angles_to_monkey < -pi] + 2*pi\n",
        "\n",
        "          angles_adjusted = np.absolute(angles_to_monkey)-np.abs(np.arcsin(np.divide(25, np.maximum(distances_to_monkey[np.where(distances_to_monkey < max_distance)[0]], 25) ))) # use torch clip to get valid arcsin input\n",
        "          angles_adjusted = np.clip(angles_adjusted, 0, pi)\n",
        "          angles_adjusted = np.sign(angles_to_monkey)* angles_adjusted\n",
        "\n",
        "\n",
        "          overall_valid_indices = valid_distance_indices[np.where(np.absolute(angles_adjusted) <= 2*pi/9)[0]]\n",
        "          \n",
        "          x = x + [[ff_real_position_sorted[i][0]] + [cum_mx[index]] for index in overall_valid_indices]\n",
        "          y = y + [[ff_real_position_sorted[i][1]] + [cum_my[index]] for index in overall_valid_indices]\n",
        "  return x, y\n",
        "# To call:\n",
        "# x, y = connect_path_ff(cum_t, cum_mx, cum_my, cum_angle, currentTrial, 250, ff_flash_sorted, ff_real_position_sorted, total_ff_num) total_ff_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvo7Fr7Gd9QJ"
      },
      "source": [
        "### connect_path_ff2\n",
        "This deals with the case that there are multiple targets to avoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVp2xSaDd9QK"
      },
      "outputs": [],
      "source": [
        "def connect_path_ff2(cum_t, cum_mx, cum_my, cum_angle, target_nums, max_distance, ff_flash_sorted, ff_real_position_sorted, total_ff_num):\n",
        "  x = []\n",
        "  y = []\n",
        "  if len(cum_t) > 5: # When cum_t is not empty\n",
        "    # For each firefly\n",
        "    total_ff_array = np.arange(total_ff_num)\n",
        "    non_targets_ff_array = np.delete(total_ff_array, target_nums)\n",
        "    for i in non_targets_ff_array:\n",
        "      # For each duration of being visible\n",
        "      overlapped_intervals = ff_flash_sorted[i][find_intersection(ff_flash_sorted[i], [cum_t[0], cum_t[-1]])]\n",
        "      for interval in overlapped_intervals:\n",
        "        overlapped_indices = np.where((cum_t >= interval[0]) & (cum_t <= interval[1]))[0]\n",
        "        distances_to_monkey = LA.norm(ff_real_position_sorted[i] - np.stack([cum_mx[overlapped_indices], cum_my[overlapped_indices]], axis=1), axis=1)\n",
        "        valid_distance_indices = overlapped_indices[np.where(distances_to_monkey < max_distance)[0]]\n",
        "        \n",
        "        if len(valid_distance_indices) > 0:\n",
        "          angles_to_monkey = np.arctan2(ff_real_position_sorted[i,1]-cum_my[valid_distance_indices], \\\n",
        "                                        ff_real_position_sorted[i,0]-cum_mx[valid_distance_indices])-cum_angle[valid_distance_indices]\n",
        "          ## The following lines turn out to be unnecessary because of the range of output for np.arctan2\n",
        "          angles_to_monkey[angles_to_monkey > pi] = angles_to_monkey[angles_to_monkey > pi] - 2*pi\n",
        "          angles_to_monkey[angles_to_monkey < -pi] = angles_to_monkey[angles_to_monkey < -pi] + 2*pi\n",
        "\n",
        "          angles_adjusted = np.absolute(angles_to_monkey)-np.abs(np.arcsin(np.divide(25, np.maximum(distances_to_monkey[np.where(distances_to_monkey < max_distance)[0]], 25) ))) # use torch clip to get valid arcsin input\n",
        "          angles_adjusted = np.clip(angles_adjusted, 0, pi)\n",
        "          angles_adjusted = np.sign(angles_to_monkey)* angles_adjusted\n",
        "\n",
        "\n",
        "          overall_valid_indices = valid_distance_indices[np.where(np.absolute(angles_adjusted) <= 2*pi/9)[0]]\n",
        "          \n",
        "          x = x + [[ff_real_position_sorted[i][0]] + [cum_mx[index]] for index in overall_valid_indices]\n",
        "          y = y + [[ff_real_position_sorted[i][1]] + [cum_my[index]] for index in overall_valid_indices]\n",
        "  return x, y\n",
        "# To call:\n",
        "# x, y = connect_path_ff2(cum_t, cum_mx, cum_my, cum_angle, target_nums, 250, ff_flash_sorted, ff_real_position_sorted, total_ff_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lMKDGi5xEiR"
      },
      "source": [
        "### connect_path_ff_with_target\n",
        "No longer exclude target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41RlBc-9hVZI"
      },
      "outputs": [],
      "source": [
        "def connect_path_ff_with_target(cum_t, cum_mx, cum_my, cum_angle, currentTrial, max_distance, ff_flash_sorted, ff_real_position_sorted, total_ff_num):\n",
        "  x = []\n",
        "  y = []\n",
        "\n",
        "  if len(cum_t) > 5: # When cum_t is not empty\n",
        "    # For each firefly\n",
        "    for i in range(total_ff_num):\n",
        "      # For each duration of being visible\n",
        "      overlapped_intervals = ff_flash_sorted[i][find_intersection(ff_flash_sorted[i], [cum_t[0], cum_t[-1]])]\n",
        "      for interval in overlapped_intervals:\n",
        "        overlapped_indices = np.where((cum_t >= interval[0]) & (cum_t <= interval[1]))[0]\n",
        "        distances_to_monkey = LA.norm(ff_real_position_sorted[i] - np.stack([cum_mx[overlapped_indices], cum_my[overlapped_indices]], axis=1), axis=1)\n",
        "        valid_distance_indices = overlapped_indices[np.where(distances_to_monkey < max_distance)[0]]\n",
        "        \n",
        "        if len(valid_distance_indices) > 0:\n",
        "          angles_to_monkey = np.arctan2(ff_real_position_sorted[i,1]-cum_my[valid_distance_indices], \\\n",
        "                                        ff_real_position_sorted[i,0]-cum_mx[valid_distance_indices])-cum_angle[valid_distance_indices]\n",
        "          ## The following lines turn out to be unnecessary because of the range of output for np.arctan2\n",
        "          angles_to_monkey[angles_to_monkey > pi] = angles_to_monkey[angles_to_monkey > pi] - 2*pi\n",
        "          angles_to_monkey[angles_to_monkey < -pi] = angles_to_monkey[angles_to_monkey < -pi] + 2*pi\n",
        "\n",
        "          angles_adjusted = np.absolute(angles_to_monkey)-np.abs(np.arcsin(np.divide(25, np.maximum(distances_to_monkey[np.where(distances_to_monkey < max_distance)[0]], 25) ))) # use torch clip to get valid arcsin input\n",
        "          angles_adjusted = np.clip(angles_adjusted, 0, pi)\n",
        "          angles_adjusted = np.sign(angles_to_monkey)* angles_adjusted\n",
        "          overall_valid_indices = valid_distance_indices[np.where(np.absolute(angles_adjusted) <= 2*pi/9)[0]]\n",
        "          x = x + [[ff_real_position_sorted[i][0]] + [cum_mx[index]] for index in overall_valid_indices]\n",
        "          y = y + [[ff_real_position_sorted[i][1]] + [cum_my[index]] for index in overall_valid_indices]\n",
        "  return x, y\n",
        "  # To call:\n",
        "  # x, y = connect_path_ff(cum_t, cum_mx, cum_my, cum_angle, currentTrial, 250, ff_flash_sorted, ff_real_position_sorted, total_ff_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSdoZGKl5q6Y"
      },
      "source": [
        "### onoff_lines\n",
        "Find the parts of the path where the target has been on. The angle of the firefly is considered. The firefly is considered visible only when it's at the right angle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0zP2G-m5pfi"
      },
      "outputs": [],
      "source": [
        "def onoff_lines(cum_t, cum_mx, cum_my, ff_real_position_sorted, currentTrial, duration):\n",
        "  target_ff_flash = ff_flash_sorted[currentTrial]\n",
        "  overlapped_intervals = target_ff_flash[find_intersection(target_ff_flash, duration)]\n",
        "  x_onoff_lines=np.array([])\n",
        "  y_onoff_lines=np.array([])\n",
        "  for interval in overlapped_intervals:\n",
        "    correspondingIndex_onPath = np.where((cum_t > np.maximum(interval[0], duration[0])) & \\\n",
        "                                          (cum_t < np.minimum(interval[1], duration[1])))[0]\n",
        "    x_onoff_lines=np.append(x_onoff_lines, cum_mx[correspondingIndex_onPath])\n",
        "    y_onoff_lines=np.append(y_onoff_lines, cum_my[correspondingIndex_onPath])\n",
        "  return x_onoff_lines, y_onoff_lines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcPgBKC89zNL"
      },
      "source": [
        "### onoff_lines_rightangle\n",
        "Find the parts of the path where the target has been on. The angle of the firefly is considered. The firefly is considered visible only when it's at the right angle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMa0ErLsn46j"
      },
      "outputs": [],
      "source": [
        "def onoff_lines_rightangle(cum_t, cum_mx, cum_my, cum_angle, currentTrial, duration, ff_real_position_sorted, ff_flash_sorted):\n",
        "  target_ff_flash = ff_flash_sorted[currentTrial]\n",
        "  overlapped_intervals = target_ff_flash[find_intersection(target_ff_flash, duration)]\n",
        "\n",
        "  indices_onoff_lines = []\n",
        "  for interval in overlapped_intervals:\n",
        "    correspondingIndex_onPath = np.where((cum_t > np.maximum(interval[0], duration[0])) & \\\n",
        "                                          (cum_t < np.minimum(interval[1], duration[1])))[0]\n",
        "    distances_to_monkey = LA.norm(ff_real_position_sorted[currentTrial] - np.stack([cum_mx[correspondingIndex_onPath], cum_my[correspondingIndex_onPath]], axis=1), axis=1)\n",
        "    angles_to_monkey = np.arctan2(ff_real_position_sorted[currentTrial,1]-cum_my[correspondingIndex_onPath], \\\n",
        "                                  ff_real_position_sorted[currentTrial,0]-cum_mx[correspondingIndex_onPath]) -cum_angle[correspondingIndex_onPath]                                 \n",
        "    angles_to_monkey[angles_to_monkey > pi] = angles_to_monkey[angles_to_monkey > pi] - 2*pi\n",
        "    angles_to_monkey[angles_to_monkey < -pi] = angles_to_monkey[angles_to_monkey < -pi] + 2*pi\n",
        "    angles_adjusted = np.absolute(angles_to_monkey)-np.abs(np.arcsin(np.divide(25, np.maximum(distances_to_monkey, 25) ))) # use torch clip to get valid arcsin input\n",
        "    angles_adjusted = np.clip(angles_adjusted, 0, pi)\n",
        "    angles_adjusted = np.sign(angles_to_monkey)* angles_adjusted\n",
        "\n",
        "    correspondingIndex_onPath_angle = correspondingIndex_onPath[np.where(np.absolute(angles_adjusted) <= 2*pi/9)[0]]\n",
        "    indices_onoff_lines = indices_onoff_lines + correspondingIndex_onPath_angle.tolist()\n",
        "  return indices_onoff_lines\n",
        "\n",
        "# To call:\n",
        "# indices_onoff_lines = onoff_lines_rightangle(cum_t, cum_mx, cum_my, cum_angle, currentTrial, duration, ff_real_position_sorted, ff_flash_sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebLJI9M79VHb"
      },
      "source": [
        "### **PlotTrials** (for LSTM)\n",
        "\n",
        "Among other things, here I eliminate the condition \"ff_distance\" < 250"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8ay-KOTNzhn"
      },
      "outputs": [],
      "source": [
        "def PlotTrials(currentTrial,\n",
        "                num_trials, \n",
        "                trail_color = \"orange\", # \"orange\" or \"viridis\" or None\n",
        "                show_reward_boundary = False,\n",
        "                show_stops = False,\n",
        "                show_colorbar = False, \n",
        "                show_believed_target_positions = False,\n",
        "                show_connect_path_target = np.array([]), # np.array([]) or target_nums\n",
        "                show_connect_path_pre_target = np.array([]), # np.array([]) or target_nums\n",
        "                show_connect_path_ff = np.array([]),\n",
        "                trial_to_show_cluster = None, # None, 0, or -1\n",
        "                show_scale_bar = False,\n",
        "                trial_to_show_cluster_around_target = None,\n",
        "                cluster_on_off_lines = False,\n",
        "                show_start = True,\n",
        "                #target_nums = \"default\", \n",
        "                ):\n",
        "  \n",
        "\n",
        "\n",
        "    cum_mxy_rotate = np.matmul(R, np.stack((cum_mx, cum_my)))\n",
        "    if trail_color == \"orange\":\n",
        "      axes.scatter(cum_mxy_rotate[0], cum_mxy_rotate[1],marker = 'o',s = 70, color=\"orange\", zorder=2)\n",
        "    elif trail_color == \"viridis\":\n",
        "      axes.scatter(cum_mxy_rotate[0], cum_mxy_rotate[1],marker = 'o',s = 70, c = cum_speed, zorder=2)\n",
        "    else:\n",
        "      axes.scatter(cum_mxy_rotate[0], cum_mxy_rotate[1],marker = 'o',s = 70, color = \"yellow\", zorder=2)\n",
        "\n",
        "    if show_start:\n",
        "      # Plot the start\n",
        "      axes.scatter(cum_mxy_rotate[0,0], cum_mxy_rotate[1,0],marker = '^',s = 220, color=\"gold\", zorder=3, alpha=0.5)\n",
        "\n",
        "\n",
        "\n",
        "    if show_stops:\n",
        "      zerospeed_index = np.where(cum_speeddummy==0)\n",
        "      zerospeedx, zerospeedy = cum_mx[zerospeed_index], cum_my[zerospeed_index]\n",
        "      zerospeed_rotate = np.matmul(R, np.stack((zerospeedx, zerospeedy)))\n",
        "      axes.scatter(zerospeed_rotate[0], zerospeed_rotate[1],marker = '*',s = 160, alpha = 0.7, color=\"black\", zorder=2)\n",
        "\n",
        "    ff_position_rotate = np.matmul(R, np.stack((ff_position_during_this_trial.T[0], ff_position_during_this_trial.T[1])))\n",
        "    axes.scatter(ff_position_rotate[0], ff_position_rotate[1], marker='o', s=10, color=\"magenta\",  zorder=2)\n",
        "\n",
        "    if show_believed_target_positions:\n",
        "      ff_believed_position_rotate = np.matmul(R, np.stack((ff_believed_position_sorted[currentTrial-num_trials+1:currentTrial+1].T[0], ff_believed_position_sorted[currentTrial-num_trials+1:currentTrial+1].T[1])))\n",
        "      axes.scatter(ff_believed_position_rotate[0], ff_believed_position_rotate[1], marker = '*',s=185, color=\"red\", alpha=0.75, zorder=2)\n",
        "    \n",
        "    if show_reward_boundary:\n",
        "      for i in ff_position_rotate.T:\n",
        "        circle2 = plt.Circle((i[0], i[1]), 25, facecolor='grey', edgecolor='orange', alpha=0.45, zorder=1)\n",
        "        axes.add_patch(circle2)\n",
        "\n",
        "    if trial_to_show_cluster != None:\n",
        "    # Find the indices of ffs in the cluster\n",
        "      cluster_indices = np.unique(cluster_dataframe_point[cluster_dataframe_point['target_index']==currentTrial+trial_to_show_cluster].ff_index.to_numpy())\n",
        "      cluster_ff_positions = ff_real_position_sorted[np.array(cluster_indices)]\n",
        "      cluster_ff_rotate = np.matmul(R, np.stack((cluster_ff_positions.T[0], cluster_ff_positions.T[1])))\n",
        "      axes.scatter(cluster_ff_rotate[0], cluster_ff_rotate[1], marker='o', c = \"blue\", s=25, zorder = 4) \n",
        "\n",
        "    if show_connect_path_target.any():\n",
        "      xy_onoff_lines = ff_dataframe.loc[ff_dataframe['target_index'].isin(show_connect_path_target)]\n",
        "      xy_onoff_lines = np.array(xy_onoff_lines.loc[(xy_onoff_lines['ff_index']==currentTrial) & (xy_onoff_lines['visible']==1)][['monkey_x', 'monkey_y']])\n",
        "      onoff_lines_rotate = np.matmul(R, xy_onoff_lines.T)\n",
        "      axes.scatter(onoff_lines_rotate[0], onoff_lines_rotate[1], s=50, c=\"green\", alpha=0.8, zorder=5) \n",
        "\n",
        "    if show_connect_path_pre_target.any():\n",
        "      xy_onoff_lines = ff_dataframe.loc[ff_dataframe['target_index'].isin(show_connect_path_pre_target)]\n",
        "      xy_onoff_lines = np.array(xy_onoff_lines.loc[(xy_onoff_lines['ff_index']==currentTrial-1) & (xy_onoff_lines['visible']==1)][['monkey_x', 'monkey_y']])\n",
        "      onoff_lines_rotate = np.matmul(R, xy_onoff_lines.T)\n",
        "      axes.scatter(onoff_lines_rotate[0], onoff_lines_rotate[1], s=65, c=\"aqua\",  alpha=0.8, zorder=3)\n",
        "\n",
        "    if show_connect_path_ff.any():\n",
        "      temp_dataframe = ff_dataframe.loc[ff_dataframe['target_index'].isin(show_connect_path_ff)]\n",
        "      temp_dataframe = temp_dataframe.loc[(temp_dataframe['visible']==1)][['ff_x', 'ff_y', 'monkey_x', 'monkey_y']]\n",
        "      #temp_dataframe = temp_dataframe.loc[~temp_dataframe['ff_index'].isin(target_nums)]\n",
        "      temp_array = temp_dataframe.to_numpy()\n",
        "      temp_ff_positions = np.matmul(R, temp_array[:,:2].T)\n",
        "      temp_monkey_positions = np.matmul(R, temp_array[:,2:].T)\n",
        "      for j in range(len(temp_array)):\n",
        "        axes.plot(np.stack([temp_ff_positions[0,j], temp_monkey_positions[0,j]]), np.stack([temp_ff_positions[1,j], temp_monkey_positions[1,j]]), '-', alpha=0.3, linewidth=1.5, c=\"#a940f5\")\n",
        "        #axes.plot(temp_ff_positions[0,j], temp_ff_positions[1,j], '-', alpha=0.2, marker=\"o\", markersize=5, color=\"brown\")\n",
        "\n",
        "    # if show_connect_path_ff.any():\n",
        "    #   if num_trials == 1:\n",
        "    #     x, y = connect_path_ff(cum_t, cum_mx, cum_my, cum_angle, currentTrial, 400, ff_flash_sorted, ff_real_position_sorted, total_ff_num)\n",
        "    #   else:\n",
        "    #     target_nums = np.arange(currentTrial-num_trials+1, currentTrial+1)\n",
        "    #     x, y = connect_path_ff2(cum_t, cum_mx, cum_my, cum_angle, target_nums, 400, ff_flash_sorted, ff_real_position_sorted, total_ff_num)\n",
        "    #   for j in range(len(x)):\n",
        "    #     xy_rotate = np.matmul(R, np.stack((x[j], y[j])))\n",
        "    #     axes.plot(xy_rotate[0], xy_rotate[1], '-', alpha=0.2, c=\"#a940f5\")\n",
        "    #     axes.plot(xy_rotate[0][0], xy_rotate[1][0], '-', alpha=0.2, marker=\"o\", markersize=5, color=\"brown\")\n",
        "\n",
        "    if trial_to_show_cluster_around_target != None:\n",
        "      cluster_ff_pos = ffs_around_target_positions[currentTrial+trial_to_show_cluster_around_target]\n",
        "      if len(cluster_ff_pos) > 0:\n",
        "        ffs_around_target_rotate = np.matmul(R, np.stack((cluster_ff_pos.T[0], cluster_ff_pos.T[1])))\n",
        "        axes.scatter(ffs_around_target_rotate[0],ffs_around_target_rotate[1], marker='o', s=30, color=\"blue\", zorder=4) \n",
        "      if cluster_on_off_lines:\n",
        "        # Find on_off_lines for ffs in the cluster\n",
        "        for i in range(len(cluster_ff_pos)):\n",
        "          index = np.array(ff_dataframe[(np.isclose(np.array(ff_dataframe['ff_x']),cluster_ff_pos[i, 0])) & (np.isclose(np.array(ff_dataframe['ff_y']),cluster_ff_pos[i, 1]))]['ff_index'])\n",
        "          if len(index) > 0:\n",
        "            index = index[0]\n",
        "            #index = ffs_around_target_indices[currentTrial-trial_to_show_cluster_around_target][i]\n",
        "            xy_onoff_lines = ff_dataframe.loc[(ff_dataframe['time']>= duration[0])&(ff_dataframe['time']<= duration[1])]\n",
        "            xy_onoff_lines = xy_onoff_lines.loc[(xy_onoff_lines['ff_index']==index) & (xy_onoff_lines['visible']==1)]\n",
        "            xy_onoff_lines2 = np.array(xy_onoff_lines[['monkey_x', 'monkey_y']])\n",
        "            onoff_lines_rotate = np.matmul(R, xy_onoff_lines2.T)\n",
        "            axes.scatter(onoff_lines_rotate[0], onoff_lines_rotate[1], s=80-10*i, color=list_of_colors[i], alpha=0.8, zorder=3+i) \n",
        "            # Use corresponding color for that ff\n",
        "            xy_onoff_lines3 = np.array(xy_onoff_lines[['ff_x', 'ff_y']]) \n",
        "            ffs_around_target_rotate = np.matmul(R, xy_onoff_lines3.T)\n",
        "            axes.scatter(ffs_around_target_rotate[0],ffs_around_target_rotate[1], marker='o', s=140, alpha = 0.8, color=list_of_colors[i], zorder=3) \n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    if show_scale_bar:\n",
        "      scale1 = ScaleBar(  \n",
        "      dx=1, length_fraction=0.2, fixed_value=100,\n",
        "          location='upper left',  # in relation to the whole plot\n",
        "          label_loc='left', scale_loc='bottom'  # in relation to the line\n",
        "      )\n",
        "      axes.add_artist(scale1)\n",
        "      axes.xaxis.set_major_locator(ticker.NullLocator())\n",
        "      axes.yaxis.set_major_locator(ticker.NullLocator())\n",
        "\n",
        "    xmin, xmax = np.min(cum_mxy_rotate[0]), np.max(cum_mxy_rotate[0])\n",
        "    ymin, ymax = np.min(cum_mxy_rotate[1]), np.max(cum_mxy_rotate[1])\n",
        "    bigger_width = max(xmax-xmin, ymax-ymin)\n",
        "    xmiddle, ymiddle = (xmin+xmax)/2, (ymin+ymax)/2\n",
        "    xmin, xmax = xmiddle-bigger_width/2, xmiddle+bigger_width/2\n",
        "    ymin, ymax = ymiddle-bigger_width/2, ymiddle+bigger_width/2\n",
        "    margin = max(bigger_width/5, 150)\n",
        "    axes.set_xlim((xmin-margin, xmax+margin))\n",
        "    axes.set_ylim((ymin-margin, ymax+margin))\n",
        "    axes.set_aspect('equal')\n",
        "\n",
        "\n",
        "    if show_colorbar == True:\n",
        "      # Make the black and red colorbar\n",
        "      A = np.reshape([1,2,1,2,2,2], (2,3))# The numbers don't matter much\n",
        "      norm_bins = np.array([0.5, 1.5, 2.5])\n",
        "      # Let's also design our color mapping: 1s should be plotted in blue, 2s in red, etc...\n",
        "      col_dict={1:\"black\",2:\"red\"}\n",
        "      # We create a colormar from our list of colors\n",
        "      speed_cm = ListedColormap([col_dict[x] for x in col_dict.keys()])\n",
        "      ## Make normalizer and formatter\n",
        "      norm = matplotlib.colors.BoundaryNorm(norm_bins, 2, clip=True)\n",
        "      labels = np.array([\"No Reward\", \"Reward\"])\n",
        "      fmt = matplotlib.ticker.FuncFormatter(lambda x, pos: labels[norm(x)])\n",
        "      # Plot our figure\n",
        "      im = axes.imshow(A, cmap=speed_cm, extent=[0,0,0,0], norm=norm)\n",
        "      cax2 = fig.add_axes([0.95, 0.15, 0.05, 0.2])\n",
        "      cb = fig.colorbar(im, format=fmt, ticks=np.array([1., 2.]), cax=cax2)\n",
        "      cb.ax.tick_params(width=0)\n",
        "      cb.ax.set_title('Stopping Points', ha='left')\n",
        "      if trail_color == \"orange\":\n",
        "        A = np.reshape([1,2,1,2,2,2], (2,3))# The numbers don't matter much\n",
        "        norm_bins = np.array([0.5, 1.5, 2.5])\n",
        "        # Let's also design our color mapping: 1s should be plotted in blue, 2s in red, etc...\n",
        "        col_dict={1:\"green\",2:\"orange\"}\n",
        "        # We create a colormar from our list of colors\n",
        "        speed_cm = ListedColormap([col_dict[x] for x in col_dict.keys()])\n",
        "        ## Make normalizer and formatter\n",
        "        norm = matplotlib.colors.BoundaryNorm(norm_bins, 2, clip=True)\n",
        "        labels = np.array([\"Top Target Visible\", \"Top Target Not Visible\"])\n",
        "        fmt = matplotlib.ticker.FuncFormatter(lambda x, pos: labels[norm(x)])\n",
        "        # Plot our figure\n",
        "        im = axes.imshow(A, cmap=speed_cm, extent=[0,0,0,0], norm=norm)\n",
        "        cax2 = fig.add_axes([0.95, 0.5, 0.05, 0.2])\n",
        "        cb = fig.colorbar(im, format=fmt, ticks=np.array([1., 2.]), cax=cax2)\n",
        "        cb.ax.tick_params(width=0)\n",
        "        cb.ax.set_title('Path Colors', ha='left', y=1.04)\n",
        "      elif trail_color == \"viridis\":\n",
        "        cmap = cm.viridis\n",
        "        norm = matplotlib.colors.Normalize(vmin=0, vmax=200)\n",
        "        cax = fig.add_axes([0.95, 0.4, 0.05, 0.43])\n",
        "        cbar = fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap),\n",
        "                  cax=cax, orientation='vertical')\n",
        "        cbar.ax.set_title('Speed(cm/s)', ha='left', y=1.04)\n",
        "        cbar.ax.tick_params(axis='y', color='white', direction=\"in\", right=True,length=5, width=1.5)\n",
        "        cbar.outline.remove()\n",
        "      global Show_Colorbar \n",
        "      Show_Colorbar = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1dpCJWwuf2Y"
      },
      "source": [
        "### **PlotTrials** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOZXSnT2uf2Z"
      },
      "outputs": [],
      "source": [
        "def PlotTrials(currentTrial,\n",
        "                num_trials, \n",
        "                trail_color = \"orange\", # \"orange\" or \"viridis\" or None\n",
        "                show_reward_boundary = False,\n",
        "                show_stops = False,\n",
        "                show_colorbar = False, \n",
        "                show_believed_target_positions = False,\n",
        "                show_connect_path_target = np.array([]), # np.array([]) or target_nums\n",
        "                show_connect_path_pre_target = np.array([]), # np.array([]) or target_nums\n",
        "                show_connect_path_ff = np.array([]),\n",
        "                trial_to_show_cluster = None, # None, 0, or -1\n",
        "                show_scale_bar = False,\n",
        "                show_start = False,\n",
        "                trial_to_show_cluster_around_target = None,\n",
        "                cluster_on_off_lines = False,\n",
        "                #target_nums = \"default\", \n",
        "                ):\n",
        "   \n",
        "\n",
        "    cum_mxy_rotate = np.matmul(R, np.stack((cum_mx, cum_my)))\n",
        "    if trail_color == \"orange\":\n",
        "      axes.scatter(cum_mxy_rotate[0], cum_mxy_rotate[1],marker = 'o',s = 10, color=\"orange\", zorder=2)\n",
        "    elif trail_color == \"viridis\":\n",
        "      axes.scatter(cum_mxy_rotate[0], cum_mxy_rotate[1],marker = 'o',s = 10, c = cum_speed, zorder=2)\n",
        "    else:\n",
        "      axes.scatter(cum_mxy_rotate[0], cum_mxy_rotate[1],marker = 'o',s = 10, color = \"yellow\", zorder=2)\n",
        "\n",
        "    if show_start:\n",
        "      # Plot the start\n",
        "      axes.scatter(cum_mxy_rotate[0,0], cum_mxy_rotate[1,0],marker = 'o',s = 100, color=\"purple\", zorder=3, alpha=0.5)\n",
        "\n",
        "    if show_stops:\n",
        "      zerospeed_index = np.where(cum_speeddummy==0)\n",
        "      zerospeedx, zerospeedy = cum_mx[zerospeed_index], cum_my[zerospeed_index]\n",
        "      zerospeed_rotate = np.matmul(R, np.stack((zerospeedx, zerospeedy)))\n",
        "      axes.scatter(zerospeed_rotate[0], zerospeed_rotate[1],marker = '*',s = 150, color=\"black\", zorder=2)\n",
        "\n",
        "    ff_position_rotate = np.matmul(R, np.stack((ff_position_during_this_trial.T[0], ff_position_during_this_trial.T[1])))\n",
        "    axes.scatter(ff_position_rotate[0], ff_position_rotate[1], marker='o', s=10, color=\"magenta\",  zorder=2)\n",
        "\n",
        "    if show_believed_target_positions:\n",
        "      ff_believed_position_rotate = np.matmul(R, np.stack((ff_believed_position_sorted[currentTrial-num_trials+1:currentTrial+1].T[0], ff_believed_position_sorted[currentTrial-num_trials+1:currentTrial+1].T[1])))\n",
        "      axes.scatter(ff_believed_position_rotate[0], ff_believed_position_rotate[1], marker = '*',s=120, color=\"red\", alpha=0.75, zorder=2)\n",
        "    \n",
        "    if show_reward_boundary:\n",
        "      for i in ff_position_rotate.T:\n",
        "        circle2 = plt.Circle((i[0], i[1]), 25, facecolor='grey', edgecolor='orange', alpha=0.45, zorder=1)\n",
        "        axes.add_patch(circle2)\n",
        "\n",
        "    if trial_to_show_cluster != None:\n",
        "    # Find the indices of ffs in the cluster\n",
        "      cluster_indices = np.unique(cluster_dataframe_point[cluster_dataframe_point['target_index']==currentTrial+trial_to_show_cluster].ff_index.to_numpy())\n",
        "      cluster_ff_positions = ff_real_position_sorted[np.array(cluster_indices)]\n",
        "      cluster_ff_rotate = np.matmul(R, np.stack((cluster_ff_positions.T[0], cluster_ff_positions.T[1])))\n",
        "      axes.scatter(cluster_ff_rotate[0], cluster_ff_rotate[1], marker='o', c = \"blue\", s=25, zorder = 4) \n",
        "\n",
        "    if show_connect_path_target.any():\n",
        "      xy_onoff_lines = ff_dataframe.loc[ff_dataframe['target_index'].isin(show_connect_path_target)]\n",
        "      xy_onoff_lines = np.array(xy_onoff_lines.loc[(xy_onoff_lines['ff_index']==currentTrial) & (xy_onoff_lines['visible']==1)&(xy_onoff_lines['ff_distance']<250)][['monkey_x', 'monkey_y']])\n",
        "      onoff_lines_rotate = np.matmul(R, xy_onoff_lines.T)\n",
        "      axes.scatter(onoff_lines_rotate[0], onoff_lines_rotate[1], s=30, c=\"green\", alpha=0.4, zorder=4) \n",
        "\n",
        "    if show_connect_path_pre_target.any():\n",
        "      xy_onoff_lines = ff_dataframe.loc[ff_dataframe['target_index'].isin(show_connect_path_pre_target)]\n",
        "      xy_onoff_lines = np.array(xy_onoff_lines.loc[(xy_onoff_lines['ff_index']==currentTrial-1) & (xy_onoff_lines['visible']==1)&(xy_onoff_lines['ff_distance']<250)][['monkey_x', 'monkey_y']])\n",
        "      onoff_lines_rotate = np.matmul(R, xy_onoff_lines.T)\n",
        "      axes.scatter(onoff_lines_rotate[0], onoff_lines_rotate[1], s=40, c=\"aqua\",  alpha=0.6, zorder=3)\n",
        "\n",
        "    if show_connect_path_ff.any():\n",
        "      target_nums = np.arange(currentTrial-num_trials+1, currentTrial+1)\n",
        "      temp_dataframe = ff_dataframe.loc[ff_dataframe['target_index'].isin(target_nums)]\n",
        "      temp_dataframe = temp_dataframe.loc[(temp_dataframe['ff_distance']<250) & (temp_dataframe['visible']==1)]\n",
        "      temp_dataframe = temp_dataframe.loc[~temp_dataframe['ff_index'].isin(target_nums)][['ff_x', 'ff_y', 'monkey_x', 'monkey_y']]\n",
        "      temp_array = temp_dataframe.to_numpy()\n",
        "      temp_ff_positions = np.matmul(R, temp_array[:,:2].T)\n",
        "      temp_monkey_positions = np.matmul(R, temp_array[:,2:].T)\n",
        "      for j in range(len(temp_array)):\n",
        "        axes.plot(np.stack([temp_ff_positions[0,j], temp_monkey_positions[0,j]]), np.stack([temp_ff_positions[1,j], temp_monkey_positions[1,j]]), '-', alpha=0.2, c=\"#a940f5\")\n",
        "        axes.plot(temp_ff_positions[0,j], temp_ff_positions[1,j], '-', alpha=0.2, marker=\"o\", markersize=5, color=\"brown\")\n",
        "\n",
        "    # if show_connect_path_ff.any():\n",
        "    #   if num_trials == 1:\n",
        "    #     x, y = connect_path_ff(cum_t, cum_mx, cum_my, cum_angle, currentTrial, 250, ff_flash_sorted, ff_real_position_sorted, total_ff_num)\n",
        "    #   else:\n",
        "    #     target_nums = np.arange(currentTrial-num_trials+1, currentTrial+1)\n",
        "    #     x, y = connect_path_ff2(cum_t, cum_mx, cum_my, cum_angle, target_nums, 250, ff_flash_sorted, ff_real_position_sorted, total_ff_num)\n",
        "    #   for j in range(len(x)):\n",
        "    #     xy_rotate = np.matmul(R, np.stack((x[j], y[j])))\n",
        "    #     axes.plot(xy_rotate[0], xy_rotate[1], '-', alpha=0.2, c=\"#a940f5\")\n",
        "    #     axes.plot(xy_rotate[0][0], xy_rotate[1][0], '-', alpha=0.2, marker=\"o\", markersize=5, color=\"brown\")\n",
        "\n",
        "\n",
        "    if trial_to_show_cluster_around_target != None:\n",
        "      cluster_ff_pos = ffs_around_target_positions[currentTrial+trial_to_show_cluster_around_target]\n",
        "      if len(cluster_ff_pos) > 0:\n",
        "        ffs_around_target_rotate = np.matmul(R, np.stack((cluster_ff_pos.T[0], cluster_ff_pos.T[1])))\n",
        "        axes.scatter(ffs_around_target_rotate[0],ffs_around_target_rotate[1], marker='o', s=30, color=\"blue\", zorder=4) \n",
        "      if cluster_on_off_lines:\n",
        "        # Find on_off_lines for ffs in the cluster\n",
        "        for i in range(len(cluster_ff_pos)):\n",
        "          index = np.array(ff_dataframe[(np.isclose(np.array(ff_dataframe['ff_x']),cluster_ff_pos[i, 0])) & (np.isclose(np.array(ff_dataframe['ff_y']),cluster_ff_pos[i, 1]))]['ff_index'])\n",
        "          if len(index) > 0:\n",
        "            index = index[0]\n",
        "            #index = ffs_around_target_indices[currentTrial-trial_to_show_cluster_around_target][i]\n",
        "            xy_onoff_lines = ff_dataframe.loc[ff_dataframe['target_index']==currentTrial]\n",
        "            xy_onoff_lines2 = np.array(xy_onoff_lines.loc[(xy_onoff_lines['ff_index']==index) & (xy_onoff_lines['visible']==1)][['monkey_x', 'monkey_y']])\n",
        "            onoff_lines_rotate = np.matmul(R, xy_onoff_lines2.T)\n",
        "            axes.scatter(onoff_lines_rotate[0], onoff_lines_rotate[1], s=15-3*i, color=list_of_colors[i], alpha=0.4, zorder=3+i) \n",
        "            # Use corresponding color for that ff\n",
        "            xy_onoff_lines3 = np.array(xy_onoff_lines[['ff_x', 'ff_y']]) \n",
        "            ffs_around_target_rotate = np.matmul(R, xy_onoff_lines3.T)\n",
        "            axes.scatter(ffs_around_target_rotate[0],ffs_around_target_rotate[1], marker='o', s=100, alpha = 0.5, color=list_of_colors[i], zorder=3) \n",
        "\n",
        "\n",
        "\n",
        "    if show_scale_bar:\n",
        "      scale1 = ScaleBar(  \n",
        "      dx=1, length_fraction=0.2, fixed_value=100,\n",
        "          location='upper left',  # in relation to the whole plot\n",
        "          label_loc='left', scale_loc='bottom'  # in relation to the line\n",
        "      )\n",
        "      axes.add_artist(scale1)\n",
        "      axes.xaxis.set_major_locator(ticker.NullLocator())\n",
        "      axes.yaxis.set_major_locator(ticker.NullLocator())\n",
        "\n",
        "    xmin, xmax = np.min(cum_mxy_rotate[0]), np.max(cum_mxy_rotate[0])\n",
        "    ymin, ymax = np.min(cum_mxy_rotate[1]), np.max(cum_mxy_rotate[1])\n",
        "    bigger_width = max(xmax-xmin, ymax-ymin)\n",
        "    xmiddle, ymiddle = (xmin+xmax)/2, (ymin+ymax)/2\n",
        "    xmin, xmax = xmiddle-bigger_width/2, xmiddle+bigger_width/2\n",
        "    ymin, ymax = ymiddle-bigger_width/2, ymiddle+bigger_width/2\n",
        "    margin = max(bigger_width/5, 150)\n",
        "    axes.set_xlim((xmin-margin, xmax+margin))\n",
        "    axes.set_ylim((ymin-margin, ymax+margin))\n",
        "    axes.set_aspect('equal')\n",
        "\n",
        "\n",
        "    if show_colorbar == True:\n",
        "      # Make the black and red colorbar\n",
        "      A = np.reshape([1,2,1,2,2,2], (2,3))# The numbers don't matter much\n",
        "      norm_bins = np.array([0.5, 1.5, 2.5])\n",
        "      # Let's also design our color mapping: 1s should be plotted in blue, 2s in red, etc...\n",
        "      col_dict={1:\"black\",2:\"red\"}\n",
        "      # We create a colormar from our list of colors\n",
        "      speed_cm = ListedColormap([col_dict[x] for x in col_dict.keys()])\n",
        "      ## Make normalizer and formatter\n",
        "      norm = matplotlib.colors.BoundaryNorm(norm_bins, 2, clip=True)\n",
        "      labels = np.array([\"No Reward\", \"Reward\"])\n",
        "      fmt = matplotlib.ticker.FuncFormatter(lambda x, pos: labels[norm(x)])\n",
        "      # Plot our figure\n",
        "      im = axes.imshow(A, cmap=speed_cm, extent=[0,0,0,0], norm=norm)\n",
        "      cax2 = fig.add_axes([0.95, 0.15, 0.05, 0.2])\n",
        "      cb = fig.colorbar(im, format=fmt, ticks=np.array([1., 2.]), cax=cax2)\n",
        "      cb.ax.tick_params(width=0)\n",
        "      cb.ax.set_title('Stopping Points', ha='left')\n",
        "      if trail_color == \"orange\":\n",
        "        A = np.reshape([1,2,1,2,2,2], (2,3))# The numbers don't matter much\n",
        "        norm_bins = np.array([0.5, 1.5, 2.5])\n",
        "        # Let's also design our color mapping: 1s should be plotted in blue, 2s in red, etc...\n",
        "        col_dict={1:\"green\",2:\"orange\"}\n",
        "        # We create a colormar from our list of colors\n",
        "        speed_cm = ListedColormap([col_dict[x] for x in col_dict.keys()])\n",
        "        ## Make normalizer and formatter\n",
        "        norm = matplotlib.colors.BoundaryNorm(norm_bins, 2, clip=True)\n",
        "        labels = np.array([\"Top Target Visible\", \"Top Target Not Visible\"])\n",
        "        fmt = matplotlib.ticker.FuncFormatter(lambda x, pos: labels[norm(x)])\n",
        "        # Plot our figure\n",
        "        im = axes.imshow(A, cmap=speed_cm, extent=[0,0,0,0], norm=norm)\n",
        "        cax2 = fig.add_axes([0.95, 0.5, 0.05, 0.2])\n",
        "        cb = fig.colorbar(im, format=fmt, ticks=np.array([1., 2.]), cax=cax2)\n",
        "        cb.ax.tick_params(width=0)\n",
        "        cb.ax.set_title('Path Colors', ha='left', y=1.04)\n",
        "      elif trail_color == \"viridis\":\n",
        "        cmap = cm.viridis\n",
        "        norm = matplotlib.colors.Normalize(vmin=0, vmax=200)\n",
        "        cax = fig.add_axes([0.95, 0.4, 0.05, 0.43])\n",
        "        cbar = fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap),\n",
        "                  cax=cax, orientation='vertical')\n",
        "        cbar.ax.set_title('Speed(cm/s)', ha='left', y=1.04)\n",
        "        cbar.ax.tick_params(axis='y', color='white', direction=\"in\", right=True,length=5, width=1.5)\n",
        "        cbar.outline.remove()\n",
        "      global Show_Colorbar \n",
        "      Show_Colorbar = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74Z9BuCF-gPA"
      },
      "source": [
        "### PlotPoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StkcReorPeC1"
      },
      "outputs": [],
      "source": [
        "def PlotPoints(point,\n",
        "                total_time, \n",
        "                show_all_ff,\n",
        "                show_flash_on_ff,\n",
        "                show_visible_ff,\n",
        "                show_in_memory_ff, \n",
        "                show_target,\n",
        "                show_reward_boundary,\n",
        "                show_legend,\n",
        "                show_colorbar, \n",
        "                show_scale_bar,\n",
        "                trial_num=None,\n",
        "                **kwargs):\n",
        "\n",
        "\n",
        "          \n",
        "  \n",
        "    alive_ff_indices= np.array([i for i,value in \n",
        "                               enumerate(ff_life_sorted) if (value[-1]>=time) and (value[0]<time)]) \n",
        "    alive_ff_positions = ff_real_position_sorted[alive_ff_indices]\n",
        "    if show_all_ff:\n",
        "      axes.scatter(alive_ff_positions.T[0], alive_ff_positions.T[1], color=\"grey\", s=30)\n",
        "\n",
        "    if show_flash_on_ff:\n",
        "      on_ff_indices = [] # Gives the indices of the ffs that are on at this point\n",
        "      # For each firefly in ff_flash_sorted:\n",
        "      for index, firefly in enumerate(ff_flash_sorted):\n",
        "        # If the firefly has flashed during that trial:\n",
        "        if index in alive_ff_indices: \n",
        "            # Let's see if the firefly has flashed at that exact moment\n",
        "            for interval in firefly:\n",
        "              if interval[0] <= time <= interval[1]:\n",
        "                on_ff_indices.append(index) \n",
        "                break  \n",
        "      on_ff_indices = np.array(on_ff_indices) \n",
        "      on_ff_positions = ff_real_position_sorted[on_ff_indices]\n",
        "      axes.scatter(on_ff_positions.T[0], on_ff_positions.T[1], color=\"red\", s=120, marker = '*', alpha = 0.7)\n",
        "    \n",
        "    if show_visible_ff:\n",
        "      visible_ffs = ff_dataframe[(ff_dataframe['point_index']==point)&(ff_dataframe['visible']==1)][['ff_x', 'ff_y']]\n",
        "      axes.scatter(visible_ffs['ff_x'], visible_ffs['ff_y'], color=\"orange\", s=40)\n",
        "\n",
        "    if show_in_memory_ff:\n",
        "      in_memory_ffs = ff_dataframe[(ff_dataframe['point_index']==point)&(ff_dataframe['visible']==0)][['ff_x', 'ff_y']]\n",
        "      axes.scatter(in_memory_ffs['ff_x'], in_memory_ffs['ff_y'], color=\"green\", s=40)\n",
        "    \n",
        "    if show_target:\n",
        "      if trial_num == None:\n",
        "        raise ValueError(\"If show_target, then trial_num cannot be None\")\n",
        "      target_num  = distance_dataframe['trial'].iloc[point]\n",
        "      target_position = ff_real_position_sorted[trial_num]\n",
        "      axes.scatter(target_position[0], target_position[1], marker = '*', s=200, color=\"grey\", alpha=0.35)\n",
        "\n",
        "\n",
        "    if show_legend == True:\n",
        "      legend_names = []\n",
        "      if show_all_ff:\n",
        "        legend_names.append(\"Invisible\")\n",
        "      if show_flash_on_ff:\n",
        "        legend_names.append(\"Flash On\")\n",
        "      if show_visible_ff:\n",
        "        legend_names.append(\"Visible\")\n",
        "      if show_in_memory_ff:\n",
        "        legend_names.append(\"In memory\")\n",
        "      if show_target:\n",
        "        legend_names.append(\"Target\")\n",
        "      axes.legend(legend_names, loc='upper right')\n",
        "      global Show_Legend\n",
        "      Show_Legend = False\n",
        "\n",
        "    if show_reward_boundary:\n",
        "      if show_all_ff:\n",
        "        for i in range(len(alive_ff_positions)):\n",
        "          circle2 = plt.Circle((alive_ff_positions[i, 0], alive_ff_positions[i, 1]), 20, facecolor='grey', edgecolor='orange', alpha=0.25, zorder=1)\n",
        "          axes.add_patch(circle2)\n",
        "      elif show_flash_on_ff:\n",
        "        if show_flash_on_ff:\n",
        "          for i in range(len(on_ff_positions)):\n",
        "            circle2 = plt.Circle((on_ff_positions[i, 0], on_ff_positions[i, 1]), 20, facecolor='grey', edgecolor='orange', alpha=0.25, zorder=1)\n",
        "            axes.add_patch(circle2)\n",
        "        if show_in_memory_ff:\n",
        "          for i in range(len(in_memory_ffs)):\n",
        "            circle2 = plt.Circle((in_memory_ffs['ff_x'].iloc[i], in_memory_ffs['ff_y'].iloc[i]), 20, facecolor='grey', edgecolor='orange', alpha=0.25, zorder=1)\n",
        "            axes.add_patch(circle2) \n",
        "      elif show_visible_ff:\n",
        "        for i in range(len(visible_ffs)):\n",
        "          circle2 = plt.Circle((visible_ffs['ff_x'].iloc[i], visible_ffs['ff_y'].iloc[i]), 20, facecolor='grey', edgecolor='orange', alpha=0.25, zorder=1)\n",
        "          axes.add_patch(circle2)  \n",
        "        if show_in_memory_ff:\n",
        "          for i in range(len(in_memory_ffs)):\n",
        "            circle2 = plt.Circle((in_memory_ffs['ff_x'].iloc[i], in_memory_ffs['ff_y'].iloc[i]), 20, facecolor='grey', edgecolor='orange', alpha=0.25, zorder=1)\n",
        "            axes.add_patch(circle2)  \n",
        "\n",
        "    axes.scatter(cum_mx, cum_my, s=15, c=index_temp, cmap=\"Blues\") \n",
        "\n",
        "    xmin, xmax = np.min(cum_mx), np.max(cum_mx)\n",
        "    ymin, ymax = np.min(cum_my), np.max(cum_my)\n",
        "    bigger_width = max(xmax-xmin, ymax-ymin)\n",
        "    xmiddle, ymiddle = (xmin+xmax)/2, (ymin+ymax)/2\n",
        "    xmin, xmax = xmiddle-bigger_width/2, xmiddle+bigger_width/2\n",
        "    ymin, ymax = ymiddle-bigger_width/2, ymiddle+bigger_width/2\n",
        "    margin = max(bigger_width/5, 250)\n",
        "    axes.set_xlim((xmin-margin, xmax+margin))\n",
        "    axes.set_ylim((ymin-margin, ymax+margin))\n",
        "    axes.set_aspect('equal')\n",
        "\n",
        "    if show_scale_bar == True:\n",
        "      scale1 = ScaleBar(  \n",
        "      dx=1, length_fraction=0.2, fixed_value=100,\n",
        "          location='upper left',  # in relation to the whole plot\n",
        "          label_loc='left', scale_loc='bottom'  # in relation to the line\n",
        "      )\n",
        "      axes.add_artist(scale1)\n",
        "\n",
        "    axes.xaxis.set_major_locator(ticker.NullLocator())\n",
        "    axes.yaxis.set_major_locator(ticker.NullLocator()) \n",
        "\n",
        "    if show_colorbar == True:\n",
        "      cmap = cm.Blues\n",
        "      cax = fig.add_axes([0.95, 0.25, 0.05, 0.52]) #[left, bottom, width, height] \n",
        "      cbar = fig.colorbar(cm.ScalarMappable(cmap=cmap),ticks=[0, 1],\n",
        "                cax=cax, orientation='vertical')\n",
        "      cbar.ax.set_title('Trajectory', ha='left', y=1.07)\n",
        "      cbar.ax.tick_params(size = 0)\n",
        "      cbar.outline.remove()\n",
        "      cbar.ax.set_yticklabels(['Least recent',  'Most recent']) \n",
        "      global Show_Colorbar \n",
        "      Show_Colorbar = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ke0ba6i4RT9"
      },
      "source": [
        "### make ff_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZvHPIme4RT-"
      },
      "outputs": [],
      "source": [
        "def MakeFFDataframe(monkey_information, ff_catched_T_sorted, ff_flash_sorted,  ff_real_position_sorted, max_distance = 400, data_folder_name = None, num_missed_index = -1):\n",
        "  # Let's use data from monkey_information. But we shall cut off portion that is before the time of capturing the first target\n",
        "  monkey_t_array0 = np.array(monkey_information['monkey_t'])\n",
        "  monkey_x_array0 = np.array(monkey_information['monkey_x'])\n",
        "  monkey_y_array0 = np.array(monkey_information['monkey_y'])\n",
        "  monkey_angle_array0 = np.array(monkey_information['monkey_angle'])\n",
        "  if num_missed_index < 0:\n",
        "    valid_index = np.where(monkey_t_array0 > ff_catched_T_sorted[0])[0]\n",
        "    num_missed_index = valid_index[0]\n",
        "  monkey_t_array = monkey_t_array0[num_missed_index:]\n",
        "  monkey_x_array = monkey_x_array0[num_missed_index:]\n",
        "  monkey_y_array = monkey_y_array0[num_missed_index:]\n",
        "  monkey_angle_array = monkey_angle_array0[num_missed_index:]\n",
        "  index_array = np.array(range(len(monkey_t_array)))\n",
        "\n",
        "  #max_distance = 400\n",
        "  reward_boundary = 25\n",
        "  #max_time = 400\n",
        "  #max_time = ff_catched_T_sorted[1251] #replace with ff_catched_T_sorted[-1]\n",
        "  ff_index = []\n",
        "  point_index = []\n",
        "  time = []\n",
        "  target_index = []\n",
        "  ff_x = []\n",
        "  ff_y = []\n",
        "  monkey_x = []\n",
        "  monkey_y = []\n",
        "  visible = []\n",
        "  memory = []\n",
        "  ff_distance = []\n",
        "  ff_angle = []\n",
        "  ff_angle_boundary = []\n",
        "  left_right = []\n",
        "  num_captures = []\n",
        "  distance_closest_ff =[]\n",
        "  distance_2ndclosest_ff = []\n",
        "  num_ff_within = []\n",
        "  reward_distance = []\n",
        "  reward_angle = []\n",
        "  catched_ff_num = len(ff_catched_T_sorted) - 200\n",
        "  total_ff_num = len(ff_life_sorted)\n",
        "\n",
        "\n",
        "  for i in range(total_ff_num):\n",
        "    if i % 100 == 0:\n",
        "      print(i,\" out of \", total_ff_num)\n",
        "\n",
        "    # visible_indices contains the indices of the points when the ff is visible (within a suitable distance & at the right angle)\n",
        "    visible_indices = []\n",
        "    # Go through every visible duration of the same ff\n",
        "    ff_flash = ff_flash_sorted[i]\n",
        "    for j in range(len(ff_flash)):\n",
        "      visible_duration = ff_flash[j]\n",
        "      ## if visible_duration[0] < max_time:\n",
        "      # Find the corresponding monkey information:\n",
        "      cum_indices = np.where((monkey_t_array >= visible_duration[0]) & \n",
        "                          (monkey_t_array <= visible_duration[1]))[0]\n",
        "      cum_t, cum_angle = monkey_t_array[cum_indices], monkey_angle_array[cum_indices]\n",
        "      cum_mx, cum_my = monkey_x_array[cum_indices], monkey_y_array[cum_indices]\n",
        "      distances_to_monkey = LA.norm(np.stack([cum_mx, cum_my], axis=1)-ff_real_position_sorted[i], axis = 1)\n",
        "      valid_distance_indices = np.where(distances_to_monkey < max_distance)[0]\n",
        "      if len(valid_distance_indices) > 0:\n",
        "        angles_to_monkey = np.arctan2(ff_real_position_sorted[i,1]-cum_my[valid_distance_indices], ff_real_position_sorted[i,0]-cum_mx[valid_distance_indices])-cum_angle[valid_distance_indices]\n",
        "        angles_to_monkey[angles_to_monkey > pi] = angles_to_monkey[angles_to_monkey > pi] - 2*pi\n",
        "        angles_to_monkey[angles_to_monkey < -pi] = angles_to_monkey[angles_to_monkey < -pi] + 2*pi\n",
        "        angles_adjusted = np.absolute(angles_to_monkey)-np.abs(np.arcsin(np.divide(reward_boundary, np.maximum(distances_to_monkey[valid_distance_indices], reward_boundary) ))) # use torch clip to get valid arcsin input\n",
        "        angles_adjusted = np.clip(angles_adjusted, 0, pi)\n",
        "        angles_to_monkey = np.sign(angles_to_monkey)* angles_adjusted\n",
        "        overall_valid_indices = valid_distance_indices[np.where(np.absolute(angles_to_monkey) <= 2*pi/9)[0]]\n",
        "        visible_indices = visible_indices + cum_indices[overall_valid_indices].tolist()\n",
        "    visible_indices = np.array(visible_indices)\n",
        "\n",
        "\n",
        "\n",
        "    if len(visible_indices) > 0:\n",
        "      # Make a numpy array of points to denote memory, with 0 means being invisible. We also append 99 extra points after the last point    \n",
        "      memory_indices0 = np.zeros(visible_indices[-1]+100, dtype=int)\n",
        "      memory_indices0[visible_indices] = 100\n",
        "\n",
        "      if len(ff_catched_T_sorted)-1 >= i:\n",
        "        # Find the index of the time at which the ff is captured\n",
        "        last_live_time = np.where(monkey_t_array <= ff_catched_T_sorted[i])[0][-1]\n",
        "        # Truncate memory_indices0 based on that \n",
        "        memory_indices0 = memory_indices0[:last_live_time+1]\n",
        "\n",
        "      \n",
        "      # Iterate through memory_indices0 to make a new list to denote memory (replacing some 0s with other numbers based on time)\n",
        "      # We preserve the first element of memory_indices0. We also separate memory_indices and point_indices (denoted as final_indices)\n",
        "      memory_indices = [memory_indices0[0]]\n",
        "\n",
        "      for k in range(1, len(memory_indices0)):\n",
        "        if memory_indices0[k] == 0:\n",
        "          memory_indices.append(memory_indices[k-1]-1)\n",
        "        else: # Else, preserve the current value\n",
        "          memory_indices.append(memory_indices0[k])\n",
        "      memory_indices_array = np.array(memory_indices)\n",
        "\n",
        "      index_array0 = np.arange(len(memory_indices0))\n",
        "      if len(index_array0) > len(monkey_t_array):\n",
        "        max_index = len(monkey_t_array)\n",
        "        index_array0 = index_array0[:max_index]\n",
        "        memory_indices_array = memory_indices_array[index_array0]\n",
        "\n",
        "      in_memory_indices = np.where(memory_indices_array > 0)[0]\n",
        "      memory_indices_array = memory_indices_array[in_memory_indices]\n",
        "      index_array = index_array0[in_memory_indices]\n",
        "      in_memory_length = len(memory_indices_array)\n",
        "      memory_indices = memory_indices_array.tolist()\n",
        "      final_indices = index_array.tolist()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # Append the values for this ff; Using list is faster than np.append\n",
        "      ff_index = ff_index + [i]*in_memory_length\n",
        "      point_index = point_index+ [point + num_missed_index for point in final_indices]\n",
        "      relevant_time = monkey_t_array[index_array]\n",
        "      time = time+relevant_time.tolist()\n",
        "      target_index = target_index + np.digitize(relevant_time, ff_catched_T_sorted).tolist()\n",
        "      ff_x = ff_x + [(ff_real_position_sorted[i][0])]*in_memory_length\n",
        "      ff_y = ff_y + [(ff_real_position_sorted[i][1])]*in_memory_length\n",
        "      monkey_x = monkey_x + monkey_x_array[index_array].tolist()\n",
        "      monkey_y = monkey_y + monkey_y_array[index_array].tolist()\n",
        "      visible = visible + [1 if point ==100 else 0 for point in memory_indices]\n",
        "      memory = memory + memory_indices\n",
        "      monkey_xy_relevant = np.stack([monkey_x_array[index_array], monkey_y_array[index_array]],axis=1)\n",
        "      monkey_angle_relevant = monkey_angle_array[index_array]\n",
        "      ff_distance_relevant = LA.norm(monkey_xy_relevant-ff_real_position_sorted[i], axis=1)\n",
        "      ff_distance = ff_distance + ff_distance_relevant.tolist()\n",
        "      angles_to_monkey = np.arctan2(ff_real_position_sorted[i,1]-monkey_xy_relevant[:,1], ff_real_position_sorted[i,0]-monkey_xy_relevant[:,0]) - monkey_angle_relevant\n",
        "      angles_to_monkey[angles_to_monkey > pi] = angles_to_monkey[angles_to_monkey > pi] - 2*pi\n",
        "      angles_to_monkey[angles_to_monkey < -pi] = angles_to_monkey[angles_to_monkey < -pi] + 2*pi\n",
        "      angles_adjusted = np.absolute(angles_to_monkey)-np.abs(np.arcsin(np.divide(reward_boundary, np.maximum(ff_distance_relevant, reward_boundary) ))) # use torch clip to get valid arcsin input\n",
        "      angles_adjusted = np.clip(angles_adjusted, 0, pi)\n",
        "      angles_adjusted = np.sign(angles_to_monkey)* angles_adjusted\n",
        "      ff_angle = ff_angle + angles_to_monkey.tolist()\n",
        "      ff_angle_boundary = ff_angle_boundary + angles_adjusted.tolist()\n",
        "      left_right = left_right + (np.array(angles_to_monkey) > 0).astype(int).tolist()\n",
        "      # num_captures.append\n",
        "      # distance_closest_ff.append\n",
        "      # distance_2ndclosest_ff.append\n",
        "      # num_ff_within.append\n",
        "      # reward_distance.append\n",
        "      # reward_angle.append\n",
        "\n",
        "        \n",
        "  # Now let's create a dictionary of the lists\n",
        "  ff_dict = {'ff_index':ff_index, 'point_index':point_index, 'time':time, 'target_index':target_index,\n",
        "              'ff_x':ff_x, 'ff_y':ff_y, 'monkey_x':monkey_x, 'monkey_y':monkey_y, 'visible':visible,\n",
        "              'memory':memory, 'ff_distance':ff_distance, 'ff_angle':ff_angle, 'ff_angle_boundary': ff_angle_boundary, 'left_right':left_right}\n",
        "  ff_dataframe = pd.DataFrame(ff_dict)\n",
        "\n",
        "  if len(ff_catched_T_sorted) > 800:\n",
        "    ff_dataframe = ff_dataframe[ff_dataframe['time'] < ff_catched_T_sorted[-200]]\n",
        "\n",
        "  if data_folder_name:\n",
        "    filepath = 'gdrive/MyDrive/fireflies_data/' + data_folder_name + '/ff_dataframe.csv'\n",
        "    os.makedirs('gdrive/MyDrive/fireflies_data/' + data_folder_name, exist_ok = True)\n",
        "    ff_dataframe.to_csv(filepath) \n",
        "\n",
        "  return ff_dataframe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMKjwgTJiVnj"
      },
      "source": [
        "### make ff_dataframe (agent)\n",
        "(only include ffs that are in obs space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81puFoWdiIiu"
      },
      "outputs": [],
      "source": [
        "def MakeFFDataframe(monkey_information, ff_catched_T_sorted, ff_flash_sorted,  ff_real_position_sorted, max_distance = 400, data_folder_name = None, num_missed_index = -1):\n",
        "  # Let's use data from monkey_information. But we shall cut off portion that is before the time of capturing the first target\n",
        "  monkey_t_array0 = np.array(monkey_information['monkey_t'])\n",
        "  monkey_x_array0 = np.array(monkey_information['monkey_x'])\n",
        "  monkey_y_array0 = np.array(monkey_information['monkey_y'])\n",
        "  monkey_angle_array0 = np.array(monkey_information['monkey_angle'])\n",
        "  valid_index = np.where(monkey_t_array0 > ff_catched_T_sorted[0])[0]\n",
        "  num_missed_index = valid_index[0]\n",
        "  monkey_t_array = monkey_t_array0[valid_index]\n",
        "  monkey_x_array = monkey_x_array0[valid_index]\n",
        "  monkey_y_array = monkey_y_array0[valid_index]\n",
        "  monkey_angle_array = monkey_angle_array0[valid_index]\n",
        "  #index_array = np.array(range(len(monkey_t_array))) this overlaps with another variable later\n",
        "\n",
        "  max_distance = 400\n",
        "  reward_boundary = 25\n",
        "  #max_time = 400\n",
        "  #max_time = ff_catched_T_sorted[1251] #replace with ff_catched_T_sorted[-1]\n",
        "  ff_index = []\n",
        "  point_index = []\n",
        "  time = []\n",
        "  target_index = []\n",
        "  ff_x = []\n",
        "  ff_y = []\n",
        "  monkey_x = []\n",
        "  monkey_y = []\n",
        "  visible = []\n",
        "  memory = []\n",
        "  ff_distance = []\n",
        "  ff_angle = []\n",
        "  ff_angle_boundary = []\n",
        "  left_right = []\n",
        "  num_captures = []\n",
        "  distance_closest_ff =[]\n",
        "  distance_2ndclosest_ff = []\n",
        "  num_ff_within = []\n",
        "  reward_distance = []\n",
        "  reward_angle = []\n",
        "  catched_ff_num = len(ff_catched_T_sorted) - 200\n",
        "  total_ff_num = len(ff_life_sorted)\n",
        "\n",
        "\n",
        "  for i in range(total_ff_num):\n",
        "\n",
        "    if i % 100 == 0:\n",
        "      print(i,\" out of \", total_ff_num)\n",
        "\n",
        "\n",
        "    # Go through every visible duration of the same ff\n",
        "    ff_flash = ff_flash_sorted[i]\n",
        "    whether_in_obs = []\n",
        "    original_index = sort_indices_all[i]\n",
        "    for index in valid_index:\n",
        "      obs_ff_indices = obs_ff_overall_indices_all[index]\n",
        "      if original_index in obs_ff_indices:\n",
        "        whether_in_obs.append(True) \n",
        "      else:\n",
        "        whether_in_obs.append(False)\n",
        "\n",
        "    \n",
        "    cum_indices = np.array(whether_in_obs).nonzero()[0]\n",
        "    if len(cum_indices) > 0:\n",
        "\n",
        "      updated_t_array = monkey_t_array[cum_indices]\n",
        "      visible_indices = cum_indices\n",
        "      index_array = cum_indices\n",
        "      in_memory_length = len(index_array)\n",
        "        \n",
        "      cum_t, cum_angle = monkey_t_array[cum_indices], monkey_angle_array[cum_indices]\n",
        "      cum_mx, cum_my = monkey_x_array[cum_indices], monkey_y_array[cum_indices]\n",
        "      distances_to_monkey = LA.norm(np.stack([cum_mx, cum_my], axis=1)-ff_real_position_sorted[i], axis = 1)\n",
        "      angles_to_monkey = np.arctan2(ff_real_position_sorted[i,1]-cum_my, ff_real_position_sorted[i,0]-cum_mx)-cum_angle\n",
        "      angles_to_monkey[angles_to_monkey > pi] = angles_to_monkey[angles_to_monkey > pi] - 2*pi\n",
        "      angles_to_monkey[angles_to_monkey < -pi] = angles_to_monkey[angles_to_monkey < -pi] + 2*pi\n",
        "      angles_adjusted = np.absolute(angles_to_monkey)-np.abs(np.arcsin(np.divide(reward_boundary, np.maximum(distances_to_monkey, reward_boundary) ))) # use torch clip to get valid arcsin input\n",
        "      angles_adjusted = np.clip(angles_adjusted, 0, pi)\n",
        "      angles_to_monkey = np.sign(angles_to_monkey)* angles_adjusted\n",
        "\n",
        "\n",
        "\n",
        "      # Make a numpy array of points to denote memory, with 0 means being invisible. We also append 99 extra points after the last point    \n",
        "      memory_indices0 = np.zeros(visible_indices[-1]+100, dtype=int)\n",
        "      memory_indices0[visible_indices] = 100\n",
        "\n",
        "      if len(ff_catched_T_sorted)-1 >= i:\n",
        "        # Find the index of the time at which the ff is captured\n",
        "        last_live_time = np.where(monkey_t_array <= ff_catched_T_sorted[i])[0][-1]\n",
        "        # Truncate memory_indices0 based on that \n",
        "        memory_indices0 = memory_indices0[:last_live_time+1]\n",
        "\n",
        "      \n",
        "      # Iterate through memory_indices0 to make a new list to denote memory (replacing some 0s with other numbers based on time)\n",
        "      # We preserve the first element of memory_indices0. We also separate memory_indices and point_indices (denoted as final_indices)\n",
        "      memory_indices = [memory_indices0[0]]\n",
        "\n",
        "      for k in range(1, len(memory_indices0)):\n",
        "        if memory_indices0[k] == 0:\n",
        "          memory_indices.append(memory_indices[k-1]-1)\n",
        "        else: # Else, preserve the current value\n",
        "          memory_indices.append(memory_indices0[k])\n",
        "      memory_indices_array = np.array(memory_indices)\n",
        "\n",
        "      index_array0 = np.arange(len(memory_indices0))\n",
        "      if len(index_array0) > len(monkey_t_array):\n",
        "        max_index = len(monkey_t_array)\n",
        "        index_array0 = index_array0[:max_index]\n",
        "        memory_indices_array = memory_indices_array[index_array0]\n",
        "\n",
        "      in_memory_indices = np.where(memory_indices_array > 0)[0]\n",
        "      memory_indices_array = memory_indices_array[in_memory_indices]\n",
        "      index_array = index_array0[in_memory_indices]\n",
        "      in_memory_length = len(memory_indices_array)\n",
        "      memory_indices = memory_indices_array.tolist()\n",
        "      final_indices = index_array.tolist()\n",
        "\n",
        "      ff_index = ff_index + [i]*in_memory_length\n",
        "      point_index = point_index+ [point + num_missed_index for point in final_indices]\n",
        "      relevant_time = monkey_t_array[index_array]\n",
        "      time = time+relevant_time.tolist()\n",
        "      target_index = target_index + np.digitize(relevant_time, ff_catched_T_sorted).tolist()\n",
        "      ff_x = ff_x + [(ff_real_position_sorted[i][0])]*in_memory_length\n",
        "      ff_y = ff_y + [(ff_real_position_sorted[i][1])]*in_memory_length\n",
        "      monkey_x = monkey_x + monkey_x_array[index_array].tolist()\n",
        "      monkey_y = monkey_y + monkey_y_array[index_array].tolist()\n",
        "      visible = visible + [1 if point ==100 else 0 for point in memory_indices]\n",
        "      memory = memory + memory_indices\n",
        "      monkey_xy_relevant = np.stack([monkey_x_array[index_array], monkey_y_array[index_array]],axis=1)\n",
        "      monkey_angle_relevant = monkey_angle_array[index_array]\n",
        "      ff_distance_relevant = LA.norm(monkey_xy_relevant-ff_real_position_sorted[i], axis=1)\n",
        "      ff_distance = ff_distance + ff_distance_relevant.tolist()\n",
        "      angles_to_monkey = np.arctan2(ff_real_position_sorted[i,1]-monkey_xy_relevant[:,1], ff_real_position_sorted[i,0]-monkey_xy_relevant[:,0]) - monkey_angle_relevant\n",
        "      angles_to_monkey[angles_to_monkey > pi] = angles_to_monkey[angles_to_monkey > pi] - 2*pi\n",
        "      angles_to_monkey[angles_to_monkey < -pi] = angles_to_monkey[angles_to_monkey < -pi] + 2*pi\n",
        "      angles_adjusted = np.absolute(angles_to_monkey)-np.abs(np.arcsin(np.divide(reward_boundary, np.maximum(ff_distance_relevant, reward_boundary) ))) # use torch clip to get valid arcsin input\n",
        "      angles_adjusted = np.clip(angles_adjusted, 0, pi)\n",
        "      angles_adjusted = np.sign(angles_to_monkey)* angles_adjusted\n",
        "      ff_angle = ff_angle + angles_to_monkey.tolist()\n",
        "      ff_angle_boundary = ff_angle_boundary + angles_adjusted.tolist()\n",
        "      left_right = left_right + (np.array(angles_to_monkey) > 0).astype(int).tolist()\n",
        "\n",
        "\n",
        "\n",
        "          \n",
        "  # Now let's create a dictionary of the lists\n",
        "  ff_dict = {'ff_index':ff_index, 'point_index':point_index, 'time':time, 'target_index':target_index,\n",
        "              'ff_x':ff_x, 'ff_y':ff_y, 'monkey_x':monkey_x, 'monkey_y':monkey_y, 'visible':visible,\n",
        "              'memory':memory, 'ff_distance':ff_distance, 'ff_angle':ff_angle, 'ff_angle_boundary': ff_angle_boundary, 'left_right':left_right}\n",
        "  ff_dataframe = pd.DataFrame(ff_dict)\n",
        "\n",
        "  filepath = 'gdrive/MyDrive/fireflies_data/' + data_folder_name + '/ff_dataframe.csv'\n",
        "  os.makedirs('gdrive/MyDrive/fireflies_data/' + data_folder_name, exist_ok = True)\n",
        "  ff_dataframe.to_csv(filepath) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Saa7n_ajfSWE"
      },
      "source": [
        "## Data from agent (SB3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0oL-gMmgC4V"
      },
      "source": [
        "### import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRhw2a3NfYiJ"
      },
      "outputs": [],
      "source": [
        "data_folder_name = \"agent_3\"\n",
        "data_num = 3\n",
        "\n",
        "\n",
        "import random\n",
        "import gym\n",
        "from gym import spaces, Env\n",
        "from gym.spaces import Dict, Box\n",
        "import torch\n",
        "from numpy import pi\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "from torch.linalg import vector_norm\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "!pip install stable-baselines3\n",
        "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from torch.utils.data.dataset import Dataset, random_split\n",
        "# For animation\n",
        "\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageOps\n",
        "from IPython.display import Image as Image2\n",
        "\n",
        "import os\n",
        "\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common import results_plotter\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
        "from stable_baselines3.common.noise import NormalActionNoise\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
        "\n",
        "!pip install optuna\n",
        "import optuna\n",
        "from optuna.pruners import MedianPruner\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "from typing import Any\n",
        "from typing import Dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCfGOYrAk3rw"
      },
      "outputs": [],
      "source": [
        "if MONKEY_DATA == True:\n",
        "  raise ValueError(\"Need to skip the codes for RL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH1oHIpeTqzY"
      },
      "source": [
        "### c10.2: dt = 0.25, noise = 1, memory = 3\n",
        "\n",
        "discount factor = 0.995\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtvGGtcSTqzZ"
      },
      "outputs": [],
      "source": [
        "retrieve_dir = \"/content/gdrive/MyDrive/fireflies_agent/July_20_6/\" # discount factor = 0.995  \n",
        "data_folder_name = \"July_20_6\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAehYHFFTqzZ"
      },
      "outputs": [],
      "source": [
        "# obs = [angle, distance, memory, angle2, distance2, memory2, ...]\n",
        "# before = [angle, angle2 ..., distance, distance2, ... memory, memory2]\n",
        "\n",
        "class MultiFF(Env):\n",
        "  def __init__(self):\n",
        "      super(MultiFF, self).__init__()\n",
        "      self.num_ff = 200\n",
        "      self.arena_radius = 1000\n",
        "      self.episode_len = 15000 \n",
        "      self.dt = 0.25 \n",
        "      #self.current_episode = 0\n",
        "      #self.action_space = spaces.Box(low=-1., high=1., shape=(2,),dtype=np.float32)\n",
        "      self.action_space = spaces.Box(low=-1., high=1., shape=(2,),dtype=np.float32)\n",
        "      self.obs_ff = 2\n",
        "      self.observation_space = spaces.Box(low=-1., high=1., shape=(self.obs_ff*4,),dtype=np.float32)\n",
        "      self.terminal_vel = 0.01\n",
        "      #self.pro_noise_std = 0.005\n",
        "      self.vgain = 200\n",
        "      self.wgain = pi/2\n",
        "      self.reward_per_ff = 100\n",
        "      #self.time_cost = 0.005\n",
        "      #self.total_time = 0\n",
        "      #self.zero_action = False\n",
        "      #self.target_update_counter_num = 20\n",
        "      #self.reward_per_episode = []\n",
        "      #self.update_slots = True\n",
        "      #self.closest_ff_distance = 200\n",
        "      self.pro_noise_std = 0.005\n",
        "      self.epi_num = -1\n",
        "      self.has_sped_up_before = False\n",
        "      self.full_memory = 3\n",
        "      self.internal_noise_factor = 1\n",
        "      self.ff_memory_all = torch.ones([self.num_ff,])*self.full_memory\n",
        "      self.invisible_distance = 400\n",
        "      self.invisible_angle = 2*pi/9\n",
        "      self.reward_boundary = 25\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self.epi_num += 1\n",
        "    print(\"\\n episode: \", self.epi_num)\n",
        "    self.num_targets = 0\n",
        "    #self.past_speeds = []\n",
        "    self.time = 0\n",
        "    #self.counter = 0\n",
        "    #self.current_target_index = torch.tensor([999], dtype=torch.int32)\n",
        "    #self.previous_target_index = self.current_target_index\n",
        "    self.ff_flash = []\n",
        "    self.has_sped_up_before = False\n",
        "    \n",
        "    for i in range(self.num_ff):\n",
        "      num_intervals = 2500\n",
        "      first_flash = torch.rand(1)\n",
        "      intervals = torch.poisson(torch.ones(num_intervals-1)*3)\n",
        "      t0 = torch.cat((first_flash, first_flash+torch.cumsum(intervals, dim=0)+torch.cumsum(torch.ones(num_intervals-1)*0.3, dim=0)))\n",
        "      t1 = t0 + torch.ones(num_intervals)*0.3\n",
        "      self.ff_flash.append(torch.stack((t0, t1), dim=1))\n",
        "\n",
        "    self.ffr = torch.sqrt(torch.rand(self.num_ff))*self.arena_radius # The radius of the arena changed from 1000 cm/s to 200 cm/s\n",
        "    self.fftheta = torch.rand(self.num_ff)*2*pi\n",
        "    #self.ffrt = torch.stack((self.ffr, self.fftheta), dim=1) \n",
        "    self.ffx = torch.cos(self.fftheta) * self.ffr\n",
        "    self.ffy = torch.sin(self.fftheta) * self.ffr\n",
        "    self.ffxy = torch.stack((self.ffx, self.ffy), dim=1)\n",
        "    self.ffx2 = self.ffx.clone()\n",
        "    self.ffy2 = self.ffy.clone()\n",
        "    self.ffxy2 = torch.stack((self.ffx2, self.ffy2), dim=1)\n",
        "    #self.ff_info={}\n",
        "    #self.update_slots = True\n",
        "    self.agentx = torch.tensor([0])\n",
        "    self.agenty = torch.tensor([0])\n",
        "    self.agentr = torch.zeros(1)\n",
        "    self.agentxy = torch.tensor([0, 0])\n",
        "    self.agentheading = torch.zeros(1).uniform_(0, 2*pi)\n",
        "    self.dv = torch.zeros(1).uniform_(-0.05, 0.05) \n",
        "    self.dw = torch.zeros(1)\n",
        "    self.end_episode = False \n",
        "    self.obs = self.beliefs().numpy()\n",
        "    #self.chunk_50s = 1\n",
        "    self.episode_reward = 0\n",
        "    #self.stop_rewarding_speed = False\n",
        "    #self.current_obs_steps = 0\n",
        "    self.ff_memory_all = torch.ones([self.num_ff,])\n",
        "    return self.obs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def calculate_reward(self):\n",
        "      #action_cost=((self.previous_action[1]-self.action[1])**2+(self.previous_action[0]-self.action[0])**2)*self.mag_cost\n",
        "      # To incorporate action_cost, we need to incorporate previous_action into decision_info\n",
        "      #self.total_time += 1\n",
        "      # In addition to rewarding the monkey for capturing the firefly, we also use different phases of rewards to teach monkey specific behaviours\n",
        "      #reward = -self.time_cost\n",
        "      reward = 0\n",
        "      # Reward shaping\n",
        "      # Phase I: reward the agent for learning to stop \n",
        "      # Always: reward the agent for capturing fireflies\n",
        "      self.num_targets = 0\n",
        "      if abs(self.sys_vel[1]) <= self.terminal_vel:\n",
        "        captured_ff_index = (self.ff_distance_all <= self.reward_boundary).nonzero().reshape(-1).tolist()\n",
        "        self.captured_ff_index = captured_ff_index\n",
        "        num_targets = len(captured_ff_index)\n",
        "        self.num_targets = num_targets\n",
        "        if num_targets > 0: # If the monkey hs captured at least 1 ff\n",
        "          # Calculate reward\n",
        "          reward = reward + self.reward_per_ff * num_targets\n",
        "          # Replace the captured ffs with ffs of new locations\n",
        "          self.ffr[captured_ff_index]= torch.sqrt(torch.rand(num_targets))*self.arena_radius\n",
        "          self.fftheta[captured_ff_index]= torch.rand(num_targets)*2*pi\n",
        "          #self.ffrt = torch.stack((self.ffr, self.fftheta), dim=1) \n",
        "          self.ffx[captured_ff_index] = torch.cos(self.fftheta[captured_ff_index]) * self.ffr[captured_ff_index]\n",
        "          self.ffy[captured_ff_index] = torch.sin(self.fftheta[captured_ff_index]) * self.ffr[captured_ff_index]\n",
        "          self.ffxy = torch.stack((self.ffx, self.ffy), dim=1)\n",
        "          self.ffx2[captured_ff_index] = self.ffx[captured_ff_index].clone()\n",
        "          self.ffy2[captured_ff_index] = self.ffy[captured_ff_index].clone()\n",
        "          self.ffxy2 = torch.stack((self.ffx2, self.ffy2), dim=1)\n",
        "          # Delete the information from self.ff_info\n",
        "          #[self.ff_info.pop(key) for key in captured_ff_index if (key in self.ff_info)]\n",
        "          #self.current_target_index = torch.tensor([999], dtype=torch.int32)\n",
        "          #self.previous_target_index = self.current_target_index\n",
        "          # Reward the firefly based on the average speed before capturing the firefly\n",
        "          ##reward = reward + sum(self.past_speeds)/len(self.past_speeds)\n",
        "          #print(round(self.time, 2), \"sys_vel: \", [round(i, 4) for i in self.sys_vel.tolist()], \"obs: \", list(np.round(self.obs, decimals = 2)), \"n_targets: \",  num_targets)\n",
        "          print(round(self.time, 2), \"sys_vel: \", [round(i, 4) for i in self.sys_vel.tolist()], \"n_targets: \",  num_targets)\n",
        "          #self.update_slots = True\n",
        "          #self.stop_rewarding_speed = True\n",
        "        #elif self.has_sped_up_before == True:\n",
        "        # based on Ruiyi's formula, using the distance of the closest ff in obs\n",
        "        #reward += math.exp(-((self.ff_current[1, 0]**2)*(25/1.5)**2)/2)\n",
        "        #self.has_sped_up_before = False\n",
        "      return reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    self.time += self.dt\n",
        "    action=torch.tensor(action)\n",
        "    action[1] = action[1]/2+0.5\n",
        "    self.sys_vel=action.clone()\n",
        "    self.state_step(action)\n",
        "    self.obs = self.beliefs().numpy()\n",
        "    reward=self.calculate_reward()\n",
        "    self.episode_reward += reward\n",
        "    \n",
        "    if self.time >= self.episode_len*self.dt:\n",
        "      self.end_episode = True\n",
        "      #self.current_episode += 1\n",
        "      print(\"Reward for the episode: \", self.episode_reward)\n",
        "    #print(\"action: \", torch.round(action, decimals = 3), \"obs: \", np.round(self.obs, decimals = 3), \"Reward: \", round(reward, 3))\n",
        "    return self.obs, reward, self.end_episode, {}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def state_step(self,action):\n",
        "    vnoise=torch.distributions.Normal(0,torch.ones([1,1])).sample()*self.pro_noise_std \n",
        "    wnoise=torch.distributions.Normal(0,torch.ones([1,1])).sample()*self.pro_noise_std\n",
        "    self.dw_normed = (action[0]+wnoise)\n",
        "    self.dv_normed = (action[1]+vnoise)\n",
        "    self.dw = (action[0]+wnoise)*self.wgain*self.dt\n",
        "    self.agentheading = self.agentheading + self.dw.item()\n",
        "    self.dv =  (action[1]+vnoise)*self.vgain*self.dt\n",
        "    self.dx= torch.cos(self.agentheading)*self.dv\n",
        "    self.dy= torch.sin(self.agentheading)*self.dv\n",
        "    self.agentx = self.agentx + self.dx.item()\n",
        "    self.agenty = self.agenty + self.dy.item()\n",
        "    self.agentxy = torch.cat((self.agentx, self.agenty))\n",
        "    self.agentr = vector_norm(self.agentxy)\n",
        "    self.agenttheta = torch.tensor(torch.atan2(self.agenty, self.agentx))  \n",
        "                               \n",
        "    if self.agentr >= self.arena_radius:\n",
        "      self.agentr = 2*self.arena_radius-self.agentr\n",
        "      self.agenttheta = self.agenttheta + pi\n",
        "      self.agentx = (self.agentr*torch.cos(self.agenttheta)).reshape(1,)\n",
        "      self.agenty = (self.agentr*torch.sin(self.agenttheta)).reshape(1,)\n",
        "      self.agentxy = torch.cat((self.agentx, self.agenty))\n",
        "      self.agentheading = self.agenttheta - pi\n",
        "    while self.agentheading >= 2*pi:\n",
        "      self.agentheading = self.agentheading - 2*pi\n",
        "    while self.agentheading < 0:\n",
        "      self.agentheading = self.agentheading + 2*pi   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def beliefs(self): \n",
        "\n",
        "    # Make a tensor containing the relative distance of all fireflies to the agent\n",
        "    self.ff_distance_all = vector_norm(self.ffxy - self.agentxy, dim=1)\n",
        "    # Make a tensor containing the relative (real) angle of all fireflies to the agent\n",
        "    ffradians = torch.atan2(self.ffy-self.agenty, self.ffx-self.agentx)\n",
        "    angle0 = ffradians - self.agentheading\n",
        "    angle0[angle0 > pi] = angle0[angle0 > pi] - 2*pi\n",
        "    angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2*pi\n",
        "    # Adjust the angle based on reward boundary\n",
        "    angle1 = torch.abs(angle0)-torch.abs(torch.arcsin(torch.div(self.reward_boundary, torch.clip( self.ff_distance_all, self.reward_boundary, 400) ))) # use torch clip to get valid arcsin input\n",
        "    angle2 = torch.clip(angle1,0,pi)\n",
        "    ff_angle_all = torch.sign(angle0)* angle2\n",
        "    # Update the tensor containing the uncertainties of all fireflies to the agent\n",
        "    visible_ff = torch.logical_and( self.ff_distance_all < self.invisible_distance, torch.abs(ff_angle_all) < self.invisible_angle)\n",
        "    self.visible_ff_indices0 = visible_ff.nonzero().reshape(-1)\n",
        "    for index in self.visible_ff_indices0:\n",
        "      ff = self.ff_flash[index]\n",
        "      if not torch.any(torch.logical_and(ff[:, 0] <= self.time, ff[:, 1] >= self.time)):\n",
        "        visible_ff[index] = False\n",
        "    self.visible_ff_indices = visible_ff.nonzero().reshape(-1)\n",
        "    # Update memory\n",
        "    self.ff_memory_all -= 1\n",
        "    self.ff_memory_all[self.visible_ff_indices] = self.full_memory\n",
        "    self.ff_memory_all = torch.clamp(self.ff_memory_all, 0, self.full_memory)\n",
        "    # Calculate the uncertainties that will be added to relative distance and angle based on memory\n",
        "    ff_uncertainty_all = (self.full_memory-self.ff_memory_all)*self.internal_noise_factor\n",
        "    self.ffx2 = self.ffx2 + torch.normal(torch.zeros([self.num_ff,]), ff_uncertainty_all)\n",
        "    self.ffy2 = self.ffy2 + torch.normal(torch.zeros([self.num_ff,]), ff_uncertainty_all)\n",
        "    self.ffx2[self.visible_ff_indices] = self.ffx[self.visible_ff_indices].clone()\n",
        "    self.ffy2[self.visible_ff_indices] = self.ffy[self.visible_ff_indices].clone()\n",
        "    self.ffxy2 = torch.stack((self.ffx2, self.ffy2), dim=1)\n",
        "    # find ffs that are in memory\n",
        "    self.ff_in_memory_indices = (self.ff_memory_all > 0).nonzero().reshape(-1)\n",
        "    # Consider the case where there are fewer than self.obs_ff fireflies that are in memory\n",
        "    \n",
        "\n",
        "   \n",
        "    if torch.numel(self.ff_in_memory_indices) >= self.obs_ff:\n",
        "      # Rank the ff whose \"memory\" is creater than 0 based on distance\n",
        "      sorted_indices =  torch.topk(-self.ff_distance_all[self.ff_in_memory_indices], self.obs_ff).indices\n",
        "      self.topk_indices = self.ff_in_memory_indices[sorted_indices]\n",
        "      self.ffxy2_topk = self.ffxy2[self.topk_indices]\n",
        "      self.ff_distance_topk = vector_norm(self.ffxy2_topk - self.agentxy, dim=1)\n",
        "      # Calculate relative angles \n",
        "      ffradians = torch.atan2(self.ffxy2_topk[:,1]-self.agenty, self.ffxy2_topk[:,0]-self.agentx)\n",
        "      angle0 = ffradians - self.agentheading\n",
        "      angle0[angle0 > pi] = angle0[angle0 > pi] - 2*pi\n",
        "      angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2*pi\n",
        "      self.ff_angle_topk_2 = angle0.clone()\n",
        "      # Calculate relative angles of all ffs based on reward boundaries\n",
        "      # Adjust the angle based on reward boundary\n",
        "      angle1 = torch.abs(angle0)-torch.abs(torch.arcsin(torch.div(self.reward_boundary, torch.clip(self.ff_distance_topk, self.reward_boundary, 400) ))) # use torch clip to get valid arcsin input\n",
        "      angle2 = torch.clip(angle1,0,pi)\n",
        "      ff_angle_topk_3 = torch.sign(angle0)* angle2\n",
        "      # Concatenate distance, angle, and memory\n",
        "      ff_array = torch.stack((self.ff_angle_topk_2, ff_angle_topk_3, self.ff_distance_topk,  self.ff_memory_all[self.topk_indices]), dim=0)\n",
        "    \n",
        "\n",
        "    elif torch.numel(self.ff_in_memory_indices) == 0:\n",
        "      ff_array = torch.tensor([[0], [0], [self.invisible_distance], [0]]).repeat([1, self.obs_ff])\n",
        "      self.topk_indices = torch.tensor([])\n",
        "\n",
        "    else:\n",
        "      sorted_distance, sorted_indices = torch.sort(-self.ff_distance_all[self.ff_in_memory_indices])\n",
        "      self.topk_indices = self.ff_in_memory_indices[sorted_indices]\n",
        "      self.ffxy2_topk = self.ffxy2[self.topk_indices ]\n",
        "      self.ff_distance_topk = vector_norm(self.ffxy2_topk - self.agentxy, dim=1)\n",
        "      # Calculate relative angles \n",
        "      ffradians = torch.atan2(self.ffxy2_topk[:,1]-self.agenty, self.ffxy2_topk[:,0]-self.agentx)\n",
        "      angle0 = ffradians - self.agentheading\n",
        "      angle0[angle0 > pi] = angle0[angle0 > pi] - 2*pi\n",
        "      angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2*pi\n",
        "      self.ff_angle_topk_2 = angle0.clone()\n",
        "      # Calculate relative angles of all ffs based on reward boundaries\n",
        "      # Adjust the angle based on reward boundary\n",
        "      angle1 = torch.abs(angle0)-torch.abs(torch.arcsin(torch.div(self.reward_boundary, torch.clip(self.ff_distance_topk, self.reward_boundary, 400) ))) # use torch clip to get valid arcsin input\n",
        "      angle2 = torch.clip(angle1,0,pi)\n",
        "      ff_angle_topk_3 = torch.sign(angle0)* angle2\n",
        "      # Concatenate distance, angle, and memory\n",
        "      ff_array0 = torch.stack((self.ff_angle_topk_2, ff_angle_topk_3, self.ff_distance_topk,  self.ff_memory_all[self.topk_indices]), dim=0)\n",
        "      needed_ff = self.obs_ff - torch.numel(self.ff_in_memory_indices)\n",
        "      ff_array = torch.stack([ff_array0.reshape([4,-1]), torch.tensor([[0], [0], [self.invisible_distance], [0]]).repeat([1, needed_ff])], dim=1)\n",
        "      \n",
        "\n",
        "    # ff_array[0:2,:] = ff_array[0:2,:]/pi\n",
        "    # ff_array[2,:] = (ff_array[2,:]/self.invisible_distance-0.5)*2\n",
        "    # ff_array[3,:] = (ff_array[3,:]/20-0.5)*2\n",
        "    self.ff_array = ff_array.clone()\n",
        "    return torch.flatten(ff_array.transpose(0, 1))\n",
        "\n",
        "\n",
        "env = MultiFF() \n",
        "env.reset() \n",
        "\n",
        "sac_model = SAC(\"MlpPolicy\", \n",
        "            env,\n",
        "            buffer_size=int(1e6),\n",
        "            batch_size=1024,\n",
        "            device='auto',\n",
        "            verbose=False,\n",
        "            train_freq=100,\n",
        "            learning_starts = int(10),\n",
        "            target_update_interval=20,\n",
        "            learning_rate=1e-2,\n",
        "            gamma=0.9999,\n",
        "            policy_kwargs=dict(activation_fn=nn.ReLU, net_arch=[64, 64])\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxsKUNUSI6Wb"
      },
      "source": [
        "### c11: dt = 0.25, noise = 1.2, memory = 4\n",
        "\n",
        "discount factor = 0.995\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdAVHaS8I6Wc"
      },
      "outputs": [],
      "source": [
        "retrieve_dir = \"/content/gdrive/MyDrive/fireflies_agent/July_30/\" # discount factor = 0.995  \n",
        "# # Last mean reward per episode: 84256.00\n",
        "# path = os.path.join(retrieve_dir, 'best_model.zip')\n",
        "# path2 = os.path.join(retrieve_dir, 'buffer.pkl')\n",
        "# sac_model = sac_model.load(path,env=env)  \n",
        "# sac_model.load_replay_buffer(path2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utvGxpKAI6Wd"
      },
      "outputs": [],
      "source": [
        "# obs = [angle, distance, memory, angle2, distance2, memory2, ...]\n",
        "# before = [angle, angle2 ..., distance, distance2, ... memory, memory2]\n",
        "\n",
        "class MultiFF(Env):\n",
        "  def __init__(self):\n",
        "      super(MultiFF, self).__init__()\n",
        "      self.num_ff = 200\n",
        "      self.arena_radius = 1000\n",
        "      self.episode_len = 10000 \n",
        "      self.dt = 0.25 \n",
        "      #self.current_episode = 0\n",
        "      #self.action_space = spaces.Box(low=-1., high=1., shape=(2,),dtype=np.float32)\n",
        "      self.action_space = spaces.Box(low=-1., high=1., shape=(2,),dtype=np.float32)\n",
        "      self.obs_ff = 2\n",
        "      self.observation_space = spaces.Box(low=-1., high=1., shape=(self.obs_ff*4,),dtype=np.float32)\n",
        "      self.terminal_vel = 0.01\n",
        "      #self.pro_noise_std = 0.005\n",
        "      self.vgain = 200\n",
        "      self.wgain = pi/2\n",
        "      self.reward_per_ff = 100\n",
        "      #self.time_cost = 0.005\n",
        "      #self.total_time = 0\n",
        "      #self.zero_action = False\n",
        "      #self.target_update_counter_num = 20\n",
        "      #self.reward_per_episode = []\n",
        "      #self.update_slots = True\n",
        "      #self.closest_ff_distance = 200\n",
        "      self.pro_noise_std = 0.005\n",
        "      self.epi_num = -1\n",
        "      self.has_sped_up_before = False\n",
        "      self.full_memory = 4\n",
        "      self.internal_noise_factor = 1.2\n",
        "      self.ff_memory_all = torch.ones([self.num_ff,])*self.full_memory\n",
        "      self.invisible_distance = 400\n",
        "      self.invisible_angle = 2*pi/9\n",
        "      self.reward_boundary = 25\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self.epi_num += 1\n",
        "    print(\"\\n episode: \", self.epi_num)\n",
        "    self.num_targets = 0\n",
        "    #self.past_speeds = []\n",
        "    self.time = 0\n",
        "    #self.counter = 0\n",
        "    #self.current_target_index = torch.tensor([999], dtype=torch.int32)\n",
        "    #self.previous_target_index = self.current_target_index\n",
        "    self.ff_flash = []\n",
        "    self.has_sped_up_before = False\n",
        "    \n",
        "    for i in range(self.num_ff):\n",
        "      num_intervals = 1500\n",
        "      first_flash = torch.rand(1)\n",
        "      intervals = torch.poisson(torch.ones(num_intervals-1)*3)\n",
        "      t0 = torch.cat((first_flash, first_flash+torch.cumsum(intervals, dim=0)+torch.cumsum(torch.ones(num_intervals-1)*0.3, dim=0)))\n",
        "      t1 = t0 + torch.ones(num_intervals)*0.3\n",
        "      self.ff_flash.append(torch.stack((t0, t1), dim=1))\n",
        "\n",
        "    self.ffr = torch.sqrt(torch.rand(self.num_ff))*self.arena_radius # The radius of the arena changed from 1000 cm/s to 200 cm/s\n",
        "    self.fftheta = torch.rand(self.num_ff)*2*pi\n",
        "    #self.ffrt = torch.stack((self.ffr, self.fftheta), dim=1) \n",
        "    self.ffx = torch.cos(self.fftheta) * self.ffr\n",
        "    self.ffy = torch.sin(self.fftheta) * self.ffr\n",
        "    self.ffxy = torch.stack((self.ffx, self.ffy), dim=1)\n",
        "    self.ffx2 = self.ffx.clone()\n",
        "    self.ffy2 = self.ffy.clone()\n",
        "    self.ffxy2 = torch.stack((self.ffx2, self.ffy2), dim=1)\n",
        "    #self.ff_info={}\n",
        "    #self.update_slots = True\n",
        "    self.agentx = torch.tensor([0])\n",
        "    self.agenty = torch.tensor([0])\n",
        "    self.agentr = torch.zeros(1)\n",
        "    self.agentxy = torch.tensor([0, 0])\n",
        "    self.agentheading = torch.zeros(1).uniform_(0, 2*pi)\n",
        "    self.dv = torch.zeros(1).uniform_(-0.05, 0.05) \n",
        "    self.dw = torch.zeros(1)\n",
        "    self.end_episode = False \n",
        "    self.obs = self.beliefs().numpy()\n",
        "    #self.chunk_50s = 1\n",
        "    self.episode_reward = 0\n",
        "    #self.stop_rewarding_speed = False\n",
        "    #self.current_obs_steps = 0\n",
        "    self.ff_memory_all = torch.ones([self.num_ff,])\n",
        "    return self.obs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def calculate_reward(self):\n",
        "      #action_cost=((self.previous_action[1]-self.action[1])**2+(self.previous_action[0]-self.action[0])**2)*self.mag_cost\n",
        "      # To incorporate action_cost, we need to incorporate previous_action into decision_info\n",
        "      #self.total_time += 1\n",
        "      # In addition to rewarding the monkey for capturing the firefly, we also use different phases of rewards to teach monkey specific behaviours\n",
        "      #reward = -self.time_cost\n",
        "      reward = 0\n",
        "      # Reward shaping\n",
        "      # Phase I: reward the agent for learning to stop \n",
        "      # Always: reward the agent for capturing fireflies\n",
        "      self.num_targets = 0\n",
        "      if abs(self.sys_vel[1]) <= self.terminal_vel:\n",
        "        captured_ff_index = (self.ff_distance_all <= self.reward_boundary).nonzero().reshape(-1).tolist()\n",
        "        self.captured_ff_index = captured_ff_index\n",
        "        num_targets = len(captured_ff_index)\n",
        "        self.num_targets = num_targets\n",
        "        if num_targets > 0: # If the monkey hs captured at least 1 ff\n",
        "          # Calculate reward\n",
        "          reward = reward + self.reward_per_ff * num_targets\n",
        "          # Replace the captured ffs with ffs of new locations\n",
        "          self.ffr[captured_ff_index]= torch.sqrt(torch.rand(num_targets))*self.arena_radius\n",
        "          self.fftheta[captured_ff_index]= torch.rand(num_targets)*2*pi\n",
        "          #self.ffrt = torch.stack((self.ffr, self.fftheta), dim=1) \n",
        "          self.ffx[captured_ff_index] = torch.cos(self.fftheta[captured_ff_index]) * self.ffr[captured_ff_index]\n",
        "          self.ffy[captured_ff_index] = torch.sin(self.fftheta[captured_ff_index]) * self.ffr[captured_ff_index]\n",
        "          self.ffxy = torch.stack((self.ffx, self.ffy), dim=1)\n",
        "          self.ffx2[captured_ff_index] = self.ffx[captured_ff_index].clone()\n",
        "          self.ffy2[captured_ff_index] = self.ffy[captured_ff_index].clone()\n",
        "          self.ffxy2 = torch.stack((self.ffx2, self.ffy2), dim=1)\n",
        "          # Delete the information from self.ff_info\n",
        "          #[self.ff_info.pop(key) for key in captured_ff_index if (key in self.ff_info)]\n",
        "          #self.current_target_index = torch.tensor([999], dtype=torch.int32)\n",
        "          #self.previous_target_index = self.current_target_index\n",
        "          # Reward the firefly based on the average speed before capturing the firefly\n",
        "          ##reward = reward + sum(self.past_speeds)/len(self.past_speeds)\n",
        "          #print(round(self.time, 2), \"sys_vel: \", [round(i, 4) for i in self.sys_vel.tolist()], \"obs: \", list(np.round(self.obs, decimals = 2)), \"n_targets: \",  num_targets)\n",
        "          print(round(self.time, 2), \"sys_vel: \", [round(i, 4) for i in self.sys_vel.tolist()], \"n_targets: \",  num_targets)\n",
        "          #self.update_slots = True\n",
        "          #self.stop_rewarding_speed = True\n",
        "        #elif self.has_sped_up_before == True:\n",
        "        # based on Ruiyi's formula, using the distance of the closest ff in obs\n",
        "        #reward += math.exp(-((self.ff_current[1, 0]**2)*(25/1.5)**2)/2)\n",
        "        #self.has_sped_up_before = False\n",
        "      return reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    self.time += self.dt\n",
        "    action=torch.tensor(action)\n",
        "    action[1] = action[1]/2+0.5\n",
        "    self.sys_vel=action.clone()\n",
        "    self.state_step(action)\n",
        "    self.obs = self.beliefs().numpy()\n",
        "    reward=self.calculate_reward()\n",
        "    self.episode_reward += reward\n",
        "    \n",
        "    if self.time >= self.episode_len*self.dt:\n",
        "      self.end_episode = True\n",
        "      #self.current_episode += 1\n",
        "      print(\"Reward for the episode: \", self.episode_reward)\n",
        "    #print(\"action: \", torch.round(action, decimals = 3), \"obs: \", np.round(self.obs, decimals = 3), \"Reward: \", round(reward, 3))\n",
        "    return self.obs, reward, self.end_episode, {}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def state_step(self,action):\n",
        "    vnoise=torch.distributions.Normal(0,torch.ones([1,1])).sample()*self.pro_noise_std \n",
        "    wnoise=torch.distributions.Normal(0,torch.ones([1,1])).sample()*self.pro_noise_std\n",
        "    self.dw_normed = (action[0]+wnoise)\n",
        "    self.dv_normed = (action[1]+vnoise)\n",
        "    self.dw = (action[0]+wnoise)*self.wgain*self.dt\n",
        "    self.agentheading = self.agentheading + self.dw.item()\n",
        "    self.dv =  (action[1]+vnoise)*self.vgain*self.dt\n",
        "    self.dx= torch.cos(self.agentheading)*self.dv\n",
        "    self.dy= torch.sin(self.agentheading)*self.dv\n",
        "    self.agentx = self.agentx + self.dx.item()\n",
        "    self.agenty = self.agenty + self.dy.item()\n",
        "    self.agentxy = torch.cat((self.agentx, self.agenty))\n",
        "    self.agentr = vector_norm(self.agentxy)\n",
        "    self.agenttheta = torch.tensor(torch.atan2(self.agenty, self.agentx))  \n",
        "                               \n",
        "    if self.agentr >= self.arena_radius:\n",
        "      self.agentr = 2*self.arena_radius-self.agentr\n",
        "      self.agenttheta = self.agenttheta + pi\n",
        "      self.agentx = (self.agentr*torch.cos(self.agenttheta)).reshape(1,)\n",
        "      self.agenty = (self.agentr*torch.sin(self.agenttheta)).reshape(1,)\n",
        "      self.agentxy = torch.cat((self.agentx, self.agenty))\n",
        "      self.agentheading = self.agenttheta - pi\n",
        "    while self.agentheading >= 2*pi:\n",
        "      self.agentheading = self.agentheading - 2*pi\n",
        "    while self.agentheading < 0:\n",
        "      self.agentheading = self.agentheading + 2*pi   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def beliefs(self): \n",
        "\n",
        "    # Make a tensor containing the relative distance of all fireflies to the agent\n",
        "    self.ff_distance_all = vector_norm(self.ffxy - self.agentxy, dim=1)\n",
        "    # Make a tensor containing the relative (real) angle of all fireflies to the agent\n",
        "    ffradians = torch.atan2(self.ffy-self.agenty, self.ffx-self.agentx)\n",
        "    angle0 = ffradians - self.agentheading\n",
        "    angle0[angle0 > pi] = angle0[angle0 > pi] - 2*pi\n",
        "    angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2*pi\n",
        "    # Adjust the angle based on reward boundary\n",
        "    angle1 = torch.abs(angle0)-torch.abs(torch.arcsin(torch.div(self.reward_boundary, torch.clip( self.ff_distance_all, self.reward_boundary, 400) ))) # use torch clip to get valid arcsin input\n",
        "    angle2 = torch.clip(angle1,0,pi)\n",
        "    ff_angle_all = torch.sign(angle0)* angle2\n",
        "    # Update the tensor containing the uncertainties of all fireflies to the agent\n",
        "    visible_ff = torch.logical_and( self.ff_distance_all < self.invisible_distance, torch.abs(ff_angle_all) < self.invisible_angle)\n",
        "    self.visible_ff_indices0 = visible_ff.nonzero().reshape(-1)\n",
        "    for index in self.visible_ff_indices0:\n",
        "      ff = self.ff_flash[index]\n",
        "      if not torch.any(torch.logical_and(ff[:, 0] <= self.time, ff[:, 1] >= self.time)):\n",
        "        visible_ff[index] = False\n",
        "    self.visible_ff_indices = visible_ff.nonzero().reshape(-1)\n",
        "    # Update memory\n",
        "    self.ff_memory_all -= 1\n",
        "    self.ff_memory_all[self.visible_ff_indices] = self.full_memory\n",
        "    self.ff_memory_all = torch.clamp(self.ff_memory_all, 0, self.full_memory)\n",
        "    # Calculate the uncertainties that will be added to relative distance and angle based on memory\n",
        "    ff_uncertainty_all = (self.full_memory-self.ff_memory_all)*self.internal_noise_factor\n",
        "    self.ffx2 = self.ffx2 + torch.normal(torch.zeros([self.num_ff,]), ff_uncertainty_all)\n",
        "    self.ffy2 = self.ffy2 + torch.normal(torch.zeros([self.num_ff,]), ff_uncertainty_all)\n",
        "    self.ffx2[self.visible_ff_indices] = self.ffx[self.visible_ff_indices].clone()\n",
        "    self.ffy2[self.visible_ff_indices] = self.ffy[self.visible_ff_indices].clone()\n",
        "    self.ffxy2 = torch.stack((self.ffx2, self.ffy2), dim=1)\n",
        "    # find ffs that are in memory\n",
        "    self.ff_in_memory_indices = (self.ff_memory_all > 0).nonzero().reshape(-1)\n",
        "    # Consider the case where there are fewer than self.obs_ff fireflies that are in memory\n",
        "    \n",
        "\n",
        "   \n",
        "    if torch.numel(self.ff_in_memory_indices) >= self.obs_ff:\n",
        "      # Rank the ff whose \"memory\" is creater than 0 based on distance\n",
        "      sorted_indices =  torch.topk(-self.ff_distance_all[self.ff_in_memory_indices], self.obs_ff).indices\n",
        "      self.topk_indices = self.ff_in_memory_indices[sorted_indices]\n",
        "      self.ffxy2_topk = self.ffxy2[self.topk_indices]\n",
        "      self.ff_distance_topk = vector_norm(self.ffxy2_topk - self.agentxy, dim=1)\n",
        "      # Calculate relative angles \n",
        "      ffradians = torch.atan2(self.ffxy2_topk[:,1]-self.agenty, self.ffxy2_topk[:,0]-self.agentx)\n",
        "      angle0 = ffradians - self.agentheading\n",
        "      angle0[angle0 > pi] = angle0[angle0 > pi] - 2*pi\n",
        "      angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2*pi\n",
        "      self.ff_angle_topk_2 = angle0.clone()\n",
        "      # Calculate relative angles of all ffs based on reward boundaries\n",
        "      # Adjust the angle based on reward boundary\n",
        "      angle1 = torch.abs(angle0)-torch.abs(torch.arcsin(torch.div(self.reward_boundary, torch.clip(self.ff_distance_topk, self.reward_boundary, 400) ))) # use torch clip to get valid arcsin input\n",
        "      angle2 = torch.clip(angle1,0,pi)\n",
        "      ff_angle_topk_3 = torch.sign(angle0)* angle2\n",
        "      # Concatenate distance, angle, and memory\n",
        "      ff_array = torch.stack((self.ff_angle_topk_2, ff_angle_topk_3, self.ff_distance_topk,  self.ff_memory_all[self.topk_indices]), dim=0)\n",
        "    \n",
        "\n",
        "    elif torch.numel(self.ff_in_memory_indices) == 0:\n",
        "      ff_array = torch.tensor([[0], [0], [self.invisible_distance], [0]]).repeat([1, self.obs_ff])\n",
        "      self.topk_indices = torch.tensor([])\n",
        "\n",
        "    else:\n",
        "      sorted_distance, sorted_indices = torch.sort(-self.ff_distance_all[self.ff_in_memory_indices])\n",
        "      self.topk_indices = self.ff_in_memory_indices[sorted_indices]\n",
        "      self.ffxy2_topk = self.ffxy2[self.topk_indices ]\n",
        "      self.ff_distance_topk = vector_norm(self.ffxy2_topk - self.agentxy, dim=1)\n",
        "      # Calculate relative angles \n",
        "      ffradians = torch.atan2(self.ffxy2_topk[:,1]-self.agenty, self.ffxy2_topk[:,0]-self.agentx)\n",
        "      angle0 = ffradians - self.agentheading\n",
        "      angle0[angle0 > pi] = angle0[angle0 > pi] - 2*pi\n",
        "      angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2*pi\n",
        "      self.ff_angle_topk_2 = angle0.clone()\n",
        "      # Calculate relative angles of all ffs based on reward boundaries\n",
        "      # Adjust the angle based on reward boundary\n",
        "      angle1 = torch.abs(angle0)-torch.abs(torch.arcsin(torch.div(self.reward_boundary, torch.clip(self.ff_distance_topk, self.reward_boundary, 400) ))) # use torch clip to get valid arcsin input\n",
        "      angle2 = torch.clip(angle1,0,pi)\n",
        "      ff_angle_topk_3 = torch.sign(angle0)* angle2\n",
        "      # Concatenate distance, angle, and memory\n",
        "      ff_array0 = torch.stack((self.ff_angle_topk_2, ff_angle_topk_3, self.ff_distance_topk,  self.ff_memory_all[self.topk_indices]), dim=0)\n",
        "      needed_ff = self.obs_ff - torch.numel(self.ff_in_memory_indices)\n",
        "      ff_array = torch.stack([ff_array0.reshape([4,-1]), torch.tensor([[0], [0], [self.invisible_distance], [0]]).repeat([1, needed_ff])], dim=1)\n",
        "      \n",
        "\n",
        "    # ff_array[0:2,:] = ff_array[0:2,:]/pi\n",
        "    # ff_array[2,:] = (ff_array[2,:]/self.invisible_distance-0.5)*2\n",
        "    # ff_array[3,:] = (ff_array[3,:]/20-0.5)*2\n",
        "    self.ff_array = ff_array.clone()\n",
        "    return torch.flatten(ff_array.transpose(0, 1))\n",
        "\n",
        "\n",
        "env = MultiFF() \n",
        "env.reset() \n",
        "\n",
        "sac_model = SAC(\"MlpPolicy\", \n",
        "            env,\n",
        "            buffer_size=int(1e6),\n",
        "            batch_size=1024,\n",
        "            device='auto',\n",
        "            verbose=False,\n",
        "            train_freq=100,\n",
        "            learning_starts = int(10),\n",
        "            target_update_interval=20,\n",
        "            learning_rate=1e-2,\n",
        "            gamma=0.9999,\n",
        "            policy_kwargs=dict(activation_fn=nn.ReLU, net_arch=[64, 64])\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MdtCim22GvZ"
      },
      "source": [
        "### c12: noise = 1.5, memory = 4\n",
        "\n",
        "self.dt = 0.25\n",
        "discount factor = 0.995\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjIdEF9uVftO"
      },
      "outputs": [],
      "source": [
        "retrieve_dir = \"/content/gdrive/MyDrive/fireflies_agent/July_20_6/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdjZnidh2Gvb"
      },
      "outputs": [],
      "source": [
        "# obs = [angle, distance, memory, angle2, distance2, memory2, ...]\n",
        "# before = [angle, angle2 ..., distance, distance2, ... memory, memory2]\n",
        "\n",
        "class MultiFF(Env):\n",
        "  def __init__(self):\n",
        "      super(MultiFF, self).__init__()\n",
        "      self.num_ff = 200\n",
        "      self.arena_radius = 1000\n",
        "      self.episode_len = 10000 \n",
        "      self.dt = 0.25 \n",
        "      #self.current_episode = 0\n",
        "      #self.action_space = spaces.Box(low=-1., high=1., shape=(2,),dtype=np.float32)\n",
        "      self.action_space = spaces.Box(low=-1., high=1., shape=(2,),dtype=np.float32)\n",
        "      self.obs_ff = 2\n",
        "      self.observation_space = spaces.Box(low=-1., high=1., shape=(self.obs_ff*4,),dtype=np.float32)\n",
        "      self.terminal_vel = 0.01\n",
        "      #self.pro_noise_std = 0.005\n",
        "      self.vgain = 200\n",
        "      self.wgain = pi/2\n",
        "      self.reward_per_ff = 100\n",
        "      #self.time_cost = 0.005\n",
        "      #self.total_time = 0\n",
        "      #self.zero_action = False\n",
        "      #self.target_update_counter_num = 20\n",
        "      #self.reward_per_episode = []\n",
        "      #self.update_slots = True\n",
        "      #self.closest_ff_distance = 200\n",
        "      self.pro_noise_std = 0.005\n",
        "      self.epi_num = -1\n",
        "      self.has_sped_up_before = False\n",
        "      self.full_memory = 4\n",
        "      self.internal_noise_factor = 1.5\n",
        "      self.ff_memory_all = torch.ones([self.num_ff,])*self.full_memory\n",
        "      self.invisible_distance = 400\n",
        "      self.invisible_angle = 2*pi/9\n",
        "      self.reward_boundary = 25\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self.epi_num += 1\n",
        "    print(\"\\n episode: \", self.epi_num)\n",
        "    self.num_targets = 0\n",
        "    #self.past_speeds = []\n",
        "    self.time = 0\n",
        "    #self.counter = 0\n",
        "    #self.current_target_index = torch.tensor([999], dtype=torch.int32)\n",
        "    #self.previous_target_index = self.current_target_index\n",
        "    self.ff_flash = []\n",
        "    self.has_sped_up_before = False\n",
        "    \n",
        "    for i in range(self.num_ff):\n",
        "      num_intervals = 1500\n",
        "      first_flash = torch.rand(1)\n",
        "      intervals = torch.poisson(torch.ones(num_intervals-1)*3)\n",
        "      t0 = torch.cat((first_flash, first_flash+torch.cumsum(intervals, dim=0)+torch.cumsum(torch.ones(num_intervals-1)*0.3, dim=0)))\n",
        "      t1 = t0 + torch.ones(num_intervals)*0.3\n",
        "      self.ff_flash.append(torch.stack((t0, t1), dim=1))\n",
        "\n",
        "    self.ffr = torch.sqrt(torch.rand(self.num_ff))*self.arena_radius # The radius of the arena changed from 1000 cm/s to 200 cm/s\n",
        "    self.fftheta = torch.rand(self.num_ff)*2*pi\n",
        "    #self.ffrt = torch.stack((self.ffr, self.fftheta), dim=1) \n",
        "    self.ffx = torch.cos(self.fftheta) * self.ffr\n",
        "    self.ffy = torch.sin(self.fftheta) * self.ffr\n",
        "    self.ffxy = torch.stack((self.ffx, self.ffy), dim=1)\n",
        "    self.ffx2 = self.ffx.clone()\n",
        "    self.ffy2 = self.ffy.clone()\n",
        "    self.ffxy2 = torch.stack((self.ffx2, self.ffy2), dim=1)\n",
        "    #self.ff_info={}\n",
        "    #self.update_slots = True\n",
        "    self.agentx = torch.tensor([0])\n",
        "    self.agenty = torch.tensor([0])\n",
        "    self.agentr = torch.zeros(1)\n",
        "    self.agentxy = torch.tensor([0, 0])\n",
        "    self.agentheading = torch.zeros(1).uniform_(0, 2*pi)\n",
        "    self.dv = torch.zeros(1).uniform_(-0.05, 0.05) \n",
        "    self.dw = torch.zeros(1)\n",
        "    self.end_episode = False \n",
        "    self.obs = self.beliefs().numpy()\n",
        "    #self.chunk_50s = 1\n",
        "    self.episode_reward = 0\n",
        "    #self.stop_rewarding_speed = False\n",
        "    #self.current_obs_steps = 0\n",
        "    self.ff_memory_all = torch.ones([self.num_ff,])\n",
        "    return self.obs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def calculate_reward(self):\n",
        "      #action_cost=((self.previous_action[1]-self.action[1])**2+(self.previous_action[0]-self.action[0])**2)*self.mag_cost\n",
        "      # To incorporate action_cost, we need to incorporate previous_action into decision_info\n",
        "      #self.total_time += 1\n",
        "      # In addition to rewarding the monkey for capturing the firefly, we also use different phases of rewards to teach monkey specific behaviours\n",
        "      #reward = -self.time_cost\n",
        "      reward = 0\n",
        "      # Reward shaping\n",
        "      # Phase I: reward the agent for learning to stop \n",
        "      # Always: reward the agent for capturing fireflies\n",
        "      self.num_targets = 0\n",
        "      if abs(self.sys_vel[1]) <= self.terminal_vel:\n",
        "        captured_ff_index = (self.ff_distance_all <= self.reward_boundary).nonzero().reshape(-1).tolist()\n",
        "        self.captured_ff_index = captured_ff_index\n",
        "        num_targets = len(captured_ff_index)\n",
        "        self.num_targets = num_targets\n",
        "        if num_targets > 0: # If the monkey hs captured at least 1 ff\n",
        "          # Calculate reward\n",
        "          reward = reward + self.reward_per_ff * num_targets\n",
        "          # Replace the captured ffs with ffs of new locations\n",
        "          self.ffr[captured_ff_index]= torch.sqrt(torch.rand(num_targets))*self.arena_radius\n",
        "          self.fftheta[captured_ff_index]= torch.rand(num_targets)*2*pi\n",
        "          #self.ffrt = torch.stack((self.ffr, self.fftheta), dim=1) \n",
        "          self.ffx[captured_ff_index] = torch.cos(self.fftheta[captured_ff_index]) * self.ffr[captured_ff_index]\n",
        "          self.ffy[captured_ff_index] = torch.sin(self.fftheta[captured_ff_index]) * self.ffr[captured_ff_index]\n",
        "          self.ffxy = torch.stack((self.ffx, self.ffy), dim=1)\n",
        "          self.ffx2[captured_ff_index] = self.ffx[captured_ff_index].clone()\n",
        "          self.ffy2[captured_ff_index] = self.ffy[captured_ff_index].clone()\n",
        "          self.ffxy2 = torch.stack((self.ffx2, self.ffy2), dim=1)\n",
        "          # Delete the information from self.ff_info\n",
        "          #[self.ff_info.pop(key) for key in captured_ff_index if (key in self.ff_info)]\n",
        "          #self.current_target_index = torch.tensor([999], dtype=torch.int32)\n",
        "          #self.previous_target_index = self.current_target_index\n",
        "          # Reward the firefly based on the average speed before capturing the firefly\n",
        "          ##reward = reward + sum(self.past_speeds)/len(self.past_speeds)\n",
        "          #print(round(self.time, 2), \"sys_vel: \", [round(i, 4) for i in self.sys_vel.tolist()], \"obs: \", list(np.round(self.obs, decimals = 2)), \"n_targets: \",  num_targets)\n",
        "          print(round(self.time, 2), \"sys_vel: \", [round(i, 4) for i in self.sys_vel.tolist()], \"n_targets: \",  num_targets)\n",
        "          #self.update_slots = True\n",
        "          #self.stop_rewarding_speed = True\n",
        "        #elif self.has_sped_up_before == True:\n",
        "        # based on Ruiyi's formula, using the distance of the closest ff in obs\n",
        "        #reward += math.exp(-((self.ff_current[1, 0]**2)*(25/1.5)**2)/2)\n",
        "        #self.has_sped_up_before = False\n",
        "      return reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    self.time += self.dt\n",
        "    action=torch.tensor(action)\n",
        "    action[1] = action[1]/2+0.5\n",
        "    self.sys_vel=action.clone()\n",
        "    self.state_step(action)\n",
        "    self.obs = self.beliefs().numpy()\n",
        "    reward=self.calculate_reward()\n",
        "    self.episode_reward += reward\n",
        "    \n",
        "    if self.time >= self.episode_len*self.dt:\n",
        "      self.end_episode = True\n",
        "      #self.current_episode += 1\n",
        "      print(\"Reward for the episode: \", self.episode_reward)\n",
        "    #print(\"action: \", torch.round(action, decimals = 3), \"obs: \", np.round(self.obs, decimals = 3), \"Reward: \", round(reward, 3))\n",
        "    return self.obs, reward, self.end_episode, {}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def state_step(self,action):\n",
        "    vnoise=torch.distributions.Normal(0,torch.ones([1,1])).sample()*self.pro_noise_std \n",
        "    wnoise=torch.distributions.Normal(0,torch.ones([1,1])).sample()*self.pro_noise_std\n",
        "    self.dw_normed = (action[0]+wnoise)\n",
        "    self.dv_normed = (action[1]+vnoise)\n",
        "    self.dw = (action[0]+wnoise)*self.wgain*self.dt\n",
        "    self.agentheading = self.agentheading + self.dw.item()\n",
        "    self.dv =  (action[1]+vnoise)*self.vgain*self.dt\n",
        "    self.dx= torch.cos(self.agentheading)*self.dv\n",
        "    self.dy= torch.sin(self.agentheading)*self.dv\n",
        "    self.agentx = self.agentx + self.dx.item()\n",
        "    self.agenty = self.agenty + self.dy.item()\n",
        "    self.agentxy = torch.cat((self.agentx, self.agenty))\n",
        "    self.agentr = vector_norm(self.agentxy)\n",
        "    self.agenttheta = torch.tensor(torch.atan2(self.agenty, self.agentx))  \n",
        "                               \n",
        "    if self.agentr >= self.arena_radius:\n",
        "      self.agentr = 2*self.arena_radius-self.agentr\n",
        "      self.agenttheta = self.agenttheta + pi\n",
        "      self.agentx = (self.agentr*torch.cos(self.agenttheta)).reshape(1,)\n",
        "      self.agenty = (self.agentr*torch.sin(self.agenttheta)).reshape(1,)\n",
        "      self.agentxy = torch.cat((self.agentx, self.agenty))\n",
        "      self.agentheading = self.agenttheta - pi\n",
        "    while self.agentheading >= 2*pi:\n",
        "      self.agentheading = self.agentheading - 2*pi\n",
        "    while self.agentheading < 0:\n",
        "      self.agentheading = self.agentheading + 2*pi   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def beliefs(self): \n",
        "\n",
        "    # Make a tensor containing the relative distance of all fireflies to the agent\n",
        "    self.ff_distance_all = vector_norm(self.ffxy - self.agentxy, dim=1)\n",
        "    # Make a tensor containing the relative (real) angle of all fireflies to the agent\n",
        "    ffradians = torch.atan2(self.ffy-self.agenty, self.ffx-self.agentx)\n",
        "    angle0 = ffradians - self.agentheading\n",
        "    angle0[angle0 > pi] = angle0[angle0 > pi] - 2*pi\n",
        "    angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2*pi\n",
        "    # Adjust the angle based on reward boundary\n",
        "    angle1 = torch.abs(angle0)-torch.abs(torch.arcsin(torch.div(self.reward_boundary, torch.clip( self.ff_distance_all, self.reward_boundary, 400) ))) # use torch clip to get valid arcsin input\n",
        "    angle2 = torch.clip(angle1,0,pi)\n",
        "    ff_angle_all = torch.sign(angle0)* angle2\n",
        "    # Update the tensor containing the uncertainties of all fireflies to the agent\n",
        "    visible_ff = torch.logical_and( self.ff_distance_all < self.invisible_distance, torch.abs(ff_angle_all) < self.invisible_angle)\n",
        "    self.visible_ff_indices0 = visible_ff.nonzero().reshape(-1)\n",
        "    for index in self.visible_ff_indices0:\n",
        "      ff = self.ff_flash[index]\n",
        "      if not torch.any(torch.logical_and(ff[:, 0] <= self.time, ff[:, 1] >= self.time)):\n",
        "        visible_ff[index] = False\n",
        "    self.visible_ff_indices = visible_ff.nonzero().reshape(-1)\n",
        "    # Update memory\n",
        "    self.ff_memory_all -= 1\n",
        "    self.ff_memory_all[self.visible_ff_indices] = self.full_memory\n",
        "    self.ff_memory_all = torch.clamp(self.ff_memory_all, 0, self.full_memory)\n",
        "    # Calculate the uncertainties that will be added to relative distance and angle based on memory\n",
        "    ff_uncertainty_all = (self.full_memory-self.ff_memory_all)*self.internal_noise_factor\n",
        "    self.ffx2 = self.ffx2 + torch.normal(torch.zeros([self.num_ff,]), ff_uncertainty_all)\n",
        "    self.ffy2 = self.ffy2 + torch.normal(torch.zeros([self.num_ff,]), ff_uncertainty_all)\n",
        "    self.ffx2[self.visible_ff_indices] = self.ffx[self.visible_ff_indices].clone()\n",
        "    self.ffy2[self.visible_ff_indices] = self.ffy[self.visible_ff_indices].clone()\n",
        "    self.ffxy2 = torch.stack((self.ffx2, self.ffy2), dim=1)\n",
        "    # find ffs that are in memory\n",
        "    self.ff_in_memory_indices = (self.ff_memory_all > 0).nonzero().reshape(-1)\n",
        "    # Consider the case where there are fewer than self.obs_ff fireflies that are in memory\n",
        "    \n",
        "\n",
        "   \n",
        "    if torch.numel(self.ff_in_memory_indices) >= self.obs_ff:\n",
        "      # Rank the ff whose \"memory\" is creater than 0 based on distance\n",
        "      sorted_indices =  torch.topk(-self.ff_distance_all[self.ff_in_memory_indices], self.obs_ff).indices\n",
        "      self.topk_indices = self.ff_in_memory_indices[sorted_indices]\n",
        "      self.ffxy2_topk = self.ffxy2[self.topk_indices]\n",
        "      self.ff_distance_topk = vector_norm(self.ffxy2_topk - self.agentxy, dim=1)\n",
        "      # Calculate relative angles \n",
        "      ffradians = torch.atan2(self.ffxy2_topk[:,1]-self.agenty, self.ffxy2_topk[:,0]-self.agentx)\n",
        "      angle0 = ffradians - self.agentheading\n",
        "      angle0[angle0 > pi] = angle0[angle0 > pi] - 2*pi\n",
        "      angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2*pi\n",
        "      self.ff_angle_topk_2 = angle0.clone()\n",
        "      # Calculate relative angles of all ffs based on reward boundaries\n",
        "      # Adjust the angle based on reward boundary\n",
        "      angle1 = torch.abs(angle0)-torch.abs(torch.arcsin(torch.div(self.reward_boundary, torch.clip(self.ff_distance_topk, self.reward_boundary, 400) ))) # use torch clip to get valid arcsin input\n",
        "      angle2 = torch.clip(angle1,0,pi)\n",
        "      ff_angle_topk_3 = torch.sign(angle0)* angle2\n",
        "      # Concatenate distance, angle, and memory\n",
        "      ff_array = torch.stack((self.ff_angle_topk_2, ff_angle_topk_3, self.ff_distance_topk,  self.ff_memory_all[self.topk_indices]), dim=0)\n",
        "    \n",
        "\n",
        "    elif torch.numel(self.ff_in_memory_indices) == 0:\n",
        "      ff_array = torch.tensor([[0], [0], [self.invisible_distance], [0]]).repeat([1, self.obs_ff])\n",
        "      self.topk_indices = torch.tensor([])\n",
        "\n",
        "    else:\n",
        "      sorted_distance, sorted_indices = torch.sort(-self.ff_distance_all[self.ff_in_memory_indices])\n",
        "      self.topk_indices = self.ff_in_memory_indices[sorted_indices]\n",
        "      self.ffxy2_topk = self.ffxy2[self.topk_indices ]\n",
        "      self.ff_distance_topk = vector_norm(self.ffxy2_topk - self.agentxy, dim=1)\n",
        "      # Calculate relative angles \n",
        "      ffradians = torch.atan2(self.ffxy2_topk[:,1]-self.agenty, self.ffxy2_topk[:,0]-self.agentx)\n",
        "      angle0 = ffradians - self.agentheading\n",
        "      angle0[angle0 > pi] = angle0[angle0 > pi] - 2*pi\n",
        "      angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2*pi\n",
        "      self.ff_angle_topk_2 = angle0.clone()\n",
        "      # Calculate relative angles of all ffs based on reward boundaries\n",
        "      # Adjust the angle based on reward boundary\n",
        "      angle1 = torch.abs(angle0)-torch.abs(torch.arcsin(torch.div(self.reward_boundary, torch.clip(self.ff_distance_topk, self.reward_boundary, 400) ))) # use torch clip to get valid arcsin input\n",
        "      angle2 = torch.clip(angle1,0,pi)\n",
        "      ff_angle_topk_3 = torch.sign(angle0)* angle2\n",
        "      # Concatenate distance, angle, and memory\n",
        "      ff_array0 = torch.stack((self.ff_angle_topk_2, ff_angle_topk_3, self.ff_distance_topk,  self.ff_memory_all[self.topk_indices]), dim=0)\n",
        "      needed_ff = self.obs_ff - torch.numel(self.ff_in_memory_indices)\n",
        "      ff_array = torch.stack([ff_array0.reshape([4,-1]), torch.tensor([[0], [0], [self.invisible_distance], [0]]).repeat([1, needed_ff])], dim=1)\n",
        "      \n",
        "\n",
        "    # ff_array[0:2,:] = ff_array[0:2,:]/pi\n",
        "    # ff_array[2,:] = (ff_array[2,:]/self.invisible_distance-0.5)*2\n",
        "    # ff_array[3,:] = (ff_array[3,:]/20-0.5)*2\n",
        "    self.ff_array = ff_array.clone()\n",
        "    return torch.flatten(ff_array.transpose(0, 1))\n",
        "\n",
        "\n",
        "env = MultiFF() \n",
        "env.reset() \n",
        "\n",
        "sac_model = SAC(\"MlpPolicy\", \n",
        "            env,\n",
        "            buffer_size=int(1e6),\n",
        "            batch_size=1024,\n",
        "            device='auto',\n",
        "            verbose=False,\n",
        "            train_freq=100,\n",
        "            learning_starts = int(10),\n",
        "            target_update_interval=20,\n",
        "            learning_rate=1e-2,\n",
        "            gamma=0.9999,\n",
        "            policy_kwargs=dict(activation_fn=nn.ReLU, net_arch=[64, 64])\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkOvyXSxuAjk"
      },
      "source": [
        "### c16: c15 + noise = 2, memory = 3\n",
        "\n",
        "(so that its epi_len is the same as LSTM)\n",
        "\n",
        "self.dt = 0.25\n",
        "discount factor = 0.995\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8wg1a0CrZ511"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7Ke_ikKuAjl"
      },
      "outputs": [],
      "source": [
        "retrieve_dir = \"/content/gdrive/MyDrive/fireflies_agent/Aug_10_2/\" # discount factor = 0.995  \n",
        "# eval rewards: 6720\n",
        "path = os.path.join(retrieve_dir, 'best_model.zip')\n",
        "path2 = os.path.join(retrieve_dir, 'buffer.pkl')\n",
        "sac_model = sac_model.load(path,env=env)  \n",
        "sac_model.load_replay_buffer(path2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLt44Ek9ZtCO"
      },
      "outputs": [],
      "source": [
        "# obs = [angle, distance, memory, angle2, distance2, memory2, ...]\n",
        "# before = [angle, angle2 ..., distance, distance2, ... memory, memory2]\n",
        "\n",
        "class MultiFF(Env):\n",
        "  def __init__(self):\n",
        "      super(MultiFF, self).__init__()\n",
        "      self.num_ff = 200\n",
        "      self.arena_radius = 1000\n",
        "      self.episode_len = 10000 \n",
        "      self.dt = 0.25 \n",
        "      #self.current_episode = 0\n",
        "      #self.action_space = spaces.Box(low=-1., high=1., shape=(2,),dtype=np.float32)\n",
        "      self.action_space = spaces.Box(low=-1., high=1., shape=(2,),dtype=np.float32)\n",
        "      self.obs_ff = 2\n",
        "      self.observation_space = spaces.Box(low=-1., high=1., shape=(self.obs_ff*4,),dtype=np.float32)\n",
        "      self.terminal_vel = 0.01\n",
        "      #self.pro_noise_std = 0.005\n",
        "      self.vgain = 200\n",
        "      self.wgain = pi/2\n",
        "      self.reward_per_ff = 100\n",
        "      #self.time_cost = 0.005\n",
        "      #self.total_time = 0\n",
        "      #self.zero_action = False\n",
        "      #self.target_update_counter_num = 20\n",
        "      #self.reward_per_episode = []\n",
        "      #self.update_slots = True\n",
        "      #self.closest_ff_distance = 200\n",
        "      self.pro_noise_std = 0.005\n",
        "      self.epi_num = -1\n",
        "      self.has_sped_up_before = False\n",
        "      self.full_memory = 3\n",
        "      self.internal_noise_factor = 2\n",
        "      self.ff_memory_all = torch.ones([self.num_ff,])*self.full_memory\n",
        "      self.invisible_distance = 400\n",
        "      self.invisible_angle = 2*pi/9\n",
        "      self.reward_boundary = 25\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self.epi_num += 1\n",
        "    print(\"\\n episode: \", self.epi_num)\n",
        "    self.num_targets = 0\n",
        "    #self.past_speeds = []\n",
        "    self.time = 0\n",
        "    #self.counter = 0\n",
        "    #self.current_target_index = torch.tensor([999], dtype=torch.int32)\n",
        "    #self.previous_target_index = self.current_target_index\n",
        "    self.ff_flash = []\n",
        "    self.has_sped_up_before = False\n",
        "    \n",
        "    for i in range(self.num_ff):\n",
        "      num_intervals = 1500\n",
        "      first_flash = torch.rand(1)\n",
        "      intervals = torch.poisson(torch.ones(num_intervals-1)*3)\n",
        "      t0 = torch.cat((first_flash, first_flash+torch.cumsum(intervals, dim=0)+torch.cumsum(torch.ones(num_intervals-1)*0.3, dim=0)))\n",
        "      t1 = t0 + torch.ones(num_intervals)*0.3\n",
        "      self.ff_flash.append(torch.stack((t0, t1), dim=1))\n",
        "\n",
        "    self.ffr = torch.sqrt(torch.rand(self.num_ff))*self.arena_radius # The radius of the arena changed from 1000 cm/s to 200 cm/s\n",
        "    self.fftheta = torch.rand(self.num_ff)*2*pi\n",
        "    #self.ffrt = torch.stack((self.ffr, self.fftheta), dim=1) \n",
        "    self.ffx = torch.cos(self.fftheta) * self.ffr\n",
        "    self.ffy = torch.sin(self.fftheta) * self.ffr\n",
        "    self.ffxy = torch.stack((self.ffx, self.ffy), dim=1)\n",
        "    self.ffx2 = self.ffx.clone()\n",
        "    self.ffy2 = self.ffy.clone()\n",
        "    self.ffxy2 = torch.stack((self.ffx2, self.ffy2), dim=1)\n",
        "    #self.ff_info={}\n",
        "    #self.update_slots = True\n",
        "    self.agentx = torch.tensor([0])\n",
        "    self.agenty = torch.tensor([0])\n",
        "    self.agentr = torch.zeros(1)\n",
        "    self.agentxy = torch.tensor([0, 0])\n",
        "    self.agentheading = torch.zeros(1).uniform_(0, 2*pi)\n",
        "    self.dv = torch.zeros(1).uniform_(-0.05, 0.05) \n",
        "    self.dw = torch.zeros(1)\n",
        "    self.end_episode = False \n",
        "    self.obs = self.beliefs().numpy()\n",
        "    #self.chunk_50s = 1\n",
        "    self.episode_reward = 0\n",
        "    #self.stop_rewarding_speed = False\n",
        "    #self.current_obs_steps = 0\n",
        "    self.ff_memory_all = torch.ones([self.num_ff,])\n",
        "    return self.obs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def calculate_reward(self):\n",
        "      #action_cost=((self.previous_action[1]-self.action[1])**2+(self.previous_action[0]-self.action[0])**2)*self.mag_cost\n",
        "      # To incorporate action_cost, we need to incorporate previous_action into decision_info\n",
        "      #self.total_time += 1\n",
        "      # In addition to rewarding the monkey for capturing the firefly, we also use different phases of rewards to teach monkey specific behaviours\n",
        "      #reward = -self.time_cost\n",
        "      reward = 0\n",
        "      # Reward shaping\n",
        "      # Phase I: reward the agent for learning to stop \n",
        "      # Always: reward the agent for capturing fireflies\n",
        "      self.num_targets = 0\n",
        "      if abs(self.sys_vel[1]) <= self.terminal_vel:\n",
        "        captured_ff_index = (self.ff_distance_all <= self.reward_boundary).nonzero().reshape(-1).tolist()\n",
        "        self.captured_ff_index = captured_ff_index\n",
        "        num_targets = len(captured_ff_index)\n",
        "        self.num_targets = num_targets\n",
        "        if num_targets > 0: # If the monkey hs captured at least 1 ff\n",
        "          # Calculate reward\n",
        "          reward = reward + self.reward_per_ff * num_targets\n",
        "          # Replace the captured ffs with ffs of new locations\n",
        "          self.ffr[captured_ff_index]= torch.sqrt(torch.rand(num_targets))*self.arena_radius\n",
        "          self.fftheta[captured_ff_index]= torch.rand(num_targets)*2*pi\n",
        "          #self.ffrt = torch.stack((self.ffr, self.fftheta), dim=1) \n",
        "          self.ffx[captured_ff_index] = torch.cos(self.fftheta[captured_ff_index]) * self.ffr[captured_ff_index]\n",
        "          self.ffy[captured_ff_index] = torch.sin(self.fftheta[captured_ff_index]) * self.ffr[captured_ff_index]\n",
        "          self.ffxy = torch.stack((self.ffx, self.ffy), dim=1)\n",
        "          self.ffx2[captured_ff_index] = self.ffx[captured_ff_index].clone()\n",
        "          self.ffy2[captured_ff_index] = self.ffy[captured_ff_index].clone()\n",
        "          self.ffxy2 = torch.stack((self.ffx2, self.ffy2), dim=1)\n",
        "          # Delete the information from self.ff_info\n",
        "          #[self.ff_info.pop(key) for key in captured_ff_index if (key in self.ff_info)]\n",
        "          #self.current_target_index = torch.tensor([999], dtype=torch.int32)\n",
        "          #self.previous_target_index = self.current_target_index\n",
        "          # Reward the firefly based on the average speed before capturing the firefly\n",
        "          ##reward = reward + sum(self.past_speeds)/len(self.past_speeds)\n",
        "          #print(round(self.time, 2), \"sys_vel: \", [round(i, 4) for i in self.sys_vel.tolist()], \"obs: \", list(np.round(self.obs, decimals = 2)), \"n_targets: \",  num_targets)\n",
        "          print(round(self.time, 2), \"sys_vel: \", [round(i, 4) for i in self.sys_vel.tolist()], \"n_targets: \",  num_targets)\n",
        "          #self.update_slots = True\n",
        "          #self.stop_rewarding_speed = True\n",
        "        #elif self.has_sped_up_before == True:\n",
        "        # based on Ruiyi's formula, using the distance of the closest ff in obs\n",
        "        #reward += math.exp(-((self.ff_current[1, 0]**2)*(25/1.5)**2)/2)\n",
        "        #self.has_sped_up_before = False\n",
        "      return reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    self.time += self.dt\n",
        "    action=torch.tensor(action)\n",
        "    action[1] = action[1]/2+0.5\n",
        "    self.sys_vel=action.clone()\n",
        "    self.state_step(action)\n",
        "    self.obs = self.beliefs().numpy()\n",
        "    reward=self.calculate_reward()\n",
        "    self.episode_reward += reward\n",
        "    \n",
        "    if self.time >= self.episode_len*self.dt:\n",
        "      self.end_episode = True\n",
        "      #self.current_episode += 1\n",
        "      print(\"Reward for the episode: \", self.episode_reward)\n",
        "    #print(\"action: \", torch.round(action, decimals = 3), \"obs: \", np.round(self.obs, decimals = 3), \"Reward: \", round(reward, 3))\n",
        "    return self.obs, reward, self.end_episode, {}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def state_step(self,action):\n",
        "    vnoise=torch.distributions.Normal(0,torch.ones([1,1])).sample()*self.pro_noise_std \n",
        "    wnoise=torch.distributions.Normal(0,torch.ones([1,1])).sample()*self.pro_noise_std\n",
        "    self.dw_normed = (action[0]+wnoise)\n",
        "    self.dv_normed = (action[1]+vnoise)\n",
        "    self.dw = (action[0]+wnoise)*self.wgain*self.dt\n",
        "    self.agentheading = self.agentheading + self.dw.item()\n",
        "    self.dv =  (action[1]+vnoise)*self.vgain*self.dt\n",
        "    self.dx= torch.cos(self.agentheading)*self.dv\n",
        "    self.dy= torch.sin(self.agentheading)*self.dv\n",
        "    self.agentx = self.agentx + self.dx.item()\n",
        "    self.agenty = self.agenty + self.dy.item()\n",
        "    self.agentxy = torch.cat((self.agentx, self.agenty))\n",
        "    self.agentr = vector_norm(self.agentxy)\n",
        "    self.agenttheta = torch.tensor(torch.atan2(self.agenty, self.agentx))  \n",
        "                               \n",
        "    if self.agentr >= self.arena_radius:\n",
        "      self.agentr = 2*self.arena_radius-self.agentr\n",
        "      self.agenttheta = self.agenttheta + pi\n",
        "      self.agentx = (self.agentr*torch.cos(self.agenttheta)).reshape(1,)\n",
        "      self.agenty = (self.agentr*torch.sin(self.agenttheta)).reshape(1,)\n",
        "      self.agentxy = torch.cat((self.agentx, self.agenty))\n",
        "      self.agentheading = self.agenttheta - pi\n",
        "    while self.agentheading >= 2*pi:\n",
        "      self.agentheading = self.agentheading - 2*pi\n",
        "    while self.agentheading < 0:\n",
        "      self.agentheading = self.agentheading + 2*pi   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def beliefs(self): \n",
        "\n",
        "    # Make a tensor containing the relative distance of all fireflies to the agent\n",
        "    self.ff_distance_all = vector_norm(self.ffxy - self.agentxy, dim=1)\n",
        "    # Make a tensor containing the relative (real) angle of all fireflies to the agent\n",
        "    ffradians = torch.atan2(self.ffy-self.agenty, self.ffx-self.agentx)\n",
        "    angle0 = ffradians - self.agentheading\n",
        "    angle0[angle0 > pi] = angle0[angle0 > pi] - 2*pi\n",
        "    angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2*pi\n",
        "    # Adjust the angle based on reward boundary\n",
        "    angle1 = torch.abs(angle0)-torch.abs(torch.arcsin(torch.div(self.reward_boundary, torch.clip( self.ff_distance_all, self.reward_boundary, 400) ))) # use torch clip to get valid arcsin input\n",
        "    angle2 = torch.clip(angle1,0,pi)\n",
        "    ff_angle_all = torch.sign(angle0)* angle2\n",
        "    # Update the tensor containing the uncertainties of all fireflies to the agent\n",
        "    visible_ff = torch.logical_and( self.ff_distance_all < self.invisible_distance, torch.abs(ff_angle_all) < self.invisible_angle)\n",
        "    self.visible_ff_indices0 = visible_ff.nonzero().reshape(-1)\n",
        "    for index in self.visible_ff_indices0:\n",
        "      ff = self.ff_flash[index]\n",
        "      if not torch.any(torch.logical_and(ff[:, 0] <= self.time, ff[:, 1] >= self.time)):\n",
        "        visible_ff[index] = False\n",
        "    self.visible_ff_indices = visible_ff.nonzero().reshape(-1)\n",
        "    # Update memory\n",
        "    self.ff_memory_all -= 1\n",
        "    self.ff_memory_all[self.visible_ff_indices] = self.full_memory\n",
        "    self.ff_memory_all = torch.clamp(self.ff_memory_all, 0, self.full_memory)\n",
        "    # Calculate the uncertainties that will be added to relative distance and angle based on memory\n",
        "    ff_uncertainty_all = (self.full_memory-self.ff_memory_all)*self.internal_noise_factor\n",
        "    self.ffx2 = self.ffx2 + torch.normal(torch.zeros([self.num_ff,]), ff_uncertainty_all)\n",
        "    self.ffy2 = self.ffy2 + torch.normal(torch.zeros([self.num_ff,]), ff_uncertainty_all)\n",
        "    self.ffx2[self.visible_ff_indices] = self.ffx[self.visible_ff_indices].clone()\n",
        "    self.ffy2[self.visible_ff_indices] = self.ffy[self.visible_ff_indices].clone()\n",
        "    self.ffxy2 = torch.stack((self.ffx2, self.ffy2), dim=1)\n",
        "    # find ffs that are in memory\n",
        "    self.ff_in_memory_indices = (self.ff_memory_all > 0).nonzero().reshape(-1)\n",
        "    # Consider the case where there are fewer than self.obs_ff fireflies that are in memory\n",
        "    \n",
        "\n",
        "   \n",
        "    if torch.numel(self.ff_in_memory_indices) >= self.obs_ff:\n",
        "      # Rank the ff whose \"memory\" is creater than 0 based on distance\n",
        "      sorted_indices =  torch.topk(-self.ff_distance_all[self.ff_in_memory_indices], self.obs_ff).indices\n",
        "      self.topk_indices = self.ff_in_memory_indices[sorted_indices]\n",
        "      self.ffxy2_topk = self.ffxy2[self.topk_indices]\n",
        "      self.ff_distance_topk = vector_norm(self.ffxy2_topk - self.agentxy, dim=1)\n",
        "      # Calculate relative angles \n",
        "      ffradians = torch.atan2(self.ffxy2_topk[:,1]-self.agenty, self.ffxy2_topk[:,0]-self.agentx)\n",
        "      angle0 = ffradians - self.agentheading\n",
        "      angle0[angle0 > pi] = angle0[angle0 > pi] - 2*pi\n",
        "      angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2*pi\n",
        "      self.ff_angle_topk_2 = angle0.clone()\n",
        "      # Calculate relative angles of all ffs based on reward boundaries\n",
        "      # Adjust the angle based on reward boundary\n",
        "      angle1 = torch.abs(angle0)-torch.abs(torch.arcsin(torch.div(self.reward_boundary, torch.clip(self.ff_distance_topk, self.reward_boundary, 400) ))) # use torch clip to get valid arcsin input\n",
        "      angle2 = torch.clip(angle1,0,pi)\n",
        "      ff_angle_topk_3 = torch.sign(angle0)* angle2\n",
        "      # Concatenate distance, angle, and memory\n",
        "      ff_array = torch.stack((self.ff_angle_topk_2, ff_angle_topk_3, self.ff_distance_topk,  self.ff_memory_all[self.topk_indices]), dim=0)\n",
        "    \n",
        "\n",
        "    elif torch.numel(self.ff_in_memory_indices) == 0:\n",
        "      ff_array = torch.tensor([[0], [0], [self.invisible_distance], [0]]).repeat([1, self.obs_ff])\n",
        "      self.topk_indices = torch.tensor([])\n",
        "\n",
        "    else:\n",
        "      sorted_distance, sorted_indices = torch.sort(-self.ff_distance_all[self.ff_in_memory_indices])\n",
        "      self.topk_indices = self.ff_in_memory_indices[sorted_indices]\n",
        "      self.ffxy2_topk = self.ffxy2[self.topk_indices ]\n",
        "      self.ff_distance_topk = vector_norm(self.ffxy2_topk - self.agentxy, dim=1)\n",
        "      # Calculate relative angles \n",
        "      ffradians = torch.atan2(self.ffxy2_topk[:,1]-self.agenty, self.ffxy2_topk[:,0]-self.agentx)\n",
        "      angle0 = ffradians - self.agentheading\n",
        "      angle0[angle0 > pi] = angle0[angle0 > pi] - 2*pi\n",
        "      angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2*pi\n",
        "      self.ff_angle_topk_2 = angle0.clone()\n",
        "      # Calculate relative angles of all ffs based on reward boundaries\n",
        "      # Adjust the angle based on reward boundary\n",
        "      angle1 = torch.abs(angle0)-torch.abs(torch.arcsin(torch.div(self.reward_boundary, torch.clip(self.ff_distance_topk, self.reward_boundary, 400) ))) # use torch clip to get valid arcsin input\n",
        "      angle2 = torch.clip(angle1,0,pi)\n",
        "      ff_angle_topk_3 = torch.sign(angle0)* angle2\n",
        "      # Concatenate distance, angle, and memory\n",
        "      ff_array0 = torch.stack((self.ff_angle_topk_2, ff_angle_topk_3, self.ff_distance_topk,  self.ff_memory_all[self.topk_indices]), dim=0)\n",
        "      needed_ff = self.obs_ff - torch.numel(self.ff_in_memory_indices)\n",
        "      ff_array = torch.stack([ff_array0.reshape([4,-1]), torch.tensor([[0], [0], [self.invisible_distance], [0]]).repeat([1, needed_ff])], dim=1)\n",
        "      \n",
        "\n",
        "    # ff_array[0:2,:] = ff_array[0:2,:]/pi\n",
        "    # ff_array[2,:] = (ff_array[2,:]/self.invisible_distance-0.5)*2\n",
        "    # ff_array[3,:] = (ff_array[3,:]/20-0.5)*2\n",
        "    self.ff_array = ff_array.clone()\n",
        "    return torch.flatten(ff_array.transpose(0, 1))\n",
        "\n",
        "\n",
        "env = MultiFF() \n",
        "env.reset() \n",
        "\n",
        "sac_model = SAC(\"MlpPolicy\", \n",
        "            env,\n",
        "            buffer_size=int(1e6),\n",
        "            batch_size=1024,\n",
        "            device='auto',\n",
        "            verbose=False,\n",
        "            train_freq=100,\n",
        "            learning_starts = int(10),\n",
        "            target_update_interval=20,\n",
        "            learning_rate=1e-2,\n",
        "            gamma=0.9999,\n",
        "            policy_kwargs=dict(activation_fn=nn.ReLU, net_arch=[64, 64])\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HacTiGzgSYtO"
      },
      "source": [
        "### Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuZzoKkXSlTj"
      },
      "outputs": [],
      "source": [
        "class CollectInformation(MultiFF):  # Note when using this wrapper, the number of steps cannot exceed one episode\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "      self.ff_information = np.ones([self.num_ff, 8])*(-9999)   #[index, x, y, time_start, time_captured, mx(when_captured), my(when_captured), index_in_flash]\n",
        "\n",
        "  def reset(self):\n",
        "      self.obs = super().reset()\n",
        "      self.ff_information[:,0] = np.arange(self.num_ff)\n",
        "      self.ff_information[:,7] = np.arange(self.num_ff)\n",
        "      self.ff_information[:,1] = self.ffx.numpy()\n",
        "      self.ff_information[:,2] = self.ffy.numpy()\n",
        "      self.ff_information[:,3] = 0\n",
        "      return self.obs\n",
        "\n",
        "  def calculate_reward(self):\n",
        "      reward = super().calculate_reward()    \n",
        "      if self.num_targets > 0: \n",
        "        for index in self.captured_ff_index:\n",
        "          overall_index = int(self.ff_information[:,0][np.where(self.ff_information[:,-1]==index)[0][-1]])\n",
        "          self.ff_information[overall_index, 4] = self.time\n",
        "          self.ff_information[overall_index, 5] = self.agentx.item()\n",
        "          self.ff_information[overall_index, 6] = self.agenty.item()\n",
        "        self.new_ff_info = np.ones([self.num_targets, 8])*(-9999)\n",
        "        self.new_ff_info[:,0] = np.arange(len(self.ff_information), len(self.ff_information)+self.num_targets)\n",
        "        self.new_ff_info[:,7] = np.array(self.captured_ff_index)\n",
        "        self.new_ff_info[:,1] = self.ffx[self.captured_ff_index].numpy()\n",
        "        self.new_ff_info[:,2] = self.ffy[self.captured_ff_index].numpy()\n",
        "        self.new_ff_info[:,3] = self.time\n",
        "        self.ff_information = np.concatenate([self.ff_information, self.new_ff_info], axis = 0)\n",
        "      return(reward)\n",
        "  #def reset_wrapper(self):\n",
        "\n",
        "\n",
        "\n",
        "env = CollectInformation()\n",
        "\n",
        "\n",
        "# Create and wrap the environment\n",
        "log_dir = \"/content/gdrive/MyDrive/fireflies_agent/July_20_6/\" # discount factor = 0.999\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "env = Monitor(env, log_dir)\n",
        "env.reset() \n",
        "\n",
        "env.flash_on_interval = 0.3\n",
        "env.distance2center_cost = 0\n",
        "\n",
        "# For direct training\n",
        "sac_model = SAC(\"MlpPolicy\", \n",
        "            env,\n",
        "            gamma=0.995,\n",
        "            learning_rate=0.0015,\n",
        "            batch_size=1024,\n",
        "            target_update_interval=50,\n",
        "            buffer_size=1000000,\n",
        "            learning_starts=10000,\n",
        "            train_freq=10,\n",
        "            ent_coef=0.00083,\n",
        "            policy_kwargs=dict(activation_fn=nn.Tanh, net_arch=[128, 128])\n",
        "                )\n",
        "\n",
        "\n",
        "path = os.path.join(retrieve_dir, 'best_model.zip')\n",
        "path2 = os.path.join(retrieve_dir, 'buffer.pkl')\n",
        "sac_model = sac_model.load(path,env=env)  \n",
        "sac_model.load_replay_buffer(path2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCYLGugxIVk9"
      },
      "source": [
        "### Test (SB3)\n",
        "\n",
        "note: num_steps cannot exceed episode_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsM-EubOIpfJ"
      },
      "outputs": [],
      "source": [
        "# Test the trained agent\n",
        "obs = env.reset()\n",
        "n_steps = 9900\n",
        "mx = []\n",
        "my = []\n",
        "monkey_t = []\n",
        "monkey_speed_info = []\n",
        "mheading = [] # in radians\n",
        "ffxy_all = []\n",
        "ffxy_visible = []\n",
        "ffxy2_all = []\n",
        "time_rl = []\n",
        "reward_log = []\n",
        "mx_rewarded = []\n",
        "my_rewarded = []\n",
        "captured_ff = []\n",
        "current_target = []\n",
        "action_reward = []\n",
        "num_targets = []\n",
        "env_obs = []\n",
        "#captured_ff_cum_x = []\n",
        "#captured_ff_cum_y = []\n",
        "all_captured_ff_x = []\n",
        "all_captured_ff_y = []\n",
        "visible_ff_indices_all = []\n",
        "memory_ff_indices_all = []\n",
        "obs_ff_indices_all = []\n",
        "obs_ff_overall_indices_all = []\n",
        "memory_all = []\n",
        "count_targets = 0\n",
        "ff_angles2 = []\n",
        "ff_distances2 = []\n",
        "\n",
        "\n",
        "for step in range(n_steps):\n",
        "  action, _ = sac_model.predict(obs, deterministic=True)\n",
        "  #print(\"Step {}\".format(step + 1))\n",
        "  #print(\"distance, action, reward\", np.concatenate([np.array([env.closest_ff_distance]), np.array([action[0]]), np.array([action[1]]), np.array([reward])]))\n",
        "  previous_ffxy = env.ffxy\n",
        "  prev_ff_information = env.ff_information.copy()\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print(step, \"Action: \", action)\n",
        "  reward_log.append(reward)\n",
        "  #print(env.num_targets)\n",
        "  #action_reward.append(np.concatenate([action, np.array([reward])]))\n",
        "  #print(step, np.concatenate([np.round(action, 4), np.round(obs, 4)]), reward)\n",
        "  num_targets.append(env.num_targets)\n",
        "  memory_all.append(env.ff_memory_all)\n",
        "  #if reward > 99:\n",
        "    #for i in env.captured_ff_index:\n",
        "      #captured_ff.append([env.time, previous_ffxy[i,0].item(), previous_ffxy[i,1].item()])\n",
        "  if env.num_targets >0:\n",
        "    count_targets += env.num_targets\n",
        "    captured_ff.append(env.captured_ff_index)\n",
        "    all_captured_ff_x = all_captured_ff_x + previous_ffxy[env.captured_ff_index][:,0].tolist()\n",
        "    all_captured_ff_y = all_captured_ff_y + previous_ffxy[env.captured_ff_index][:,1].tolist()\n",
        "  else:\n",
        "    captured_ff.append(0)\n",
        "  #captured_ff_cum_x.append(all_captured_ff_x)\n",
        "  #captured_ff_cum_y.append(all_captured_ff_y)\n",
        "  mx.append(env.agentx.item())\n",
        "  my.append(env.agenty.item())\n",
        "  monkey_t.append(env.time)\n",
        "  monkey_speed_info.append(env.dv.item())\n",
        "  mheading.append(env.agentheading.item())\n",
        "  time_rl.append(env.time)\n",
        "  ffxy_all.append(env.ffxy.clone())\n",
        "  ffxy2_all.append(env.ffxy2.clone())\n",
        "  ffxy_visible.append(env.ffxy[env.visible_ff_indices].clone())\n",
        "  env_obs.append(obs)\n",
        "  visible_ff_indices_all.append(env.visible_ff_indices)\n",
        "  memory_ff_indices_all.append(env.ff_in_memory_indices)\n",
        "  obs_ff_indices_all.append(env.topk_indices)\n",
        "  real_indices = []\n",
        "  for index in env.topk_indices:\n",
        "    real_indices.append(int(prev_ff_information[:,0][np.where(prev_ff_information[:,7]==index.item())[0][-1]].copy()))\n",
        "  obs_ff_overall_indices_all.append(real_indices)\n",
        "  if len(env.topk_indices) > 0:\n",
        "    ff_angles2.append(env.ff_angle_topk_2)\n",
        "    ff_distances2.append(env.ff_distance_topk)\n",
        "  else:\n",
        "    ff_angles2.append(torch.tensor([]))\n",
        "    ff_distances2.append(torch.tensor([]))\n",
        "  if done:\n",
        "    obs = env.reset()\n",
        "  #print(step, ffxy_visible[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKXh34PFINq7"
      },
      "source": [
        "### monkey & ff information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogAh4Lu8-ps9"
      },
      "outputs": [],
      "source": [
        "monkey_speed = np.array(monkey_speed_info)\n",
        "\n",
        "monkey_information = {\n",
        "'monkey_x': np.array(mx),\n",
        "'monkey_y': np.array(my),\n",
        "'monkey_t': np.array(monkey_t),\n",
        "'monkey_speed': monkey_speed,\n",
        "'monkey_angle': np.array(mheading),\n",
        "}\n",
        "\n",
        "\n",
        "monkey_information['monkey_speeddummy'] = (monkey_speed>200*0.01*env.dt).astype(int) \n",
        "monkey_speeddummy = (monkey_speed>200*0.01*env.dt).astype(int) \n",
        "delta_time = np.delete(monkey_information['monkey_t'], 0) - np.delete(monkey_information['monkey_t'], -1)\n",
        "monkey_dw = np.diff(np.array(mheading), prepend=mheading[0])/env.dt\n",
        "monkey_information['monkey_dw'] = monkey_dw\n",
        "\n",
        "\n",
        "#ff: \n",
        "#[index, x, y, time_start, time_captured, mx(when_captured), my(when_captured), index_in_flash]\n",
        "# Note: when collecting firefly data, no more than one episode can be run. Otherwise the data might be messed up.\n",
        "# A solution is to elongate an episode\n",
        "\n",
        "\n",
        "# sort the time of capture for ff (if they have been captured)\n",
        "ff_information = env.ff_information.copy()\n",
        "ff_time_captured_all = ff_information[:,4]\n",
        "captured_ff_indices = np.where(ff_time_captured_all != -9999)[0]\n",
        "not_captured_ff_indices = np.where(ff_time_captured_all == -9999)[0]\n",
        "num_captured_ff = len(captured_ff_indices)\n",
        "sorted_indices_captured = captured_ff_indices[np.argsort(ff_time_captured_all[captured_ff_indices])]\n",
        "sort_indices_all = np.concatenate([sorted_indices_captured, not_captured_ff_indices])\n",
        "ff_information_sorted = ff_information[sort_indices_all]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# make ff_flash_sorted\n",
        "ff_flash_sorted = []\n",
        "env_end_time = env.time\n",
        "for ff in ff_information[sorted_indices_captured]:\n",
        "  flash = env.ff_flash[int(ff[7])].numpy()\n",
        "  replace_start = False\n",
        "  replace_end = False\n",
        "\n",
        "  # first flatten flash, and then find the elements that are before ff start time\n",
        "  # if the number of elements is even, then the flashing_start_time is the start time of the next interval\n",
        "  before_start = np.where(flash.flatten() <= ff[3])[0]\n",
        "  if len(before_start) > 0:\n",
        "    if len(before_start)%2 == 0:\n",
        "      start_flash_index = int(len(before_start)/2)\n",
        "    else:\n",
        "      start_flash_index = int((len(before_start)-1)/2)\n",
        "      replace_start = True\n",
        "  else:\n",
        "      start_flash_index = 0\n",
        "\n",
        "\n",
        "  after_finish = np.where(flash.flatten() >= ff[4])[0]\n",
        "  if len(after_finish) > 0:\n",
        "    num_indices_before_finish = after_finish[0]\n",
        "    if num_indices_before_finish%2 == 0:\n",
        "      end_flash_index = int(num_indices_before_finish/2) - 1\n",
        "    else: \n",
        "      end_flash_index = int((num_indices_before_finish+1)/2) - 1\n",
        "      replace_end = True\n",
        "  else:\n",
        "    end_flash_index = len(flash)-1\n",
        "\n",
        "\n",
        "  \n",
        "  ff_flash = flash[start_flash_index:(end_flash_index+1)]\n",
        "\n",
        "  if len(ff_flash) < 2:\n",
        "    ff_flash = flash[end_flash_index-1: end_flash_index+1]\n",
        "    if len(ff_flash) < 2:\n",
        "      ff_flash = np.array([[-1,-1]])\n",
        "\n",
        "\n",
        "  if replace_start == True:\n",
        "    ff_flash[0, 0] = ff[3]\n",
        "  if replace_end == True:\n",
        "    ff_flash[-1, 1] = ff[4]\n",
        "\n",
        "  if len(ff_flash) == 0:\n",
        "    ff_flash = np.array([[-1,-1]])\n",
        "\n",
        "  ff_flash_sorted.append(ff_flash)\n",
        "\n",
        "\n",
        "\n",
        "# For the ffs that have never been captured, the end_flash_time is evaluated not in relation to the \n",
        "# time of captrue, but to the time that the env ends\n",
        "for ff in ff_information[not_captured_ff_indices]:\n",
        "  flash = env.ff_flash[int(ff[7])].numpy()\n",
        "  replace_start = False\n",
        "\n",
        "\n",
        "  # first flatten flash, and then find the elements that are before ff start time\n",
        "  # if the number of elements is even, then the flashing_start_time is the start time of the next interval\n",
        "  before_start = np.where(flash.flatten() <= ff[3])[0]\n",
        "  if len(before_start)%2 == 0:\n",
        "    start_flash_index = int(len(before_start)/2)\n",
        "  else:\n",
        "    start_flash_index = int((len(before_start)-1)/2)\n",
        "    replace_start = True\n",
        "\n",
        "\n",
        "# # if we only want the part before the testing ends\n",
        "#   after_finish = np.where(flash.flatten() >= env_end_time)[0] # differing from captured ffs\n",
        "#   num_indices_before_finish = after_finish[0]\n",
        "#   if num_indices_before_finish%2 == 0:\n",
        "#     end_flash_index = int(num_indices_before_finish/2) - 1\n",
        "#   else: \n",
        "#     end_flash_index = int((num_indices_before_finish+1)/2) - 1\n",
        "#     replace_end = True\n",
        "#   ff_flash = flash[start_flash_index:(end_flash_index+1)]\n",
        "\n",
        "\n",
        "  ff_flash = flash[start_flash_index:]\n",
        "\n",
        "  if len(ff_flash) < 2:\n",
        "    ff_flash = flash[end_flash_index-1: end_flash_index+1]\n",
        "    if len(ff_flash) < 2:\n",
        "      ff_flash = np.array([[-1,-1]])\n",
        "\n",
        "\n",
        "  if replace_start == True:\n",
        "    ff_flash[0, 0] = ff[3]\n",
        "\n",
        "  # # if we only want the part before the testing ends\n",
        "  # if replace_end == True:\n",
        "  #   ff_flash[-1, 1] = ff[4]\n",
        "  if len(ff_flash) == 0:\n",
        "    ff_flash = np.array([[-1,-1]])\n",
        "\n",
        "  ff_flash_sorted.append(ff_flash)\n",
        "\n",
        "\n",
        "ff_catched_T_sorted = ff_time_captured_all[sorted_indices_captured] # Note that these two will be shorter than the other arrays\n",
        "ff_believed_position_sorted = ff_information[:,5:7][sorted_indices_captured]\n",
        "\n",
        "ff_real_position_sorted = ff_information[:,1:3][sort_indices_all]\n",
        "ff_life_sorted = ff_information[:,3:5][sort_indices_all]\n",
        "ff_life_sorted[:,1][np.where(ff_life_sorted[:,1]==-9999)[0]] = env.time\n",
        "ff_flash_end_sorted = [flash[-1,1] if len(flash) > 0 else env.time for flash in ff_flash_sorted]\n",
        "ff_flash_end_sorted = np.array(ff_flash_end_sorted)\n",
        "\n",
        "catched_ff_num = len(ff_catched_T_sorted) - 200\n",
        "total_ff_num = len(ff_life_sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0G-pEI6Q_CH"
      },
      "source": [
        "## Data from agent (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XQiUVvBAFNy"
      },
      "source": [
        "### env (LSTM)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v42eVAFVRGpn"
      },
      "source": [
        "#### e1: one firefly env, no noise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vYLvxmXRGpn"
      },
      "outputs": [],
      "source": [
        "# self.distance_noise_factor = 1/6, self.angle_noise_factor  = 1/8\n",
        "model_path = \"/content/gdrive/MyDrive/fireflies_agent/lstm/July_26\" \n",
        "\n",
        "# best_avg_rewards = 7124\n",
        "\n",
        "# A sample of actions:\n",
        "# 95 Action:  [ 0.31841782 -0.999913  ]\n",
        "# 96 Action:  [-0.07380565  0.53944504]\n",
        "# 97 Action:  [-0.22972749 -0.9984572 ]\n",
        "# 98 Action:  [-0.7427281   0.40428725]\n",
        "# 99 Action:  [0.7400373  0.76522696]\n",
        "# 100 Action:  [-0.34833822 -0.99916583]\n",
        "# 101 Action:  [0.41152725 0.41533998]\n",
        "# 102 Action:  [ 0.940351   -0.99330115]\n",
        "# 103 Action:  [0.9275105  0.40295646]\n",
        "# 104 Action:  [ 0.17494562 -0.9831464 ]\n",
        "# 26.0 sys_vel:  [0.1749, 0.0084] n_targets:  1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHohmj00RGpn"
      },
      "outputs": [],
      "source": [
        "gamma= 0.995\n",
        "soft_q_lr = 0.0015\n",
        "policy_lr = 0.003  \n",
        "alpha_lr = 0.002 \n",
        "update_itr= 1\n",
        "hidden_dim= 128\n",
        "reward_scale= 10\n",
        "target_entropy= -2\n",
        "soft_tau= 0.015"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mQxcOSbRGpn"
      },
      "outputs": [],
      "source": [
        "# obs = [angle, distance, memory, angle2, distance2, memory2, ...]\n",
        "# before = [angle, angle2 ..., distance, distance2, ... memory, memory2]\n",
        "\n",
        "\n",
        "reward_per_episode = 0\n",
        "\n",
        "class MultiFF(Env):\n",
        "  def __init__(self):\n",
        "      super(MultiFF, self).__init__()\n",
        "      self.num_ff = 200\n",
        "      self.arena_radius = 1000\n",
        "      self.episode_len = 15000\n",
        "      self.dt = 0.25 \n",
        "      #self.current_episode = 0\n",
        "      #self.action_space = spaces.Box(low=-1., high=1., shape=(2,),dtype=np.float32)\n",
        "      self.action_space = spaces.Box(low=-1., high=1., shape=(2,),dtype=np.float32)\n",
        "      self.obs_ff = 1\n",
        "      self.observation_space = spaces.Box(low=-1., high=1., shape=(self.obs_ff*3,),dtype=np.float32)\n",
        "      self.terminal_vel = 0.01\n",
        "      #self.pro_noise_std = 0.005\n",
        "      self.vgain = 200\n",
        "      self.wgain = pi/4\n",
        "      self.reward_per_ff = 100\n",
        "      #self.time_cost = 0.005\n",
        "      #self.total_time = 0\n",
        "      #self.zero_action = False\n",
        "      #self.target_update_counter_num = 20\n",
        "      #self.reward_per_episode = []\n",
        "      #self.update_slots = True\n",
        "      #self.closest_ff_distance = 200\n",
        "      self.pro_noise_std = 0.005\n",
        "      self.epi_num = -1\n",
        "      self.has_sped_up_before = False\n",
        "      self.invisible_distance = 400\n",
        "      self.invisible_angle = 2*pi/9\n",
        "      self.reward_boundary = 25\n",
        "      # self.dev_v_cost = 1\n",
        "      self.flash_on_interval = 2.1\n",
        "      # self.distance_noise_factor = 1/6\n",
        "      # self.distance_noise_constant = 5\n",
        "      # self.angle_noise_factor = 1/8\n",
        "      # self.angle_noise_constant = 1/8\n",
        "      self.num_intervals = 25000\n",
        "      self.distance2center_cost = 2\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self.epi_num += 1\n",
        "    print(\"\\n episode: \", self.epi_num)\n",
        "    self.num_targets = 0\n",
        "    #self.past_speeds = []\n",
        "    self.time = 0\n",
        "    #self.counter = 0\n",
        "    #self.current_target_index = torch.tensor([999], dtype=torch.int32)\n",
        "    #self.previous_target_index = self.current_target_index\n",
        "    self.ff_flash = []\n",
        "    self.has_sped_up_before = False\n",
        "    \n",
        "\n",
        "    if self.flash_on_interval > 0.31:\n",
        "      global reward_per_episode\n",
        "      if reward_per_episode >= 1000:\n",
        "        self.flash_on_interval-=0.3 \n",
        "        reward_per_episode = 800\n",
        "        global best_cum_rewards\n",
        "        best_cum_rewards = best_cum_rewards/2\n",
        "    else:\n",
        "      self.distance2center_cost = 0\n",
        "    print(\"flash_on_interval: \",   self.flash_on_interval)\n",
        "\n",
        "    for i in range(self.num_ff):\n",
        "      \n",
        "      first_flash = torch.rand(1)\n",
        "      intervals = torch.poisson(torch.ones(self.num_intervals-1)*3)\n",
        "      t0 = torch.cat((first_flash, first_flash+torch.cumsum(intervals, dim=0)+torch.cumsum(torch.ones(self.num_intervals-1)*0.3, dim=0)))\n",
        "      t1 = t0 + torch.ones(self.num_intervals)*self.flash_on_interval\n",
        "      self.ff_flash.append(torch.stack((t0, t1), dim=1))\n",
        "\n",
        "    self.ffr = torch.sqrt(torch.rand(self.num_ff))*self.arena_radius # The radius of the arena changed from 1000 cm/s to 200 cm/s\n",
        "    self.fftheta = torch.rand(self.num_ff)*2*pi\n",
        "    #self.ffrt = torch.stack((self.ffr, self.fftheta), dim=1) \n",
        "    self.ffx = torch.cos(self.fftheta) * self.ffr\n",
        "    self.ffy = torch.sin(self.fftheta) * self.ffr\n",
        "    self.ffxy = torch.stack((self.ffx, self.ffy), dim=1)\n",
        "    self.ffx2 = self.ffx.clone()\n",
        "    self.ffy2 = self.ffy.clone()\n",
        "    self.ffxy2 = torch.stack((self.ffx2, self.ffy2), dim=1)\n",
        "    #self.ff_info={}\n",
        "    #self.update_slots = True\n",
        "    self.agentx = torch.tensor([0])\n",
        "    self.agenty = torch.tensor([0])\n",
        "    self.agentr = torch.zeros(1)\n",
        "    self.agentxy = torch.tensor([0, 0])\n",
        "    self.agentheading = torch.zeros(1).uniform_(0, 2*pi)\n",
        "    self.dv = torch.zeros(1).uniform_(-0.05, 0.05) \n",
        "    self.dw = torch.zeros(1)\n",
        "    self.end_episode = False \n",
        "    self.obs = self.beliefs().numpy()\n",
        "    #self.chunk_50s = 1\n",
        "    self.episode_reward = 0\n",
        "    #self.stop_rewarding_speed = False\n",
        "    #self.current_obs_steps = 0\n",
        "    return self.obs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def calculate_reward(self):\n",
        "      #action_cost=((self.previous_action[1]-self.action[1])**2+(self.previous_action[0]-self.action[0])**2)*self.mag_cost\n",
        "      # To incorporate action_cost, we need to incorporate previous_action into decision_info\n",
        "      #self.total_time += 1\n",
        "      # In addition to rewarding the monkey for capturing the firefly, we also use different phases of rewards to teach monkey specific behaviours\n",
        "      #reward = -self.time_cost\n",
        "      reward = 0\n",
        "      # Reward shaping\n",
        "      # Phase I: reward the agent for learning to stop \n",
        "      # Always: reward the agent for capturing fireflies\n",
        "      self.num_targets = 0\n",
        "      if abs(self.sys_vel[1]) <= self.terminal_vel:\n",
        "        captured_ff_index0 = (self.ff_distance_all <= self.reward_boundary).nonzero().reshape(-1)\n",
        "        total_deviated_distance = torch.sum(self.ff_distance_all[captured_ff_index0]).item()\n",
        "        captured_ff_index = captured_ff_index0.tolist()\n",
        "        self.captured_ff_index = captured_ff_index\n",
        "        num_targets = len(captured_ff_index)\n",
        "        self.num_targets = num_targets\n",
        "        if num_targets > 0: # If the monkey hs captured at least 1 ff\n",
        "          # Calculate reward\n",
        "          reward = reward + self.reward_per_ff * num_targets - total_deviated_distance*self.distance2center_cost \n",
        "          # Replace the captured ffs with ffs of new locations\n",
        "          self.ffr[captured_ff_index]= torch.sqrt(torch.rand(num_targets))*self.arena_radius\n",
        "          self.fftheta[captured_ff_index]= torch.rand(num_targets)*2*pi\n",
        "          #self.ffrt = torch.stack((self.ffr, self.fftheta), dim=1) \n",
        "          self.ffx[captured_ff_index] = torch.cos(self.fftheta[captured_ff_index]) * self.ffr[captured_ff_index]\n",
        "          self.ffy[captured_ff_index] = torch.sin(self.fftheta[captured_ff_index]) * self.ffr[captured_ff_index]\n",
        "          self.ffxy = torch.stack((self.ffx, self.ffy), dim=1)\n",
        "          # Delete the information from self.ff_info\n",
        "          #[self.ff_info.pop(key) for key in captured_ff_index if (key in self.ff_info)]\n",
        "          #self.current_target_index = torch.tensor([999], dtype=torch.int32)\n",
        "          #self.previous_target_index = self.current_target_index\n",
        "          # Reward the firefly based on the average speed before capturing the firefly\n",
        "          ##reward = reward + sum(self.past_speeds)/len(self.past_speeds)\n",
        "          #print(round(self.time, 2), \"sys_vel: \", [round(i, 4) for i in self.sys_vel.tolist()], \"obs: \", list(np.round(self.obs, decimals = 2)), \"n_targets: \",  num_targets)\n",
        "          print(round(self.time, 2), \"sys_vel: \", [round(i, 4) for i in self.sys_vel.tolist()], \"n_targets: \",  num_targets)\n",
        "          #self.update_slots = True\n",
        "          #self.stop_rewarding_speed = True\n",
        "        #elif self.has_sped_up_before == True:\n",
        "        # based on Ruiyi's formula, using the distance of the closest ff in obs\n",
        "        #reward += math.exp(-((self.ff_current[1, 0]**2)*(25/1.5)**2)/2)\n",
        "        #self.has_sped_up_before = False\n",
        "      self.prev_action = self.action.clone()\n",
        "\n",
        "      return reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    # For a small possibility, the agent will just stop\n",
        "    self.time += self.dt\n",
        "    action=torch.tensor(action)\n",
        "    self.action = action.clone()\n",
        "    action[1] = action[1]/2+0.5\n",
        "    self.sys_vel=action.clone()\n",
        "    self.state_step(action)\n",
        "    self.obs = self.beliefs().numpy()\n",
        "    reward=self.calculate_reward()\n",
        "    self.episode_reward += reward\n",
        "    if self.time >= self.episode_len*self.dt:\n",
        "      self.end_episode = True\n",
        "      #self.current_episode += 1\n",
        "      print(\"Reward for the episode: \", self.episode_reward)\n",
        "    #print(\"action: \", torch.round(action, decimals = 3), \"obs: \", np.round(self.obs, decimals = 3), \"Reward: \", round(reward, 3))\n",
        "    return self.obs, reward, self.end_episode, {}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def state_step(self,action):\n",
        "    #vnoise=torch.distributions.Normal(0,torch.ones([1,1])).sample()*self.pro_noise_std \n",
        "    #wnoise=torch.distributions.Normal(0,torch.ones([1,1])).sample()*self.pro_noise_std\n",
        "    vnoise = 0\n",
        "    wnoise = 0\n",
        "    self.dw_normed = (action[0]+wnoise)\n",
        "    self.dv_normed = (action[1]+vnoise)\n",
        "    self.dw = (action[0]+wnoise)*self.wgain*self.dt\n",
        "    self.agentheading = self.agentheading + self.dw.item()\n",
        "    self.dv =  (action[1]+vnoise)*self.vgain*self.dt\n",
        "    self.dx= torch.cos(self.agentheading)*self.dv\n",
        "    self.dy= torch.sin(self.agentheading)*self.dv\n",
        "    self.agentx = self.agentx + self.dx.item()\n",
        "    self.agenty = self.agenty + self.dy.item()\n",
        "    self.agentxy = torch.cat((self.agentx, self.agenty))\n",
        "    self.agentr = vector_norm(self.agentxy)\n",
        "    self.agenttheta = torch.tensor(torch.atan2(self.agenty, self.agentx))  \n",
        "                               \n",
        "    if self.agentr >= self.arena_radius:\n",
        "      self.agentr = 2*self.arena_radius-self.agentr\n",
        "      self.agenttheta = self.agenttheta + pi\n",
        "      self.agentx = (self.agentr*torch.cos(self.agenttheta)).reshape(1,)\n",
        "      self.agenty = (self.agentr*torch.sin(self.agenttheta)).reshape(1,)\n",
        "      self.agentxy = torch.cat((self.agentx, self.agenty))\n",
        "      self.agentheading = self.agenttheta - pi\n",
        "    while self.agentheading >= 2*pi:\n",
        "      self.agentheading = self.agentheading - 2*pi\n",
        "    while self.agentheading < 0:\n",
        "      self.agentheading = self.agentheading + 2*pi   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def beliefs(self): \n",
        "    torch.distributions.Normal(0,torch.ones([1,1])).sample()*self.pro_noise_std \n",
        "\n",
        "\n",
        "    # Make a tensor containing the relative distance of all fireflies to the agent\n",
        "    self.ff_distance_all = vector_norm(self.ffxy - self.agentxy, dim=1)\n",
        "    # distance_noise = torch.distributions.Normal(0,self.ff_distance_all*self.distance_noise_factor+ self.distance_noise_constant).sample() \n",
        "    # self.ff_distance_all = self.ff_distance_all + distance_noise\n",
        "    # Make a tensor containing the relative (real) angle of all fireflies to the agent\n",
        "    ffradians = torch.atan2(self.ffy-self.agenty, self.ffx-self.agentx)\n",
        "    angle0 = ffradians - self.agentheading\n",
        "    angle0[angle0 > pi] = angle0[angle0 > pi] - 2*pi\n",
        "    angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2*pi\n",
        "\n",
        "    # angle_noise = torch.distributions.Normal(0,torch.abs(angle0)*self.angle_noise_factor+ self.angle_noise_constant).sample() \n",
        "    # angle0 = angle0+angle_noise\n",
        "    # Adjust the angle based on reward boundary\n",
        "    angle1 = torch.abs(angle0)-torch.abs(torch.arcsin(torch.div(self.reward_boundary, torch.clip( self.ff_distance_all, self.reward_boundary, 400) ))) # use torch clip to get valid arcsin input\n",
        "    angle2 = torch.clip(angle1,0,pi)\n",
        "    ff_angle_all = torch.sign(angle0)* angle2\n",
        "    # Update the tensor containing the uncertainties of all fireflies to the agent\n",
        "    visible_ff = torch.logical_and(self.ff_distance_all < self.invisible_distance, torch.abs(angle0) < self.invisible_angle)\n",
        "    self.visible_ff_indices0 = visible_ff.nonzero().reshape(-1)\n",
        "    for index in self.visible_ff_indices0:\n",
        "      ff = self.ff_flash[index]\n",
        "      if not torch.any(torch.logical_and(ff[:, 0] <= self.time, ff[:, 1] >= self.time)):\n",
        "        visible_ff[index] = False\n",
        "    self.visible_ff_indices = visible_ff.nonzero().reshape(-1)\n",
        "\n",
        "    # Consider the case where there are fewer than self.obs_ff fireflies that are in memory\n",
        "    if torch.numel(self.visible_ff_indices) >= self.obs_ff:\n",
        "      # Rank the ff whose \"memory\" is creater than 0 based on distance\n",
        "      sorted_indices =  torch.topk(-self.ff_distance_all[self.visible_ff_indices], self.obs_ff).indices\n",
        "      self.topk_indices = self.visible_ff_indices[sorted_indices]\n",
        "      self.ffxy_topk = self.ffxy[self.topk_indices]\n",
        "      #self.ff_distance_topk = vector_norm(self.ffxy_topk - self.agentxy, dim=1)\n",
        "      self.ff_distance_topk = self.ff_distance_all[self.topk_indices]\n",
        "      self.ff_angle_topk_2 = angle0[self.topk_indices]\n",
        "      ff_angle_topk_3 = ff_angle_all[self.topk_indices]\n",
        "\n",
        "      # # Calculate relative angles \n",
        "      # ffradians = torch.atan2(self.ffxy_topk[:,1]-self.agenty, self.ffxy_topk[:,0]-self.agentx)\n",
        "      # angle0 = ffradians - self.agentheading\n",
        "      # angle0[angle0 > pi] = angle0[angle0 > pi] - 2*pi\n",
        "      # angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2*pi\n",
        "      # self.ff_angle_topk_2 = angle0.clone()\n",
        "     \n",
        "      # # Calculate relative angles of all ffs based on reward boundaries\n",
        "      # # Adjust the angle based on reward boundary\n",
        "      # angle1 = torch.abs(angle0)-torch.abs(torch.arcsin(torch.div(self.reward_boundary, torch.clip(self.ff_distance_topk, self.reward_boundary, 400) ))) # use torch clip to get valid arcsin input\n",
        "      # angle2 = torch.clip(angle1,0,pi)\n",
        "      # ff_angle_topk_3 = torch.sign(angle0)* angle2\n",
        "      \n",
        "      \n",
        "      # Concatenate distance, angle, and memory\n",
        "      ff_array = torch.stack((self.ff_angle_topk_2, ff_angle_topk_3, self.ff_distance_topk), dim=0)\n",
        "    \n",
        "\n",
        "    elif torch.numel(self.visible_ff_indices) == 0:\n",
        "      ff_array = torch.tensor([[0], [0], [self.invisible_distance]]).repeat([1, self.obs_ff])\n",
        "      self.topk_indices = torch.tensor([])\n",
        "\n",
        "    else:\n",
        "      sorted_distance, sorted_indices = torch.sort(-self.ff_distance_all[self.visible_ff_indices])\n",
        "      self.topk_indices = self.visible_ff_indices[sorted_indices]\n",
        "      self.ffxy_topk = self.ffxy[self.topk_indices ]\n",
        "      \n",
        "      self.ff_distance_topk = self.ff_distance_all[self.topk_indices]\n",
        "      self.ff_angle_topk_2 = angle0[self.topk_indices]\n",
        "      ff_angle_topk_3 = ff_angle_all[self.topk_indices]\n",
        "      # self.ff_distance_topk = vector_norm(self.ffxy_topk - self.agentxy, dim=1)\n",
        "      # # Calculate relative angles \n",
        "      # ffradians = torch.atan2(self.ffxy_topk[:,1]-self.agenty, self.ffxy_topk[:,0]-self.agentx)\n",
        "      # angle0 = ffradians - self.agentheading\n",
        "      # angle0[angle0 > pi] = angle0[angle0 > pi] - 2*pi\n",
        "      # angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2*pi\n",
        "      # self.ff_angle_topk_2 = angle0.clone()\n",
        "      # # Calculate relative angles of all ffs based on reward boundaries\n",
        "      # # Adjust the angle based on reward boundary\n",
        "      # angle1 = torch.abs(angle0)-torch.abs(torch.arcsin(torch.div(self.reward_boundary, torch.clip(self.ff_distance_topk, self.reward_boundary, 400) ))) # use torch clip to get valid arcsin input\n",
        "      # angle2 = torch.clip(angle1,0,pi)\n",
        "      # ff_angle_topk_3 = torch.sign(angle0)* angle2\n",
        "      \n",
        "      # Concatenate distance, angle, and memory\n",
        "      ff_array0 = torch.stack((self.ff_angle_topk_2, ff_angle_topk_3, self.ff_distance_topk), dim=0)\n",
        "      needed_ff = self.obs_ff - torch.numel(self.visible_ff_indices)\n",
        "      ff_array = torch.stack([ff_array0.reshape([3,-1]), torch.tensor([[0], [0], [self.invisible_distance]]).repeat([1, needed_ff])], dim=1)\n",
        "    self.real_indices = []\n",
        "    for index in env.topk_indices:\n",
        "      self.real_indices.append(int(env.ff_information[:,0][np.where(env.ff_information[:,-1]==index.item())[0][-1]].copy()))\n",
        "    # ff_array[0:2,:] = ff_array[0:2,:]/pi\n",
        "    # ff_array[2,:] = (ff_array[2,:]/self.invisible_distance-0.5)*2\n",
        "    ff_array[2,:] = ff_array[2,:]/self.invisible_distance\n",
        "    self.ff_array = ff_array.clone()\n",
        "    return torch.flatten(ff_array.transpose(0, 1))\n",
        "\n",
        "\n",
        "env = MultiFF() \n",
        "env.reset() \n",
        "\n",
        "sac_model = SAC(\"MlpPolicy\", \n",
        "            env,\n",
        "            buffer_size=int(1e6),\n",
        "            batch_size=1024,\n",
        "            device='auto',\n",
        "            verbose=False,\n",
        "            train_freq=100,\n",
        "            learning_starts = int(10),\n",
        "            target_update_interval=20,\n",
        "            learning_rate=1e-2,\n",
        "            gamma=0.9999,\n",
        "            policy_kwargs=dict(activation_fn=nn.ReLU, net_arch=[64, 64])\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRtEu3J08MLN"
      },
      "source": [
        "#### e3: 2nd slot is always placeholder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LuDj7A18MLT"
      },
      "outputs": [],
      "source": [
        "# self.distance_noise_factor = 1/6, self.angle_noise_factor  = 1/8\n",
        "model_path = \"/content/gdrive/MyDrive/fireflies_agent/lstm/July_29\" \n",
        "\n",
        "# test rewards = 11080\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUlyVYJYcJvs"
      },
      "outputs": [],
      "source": [
        "model_path = \"/content/gdrive/MyDrive/fireflies_agent/lstm/July_26_2\" \n",
        "\n",
        "## Trained from d22\n",
        "\n",
        "\n",
        "## Reward: 12480 after training for 3000 + episodes\n",
        "\n",
        "## Reward: 12100 for e3 env. Lol...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbAEVT658MLV"
      },
      "outputs": [],
      "source": [
        "gamma= 0.995\n",
        "soft_q_lr = 0.0015\n",
        "policy_lr = 0.003  \n",
        "alpha_lr = 0.002 \n",
        "update_itr= 1\n",
        "hidden_dim= 128\n",
        "reward_scale= 10\n",
        "target_entropy= -2\n",
        "soft_tau= 0.015"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehdx9Pmd8q61"
      },
      "outputs": [],
      "source": [
        "# obs = [angle, distance, memory, angle2, distance2, memory2, ...]\n",
        "# before = [angle, angle2 ..., distance, distance2, ... memory, memory2]\n",
        "\n",
        "\n",
        "reward_per_episode = 0\n",
        "\n",
        "class MultiFF(Env):\n",
        "  def __init__(self):\n",
        "      super(MultiFF, self).__init__()\n",
        "      self.num_ff = 200\n",
        "      self.arena_radius = 1000\n",
        "      self.episode_len = 10000 \n",
        "      self.dt = 0.25 \n",
        "      #self.current_episode = 0\n",
        "      #self.action_space = spaces.Box(low=-1., high=1., shape=(2,),dtype=np.float32)\n",
        "      self.action_space = spaces.Box(low=-1., high=1., shape=(2,),dtype=np.float32)\n",
        "      self.obs_ff = 2\n",
        "      self.observation_space = spaces.Box(low=-1., high=1., shape=(self.obs_ff*3,),dtype=np.float32)\n",
        "      self.terminal_vel = 0.01\n",
        "      #self.pro_noise_std = 0.005\n",
        "      self.vgain = 200\n",
        "      self.wgain = pi/4\n",
        "      self.reward_per_ff = 100\n",
        "      #self.time_cost = 0.005\n",
        "      #self.total_time = 0\n",
        "      #self.zero_action = False\n",
        "      #self.target_update_counter_num = 20\n",
        "      #self.reward_per_episode = []\n",
        "      #self.update_slots = True\n",
        "      #self.closest_ff_distance = 200\n",
        "      self.pro_noise_std = 0.005\n",
        "      self.epi_num = -1\n",
        "      self.has_sped_up_before = False\n",
        "      self.invisible_distance = 400\n",
        "      self.invisible_angle = 2*pi/9\n",
        "      self.reward_boundary = 25\n",
        "      # self.dev_v_cost = 1\n",
        "      self.flash_on_interval = 2.1\n",
        "      # self.distance_noise_factor = 1/6\n",
        "      # self.distance_noise_constant = 5\n",
        "      # self.angle_noise_factor = 1/8\n",
        "      # self.angle_noise_constant = 1/8\n",
        "      self.num_intervals = 25000\n",
        "      self.distance2center_cost = 2\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self.epi_num += 1\n",
        "    print(\"\\n episode: \", self.epi_num)\n",
        "    self.num_targets = 0\n",
        "    #self.past_speeds = []\n",
        "    self.time = 0\n",
        "    #self.counter = 0\n",
        "    #self.current_target_index = torch.tensor([999], dtype=torch.int32)\n",
        "    #self.previous_target_index = self.current_target_index\n",
        "    self.ff_flash = []\n",
        "    self.has_sped_up_before = False\n",
        "    \n",
        "\n",
        "    if self.flash_on_interval > 0.31:\n",
        "      global reward_per_episode\n",
        "      if reward_per_episode >= 1000:\n",
        "        self.flash_on_interval-=0.3 \n",
        "        reward_per_episode = 800\n",
        "        global best_cum_rewards\n",
        "        best_cum_rewards = best_cum_rewards/2\n",
        "    else:\n",
        "      self.distance2center_cost = 0\n",
        "    print(\"flash_on_interval: \",   self.flash_on_interval)\n",
        "\n",
        "    for i in range(self.num_ff):\n",
        "      \n",
        "      first_flash = torch.rand(1)\n",
        "      intervals = torch.poisson(torch.ones(self.num_intervals-1)*3)\n",
        "      t0 = torch.cat((first_flash, first_flash+torch.cumsum(intervals, dim=0)+torch.cumsum(torch.ones(self.num_intervals-1)*0.3, dim=0)))\n",
        "      t1 = t0 + torch.ones(self.num_intervals)*self.flash_on_interval\n",
        "      self.ff_flash.append(torch.stack((t0, t1), dim=1))\n",
        "\n",
        "    self.ffr = torch.sqrt(torch.rand(self.num_ff))*self.arena_radius # The radius of the arena changed from 1000 cm/s to 200 cm/s\n",
        "    self.fftheta = torch.rand(self.num_ff)*2*pi\n",
        "    #self.ffrt = torch.stack((self.ffr, self.fftheta), dim=1) \n",
        "    self.ffx = torch.cos(self.fftheta) * self.ffr\n",
        "    self.ffy = torch.sin(self.fftheta) * self.ffr\n",
        "    self.ffxy = torch.stack((self.ffx, self.ffy), dim=1)\n",
        "    self.ffx2 = self.ffx.clone()\n",
        "    self.ffy2 = self.ffy.clone()\n",
        "    self.ffxy2 = torch.stack((self.ffx2, self.ffy2), dim=1)\n",
        "    #self.ff_info={}\n",
        "    #self.update_slots = True\n",
        "    self.agentx = torch.tensor([0])\n",
        "    self.agenty = torch.tensor([0])\n",
        "    self.agentr = torch.zeros(1)\n",
        "    self.agentxy = torch.tensor([0, 0])\n",
        "    self.agentheading = torch.zeros(1).uniform_(0, 2*pi)\n",
        "    self.dv = torch.zeros(1).uniform_(-0.05, 0.05) \n",
        "    self.dw = torch.zeros(1)\n",
        "    self.end_episode = False \n",
        "    self.obs = self.beliefs().numpy()\n",
        "    #self.chunk_50s = 1\n",
        "    self.episode_reward = 0\n",
        "    #self.stop_rewarding_speed = False\n",
        "    #self.current_obs_steps = 0\n",
        "    return self.obs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def calculate_reward(self):\n",
        "      #action_cost=((self.previous_action[1]-self.action[1])**2+(self.previous_action[0]-self.action[0])**2)*self.mag_cost\n",
        "      # To incorporate action_cost, we need to incorporate previous_action into decision_info\n",
        "      #self.total_time += 1\n",
        "      # In addition to rewarding the monkey for capturing the firefly, we also use different phases of rewards to teach monkey specific behaviours\n",
        "      #reward = -self.time_cost\n",
        "      reward = 0\n",
        "      # Reward shaping\n",
        "      # Phase I: reward the agent for learning to stop \n",
        "      # Always: reward the agent for capturing fireflies\n",
        "      self.num_targets = 0\n",
        "      if abs(self.sys_vel[1]) <= self.terminal_vel:\n",
        "        captured_ff_index0 = (self.ff_distance_all <= self.reward_boundary).nonzero().reshape(-1)\n",
        "        total_deviated_distance = torch.sum(self.ff_distance_all[captured_ff_index0]).item()\n",
        "        captured_ff_index = captured_ff_index0.tolist()\n",
        "        self.captured_ff_index = captured_ff_index\n",
        "        num_targets = len(captured_ff_index)\n",
        "        self.num_targets = num_targets\n",
        "        if num_targets > 0: # If the monkey hs captured at least 1 ff\n",
        "          # Calculate reward\n",
        "          reward = reward + self.reward_per_ff * num_targets - total_deviated_distance*self.distance2center_cost \n",
        "          # Replace the captured ffs with ffs of new locations\n",
        "          self.ffr[captured_ff_index]= torch.sqrt(torch.rand(num_targets))*self.arena_radius\n",
        "          self.fftheta[captured_ff_index]= torch.rand(num_targets)*2*pi\n",
        "          #self.ffrt = torch.stack((self.ffr, self.fftheta), dim=1) \n",
        "          self.ffx[captured_ff_index] = torch.cos(self.fftheta[captured_ff_index]) * self.ffr[captured_ff_index]\n",
        "          self.ffy[captured_ff_index] = torch.sin(self.fftheta[captured_ff_index]) * self.ffr[captured_ff_index]\n",
        "          self.ffxy = torch.stack((self.ffx, self.ffy), dim=1)\n",
        "          # Delete the information from self.ff_info\n",
        "          #[self.ff_info.pop(key) for key in captured_ff_index if (key in self.ff_info)]\n",
        "          #self.current_target_index = torch.tensor([999], dtype=torch.int32)\n",
        "          #self.previous_target_index = self.current_target_index\n",
        "          # Reward the firefly based on the average speed before capturing the firefly\n",
        "          ##reward = reward + sum(self.past_speeds)/len(self.past_speeds)\n",
        "          #print(round(self.time, 2), \"sys_vel: \", [round(i, 4) for i in self.sys_vel.tolist()], \"obs: \", list(np.round(self.obs, decimals = 2)), \"n_targets: \",  num_targets)\n",
        "          print(round(self.time, 2), \"sys_vel: \", [round(i, 4) for i in self.sys_vel.tolist()], \"n_targets: \",  num_targets)\n",
        "          #self.update_slots = True\n",
        "          #self.stop_rewarding_speed = True\n",
        "        #elif self.has_sped_up_before == True:\n",
        "        # based on Ruiyi's formula, using the distance of the closest ff in obs\n",
        "        #reward += math.exp(-((self.ff_current[1, 0]**2)*(25/1.5)**2)/2)\n",
        "        #self.has_sped_up_before = False\n",
        "      self.prev_action = self.action.clone()\n",
        "\n",
        "      return reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    self.time += self.dt\n",
        "    action=torch.tensor(action)\n",
        "    self.action = action.clone()\n",
        "    action[1] = action[1]/2+0.5\n",
        "    self.sys_vel=action.clone()\n",
        "    self.state_step(action)\n",
        "    self.obs = self.beliefs().numpy()\n",
        "    reward=self.calculate_reward()\n",
        "    self.episode_reward += reward\n",
        "    \n",
        "    if self.time >= self.episode_len*self.dt:\n",
        "      self.end_episode = True\n",
        "      #self.current_episode += 1\n",
        "      print(\"Reward for the episode: \", self.episode_reward)\n",
        "    #print(\"action: \", torch.round(action, decimals = 3), \"obs: \", np.round(self.obs, decimals = 3), \"Reward: \", round(reward, 3))\n",
        "    return self.obs, reward, self.end_episode, {}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def state_step(self,action):\n",
        "    #vnoise=torch.distributions.Normal(0,torch.ones([1,1])).sample()*self.pro_noise_std \n",
        "    #wnoise=torch.distributions.Normal(0,torch.ones([1,1])).sample()*self.pro_noise_std\n",
        "    vnoise = 0\n",
        "    wnoise = 0\n",
        "    self.dw_normed = (action[0]+wnoise)\n",
        "    self.dv_normed = (action[1]+vnoise)\n",
        "    self.dw = (action[0]+wnoise)*self.wgain*self.dt\n",
        "    self.agentheading = self.agentheading + self.dw.item()\n",
        "    self.dv =  (action[1]+vnoise)*self.vgain*self.dt\n",
        "    self.dx= torch.cos(self.agentheading)*self.dv\n",
        "    self.dy= torch.sin(self.agentheading)*self.dv\n",
        "    self.agentx = self.agentx + self.dx.item()\n",
        "    self.agenty = self.agenty + self.dy.item()\n",
        "    self.agentxy = torch.cat((self.agentx, self.agenty))\n",
        "    self.agentr = vector_norm(self.agentxy)\n",
        "    self.agenttheta = torch.tensor(torch.atan2(self.agenty, self.agentx))  \n",
        "                               \n",
        "    if self.agentr >= self.arena_radius:\n",
        "      self.agentr = 2*self.arena_radius-self.agentr\n",
        "      self.agenttheta = self.agenttheta + pi\n",
        "      self.agentx = (self.agentr*torch.cos(self.agenttheta)).reshape(1,)\n",
        "      self.agenty = (self.agentr*torch.sin(self.agenttheta)).reshape(1,)\n",
        "      self.agentxy = torch.cat((self.agentx, self.agenty))\n",
        "      self.agentheading = self.agenttheta - pi\n",
        "    while self.agentheading >= 2*pi:\n",
        "      self.agentheading = self.agentheading - 2*pi\n",
        "    while self.agentheading < 0:\n",
        "      self.agentheading = self.agentheading + 2*pi   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def beliefs(self): \n",
        "\n",
        "\n",
        "    # Make a tensor containing the relative distance of all fireflies to the agent\n",
        "    self.ff_distance_all = vector_norm(self.ffxy - self.agentxy, dim=1)\n",
        "    #distance_noise = torch.distributions.Normal(0,self.ff_distance_all*self.distance_noise_factor+ self.distance_noise_constant).sample() \n",
        "    #self.ff_distance_all = self.ff_distance_all + distance_noise\n",
        "    # Make a tensor containing the relative (real) angle of all fireflies to the agent\n",
        "    ffradians = torch.atan2(self.ffy-self.agenty, self.ffx-self.agentx)\n",
        "    angle0 = ffradians - self.agentheading\n",
        "    angle0[angle0 > pi] = angle0[angle0 > pi] - 2*pi\n",
        "    angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2*pi\n",
        "\n",
        "    #angle_noise = torch.distributions.Normal(0,torch.abs(angle0)*self.angle_noise_factor+ self.angle_noise_constant).sample() \n",
        "    #angle0 = angle0+angle_noise\n",
        "    # Adjust the angle based on reward boundary\n",
        "    angle1 = torch.abs(angle0)-torch.abs(torch.arcsin(torch.div(self.reward_boundary, torch.clip( self.ff_distance_all, self.reward_boundary, 400) ))) # use torch clip to get valid arcsin input\n",
        "    angle2 = torch.clip(angle1,0,pi)\n",
        "    ff_angle_all = torch.sign(angle0)* angle2\n",
        "    # Update the tensor containing the uncertainties of all fireflies to the agent\n",
        "    visible_ff = torch.logical_and( self.ff_distance_all < self.invisible_distance, torch.abs(angle0) < self.invisible_angle)\n",
        "    self.visible_ff_indices0 = visible_ff.nonzero().reshape(-1)\n",
        "    for index in self.visible_ff_indices0:\n",
        "      ff = self.ff_flash[index]\n",
        "      if not torch.any(torch.logical_and(ff[:, 0] <= self.time, ff[:, 1] >= self.time)):\n",
        "        visible_ff[index] = False\n",
        "    self.visible_ff_indices = visible_ff.nonzero().reshape(-1)\n",
        "\n",
        "    # Consider the case where there are fewer than self.obs_ff fireflies that are in memory\n",
        "    if torch.numel(self.visible_ff_indices) >= self.obs_ff-1:\n",
        "      # Rank the ff whose \"memory\" is creater than 0 based on distance\n",
        "      sorted_indices =  torch.topk(-self.ff_distance_all[self.visible_ff_indices], self.obs_ff-1).indices\n",
        "      self.topk_indices = self.visible_ff_indices[sorted_indices]\n",
        "      self.ffxy_topk = self.ffxy[self.topk_indices]\n",
        "      #self.ff_distance_topk = vector_norm(self.ffxy_topk - self.agentxy, dim=1)\n",
        "      self.ff_distance_topk = self.ff_distance_all[self.topk_indices]\n",
        "      self.ff_angle_topk_2 = angle0[self.topk_indices]\n",
        "      ff_angle_topk_3 = ff_angle_all[self.topk_indices]\n",
        "\n",
        "      # # Calculate relative angles \n",
        "      # ffradians = torch.atan2(self.ffxy_topk[:,1]-self.agenty, self.ffxy_topk[:,0]-self.agentx)\n",
        "      # angle0 = ffradians - self.agentheading\n",
        "      # angle0[angle0 > pi] = angle0[angle0 > pi] - 2*pi\n",
        "      # angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2*pi\n",
        "      # self.ff_angle_topk_2 = angle0.clone()\n",
        "     \n",
        "      # # Calculate relative angles of all ffs based on reward boundaries\n",
        "      # # Adjust the angle based on reward boundary\n",
        "      # angle1 = torch.abs(angle0)-torch.abs(torch.arcsin(torch.div(self.reward_boundary, torch.clip(self.ff_distance_topk, self.reward_boundary, 400) ))) # use torch clip to get valid arcsin input\n",
        "      # angle2 = torch.clip(angle1,0,pi)\n",
        "      # ff_angle_topk_3 = torch.sign(angle0)* angle2\n",
        "      # Concatenate distance, angle, and memory\n",
        "      ff_array0 = torch.stack((self.ff_angle_topk_2, ff_angle_topk_3, self.ff_distance_topk), dim=0)\n",
        "      needed_ff = 1\n",
        "      ff_array = torch.stack([ff_array0.reshape([3,-1]), torch.tensor([[0], [0], [self.invisible_distance]]).repeat([1, needed_ff])], dim=1)\n",
        "      \n",
        "    else:\n",
        "      ff_array = torch.tensor([[0], [0], [self.invisible_distance]]).repeat([1, self.obs_ff])\n",
        "      self.topk_indices = torch.tensor([])\n",
        "\n",
        "\n",
        "\n",
        "    # ff_array[0:2,:] = ff_array[0:2,:]/pi\n",
        "    # ff_array[2,:] = (ff_array[2,:]/self.invisible_distance-0.5)*2\n",
        "    ff_array[2,:] = ff_array[2,:]/self.invisible_distance\n",
        "    self.ff_array = ff_array.clone()\n",
        "    return torch.flatten(ff_array.transpose(0, 1))\n",
        "\n",
        "\n",
        "env = MultiFF() \n",
        "env.reset() \n",
        "\n",
        "sac_model = SAC(\"MlpPolicy\", \n",
        "            env,\n",
        "            buffer_size=int(1e6),\n",
        "            batch_size=1024,\n",
        "            device='auto',\n",
        "            verbose=False,\n",
        "            train_freq=100,\n",
        "            learning_starts = int(10),\n",
        "            target_update_interval=20,\n",
        "            learning_rate=1e-2,\n",
        "            gamma=0.9999,\n",
        "            policy_kwargs=dict(activation_fn=nn.ReLU, net_arch=[64, 64])\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "addDyn8N2Bcd"
      },
      "source": [
        "#### d22: no noise; fixes wgain \n",
        "\n",
        "(The previous environments are fixdd too)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hx1xTZDq2Bce"
      },
      "outputs": [],
      "source": [
        "# self.distance_noise_factor = 1/6, self.angle_noise_factor  = 1/8\n",
        "model_path = \"/content/gdrive/MyDrive/fireflies_agent/lstm/July_26_2\" \n",
        "\n",
        "\n",
        "## Reward: 12480 after training for 3000 + episodes\n",
        "\n",
        "\n",
        "\n",
        "# for agent\n",
        "NEW_DATASET = True\n",
        "MONKEY_DATA = False\n",
        "NO_PLOT_NEEDED = True\n",
        "data_folder_name = \"LSTM_July_29\"\n",
        "data_num = 721\n",
        "trial_total_num = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUf5y86L2Bce"
      },
      "outputs": [],
      "source": [
        "gamma= 0.995\n",
        "soft_q_lr = 0.0015\n",
        "policy_lr = 0.003  \n",
        "alpha_lr = 0.002 \n",
        "update_itr= 1\n",
        "hidden_dim= 128\n",
        "reward_scale= 10\n",
        "target_entropy= -2\n",
        "soft_tau= 0.015"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKRRmmdw2Bce"
      },
      "outputs": [],
      "source": [
        "# obs = [angle, distance, memory, angle2, distance2, memory2, ...]\n",
        "# before = [angle, angle2 ..., distance, distance2, ... memory, memory2]\n",
        "\n",
        "\n",
        "reward_per_episode = 0\n",
        "\n",
        "class MultiFF(Env):\n",
        "  def __init__(self):\n",
        "      super(MultiFF, self).__init__()\n",
        "      self.num_ff = 200\n",
        "      self.arena_radius = 1000\n",
        "      self.episode_len = 15000\n",
        "      self.dt = 0.25 \n",
        "      #self.current_episode = 0\n",
        "      #self.action_space = spaces.Box(low=-1., high=1., shape=(2,),dtype=np.float32)\n",
        "      self.action_space = spaces.Box(low=-1., high=1., shape=(2,),dtype=np.float32)\n",
        "      self.obs_ff = 2\n",
        "      self.observation_space = spaces.Box(low=-1., high=1., shape=(self.obs_ff*3,),dtype=np.float32)\n",
        "      self.terminal_vel = 0.01\n",
        "      #self.pro_noise_std = 0.005\n",
        "      self.vgain = 200\n",
        "      self.wgain = pi/4\n",
        "      self.reward_per_ff = 100\n",
        "      #self.time_cost = 0.005\n",
        "      #self.total_time = 0\n",
        "      #self.zero_action = False\n",
        "      #self.target_update_counter_num = 20\n",
        "      #self.reward_per_episode = []\n",
        "      #self.update_slots = True\n",
        "      #self.closest_ff_distance = 200\n",
        "      self.pro_noise_std = 0.005\n",
        "      self.epi_num = -1\n",
        "      self.has_sped_up_before = False\n",
        "      self.invisible_distance = 400\n",
        "      self.invisible_angle = 2*pi/9\n",
        "      self.reward_boundary = 25\n",
        "      # self.dev_v_cost = 1\n",
        "      self.flash_on_interval = 2.1\n",
        "      # self.distance_noise_factor = 1/6\n",
        "      # self.distance_noise_constant = 5\n",
        "      # self.angle_noise_factor = 1/8\n",
        "      # self.angle_noise_constant = 1/8\n",
        "      self.distance2center_cost = 2\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self.epi_num += 1\n",
        "    print(\"\\n episode: \", self.epi_num)\n",
        "    self.num_targets = 0\n",
        "    #self.past_speeds = []\n",
        "    self.time = 0\n",
        "    #self.counter = 0\n",
        "    #self.current_target_index = torch.tensor([999], dtype=torch.int32)\n",
        "    #self.previous_target_index = self.current_target_index\n",
        "    self.ff_flash = []\n",
        "    self.has_sped_up_before = False\n",
        "    \n",
        "\n",
        "    if self.flash_on_interval > 0.31:\n",
        "      global reward_per_episode\n",
        "      if reward_per_episode >= 1000:\n",
        "        self.flash_on_interval-=0.3 \n",
        "        reward_per_episode = 800\n",
        "        global best_cum_rewards\n",
        "        best_cum_rewards = best_cum_rewards/2\n",
        "    else:\n",
        "      self.distance2center_cost = 0\n",
        "    print(\"flash_on_interval: \",   self.flash_on_interval)\n",
        "\n",
        "    self.num_intervals = 25000\n",
        "    for i in range(self.num_ff):   \n",
        "      first_flash = torch.rand(1)\n",
        "      intervals = torch.poisson(torch.ones(self.num_intervals-1)*3)\n",
        "      t0 = torch.cat((first_flash, first_flash+torch.cumsum(intervals, dim=0)+torch.cumsum(torch.ones(self.num_intervals-1)*0.3, dim=0)))\n",
        "      t1 = t0 + torch.ones(self.num_intervals)*self.flash_on_interval\n",
        "      self.ff_flash.append(torch.stack((t0, t1), dim=1))\n",
        "\n",
        "    self.ffr = torch.sqrt(torch.rand(self.num_ff))*self.arena_radius # The radius of the arena changed from 1000 cm/s to 200 cm/s\n",
        "    self.fftheta = torch.rand(self.num_ff)*2*pi\n",
        "    #self.ffrt = torch.stack((self.ffr, self.fftheta), dim=1) \n",
        "    self.ffx = torch.cos(self.fftheta) * self.ffr\n",
        "    self.ffy = torch.sin(self.fftheta) * self.ffr\n",
        "    self.ffxy = torch.stack((self.ffx, self.ffy), dim=1)\n",
        "    self.ffx2 = self.ffx.clone()\n",
        "    self.ffy2 = self.ffy.clone()\n",
        "    self.ffxy2 = torch.stack((self.ffx2, self.ffy2), dim=1)\n",
        "    #self.ff_info={}\n",
        "    #self.update_slots = True\n",
        "    self.agentx = torch.tensor([0])\n",
        "    self.agenty = torch.tensor([0])\n",
        "    self.agentr = torch.zeros(1)\n",
        "    self.agentxy = torch.tensor([0, 0])\n",
        "    self.agentheading = torch.zeros(1).uniform_(0, 2*pi)\n",
        "    self.dv = torch.zeros(1).uniform_(-0.05, 0.05) \n",
        "    self.dw = torch.zeros(1)\n",
        "    self.end_episode = False \n",
        "    self.obs = self.beliefs().numpy()\n",
        "    #self.chunk_50s = 1\n",
        "    self.episode_reward = 0\n",
        "    #self.stop_rewarding_speed = False\n",
        "    #self.current_obs_steps = 0\n",
        "    return self.obs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def calculate_reward(self):\n",
        "      #action_cost=((self.previous_action[1]-self.action[1])**2+(self.previous_action[0]-self.action[0])**2)*self.mag_cost\n",
        "      # To incorporate action_cost, we need to incorporate previous_action into decision_info\n",
        "      #self.total_time += 1\n",
        "      # In addition to rewarding the monkey for capturing the firefly, we also use different phases of rewards to teach monkey specific behaviours\n",
        "      #reward = -self.time_cost\n",
        "      reward = 0\n",
        "      # Reward shaping\n",
        "      # Phase I: reward the agent for learning to stop \n",
        "      # Always: reward the agent for capturing fireflies\n",
        "      self.num_targets = 0\n",
        "      if abs(self.sys_vel[1]) <= self.terminal_vel:\n",
        "        captured_ff_index0 = (self.ff_distance_all <= self.reward_boundary).nonzero().reshape(-1)\n",
        "        total_deviated_distance = torch.sum(self.ff_distance_all[captured_ff_index0]).item()\n",
        "        captured_ff_index = captured_ff_index0.tolist()\n",
        "        self.captured_ff_index = captured_ff_index\n",
        "        num_targets = len(captured_ff_index)\n",
        "        self.num_targets = num_targets\n",
        "        if num_targets > 0: # If the monkey hs captured at least 1 ff\n",
        "          # Calculate reward\n",
        "          reward = reward + self.reward_per_ff * num_targets - total_deviated_distance*self.distance2center_cost \n",
        "          # Replace the captured ffs with ffs of new locations\n",
        "          self.ffr[captured_ff_index]= torch.sqrt(torch.rand(num_targets))*self.arena_radius\n",
        "          self.fftheta[captured_ff_index]= torch.rand(num_targets)*2*pi\n",
        "          #self.ffrt = torch.stack((self.ffr, self.fftheta), dim=1) \n",
        "          self.ffx[captured_ff_index] = torch.cos(self.fftheta[captured_ff_index]) * self.ffr[captured_ff_index]\n",
        "          self.ffy[captured_ff_index] = torch.sin(self.fftheta[captured_ff_index]) * self.ffr[captured_ff_index]\n",
        "          self.ffxy = torch.stack((self.ffx, self.ffy), dim=1)\n",
        "          # Delete the information from self.ff_info\n",
        "          #[self.ff_info.pop(key) for key in captured_ff_index if (key in self.ff_info)]\n",
        "          #self.current_target_index = torch.tensor([999], dtype=torch.int32)\n",
        "          #self.previous_target_index = self.current_target_index\n",
        "          # Reward the firefly based on the average speed before capturing the firefly\n",
        "          ##reward = reward + sum(self.past_speeds)/len(self.past_speeds)\n",
        "          #print(round(self.time, 2), \"sys_vel: \", [round(i, 4) for i in self.sys_vel.tolist()], \"obs: \", list(np.round(self.obs, decimals = 2)), \"n_targets: \",  num_targets)\n",
        "          print(round(self.time, 2), \"sys_vel: \", [round(i, 4) for i in self.sys_vel.tolist()], \"n_targets: \",  num_targets)\n",
        "          #self.update_slots = True\n",
        "          #self.stop_rewarding_speed = True\n",
        "        #elif self.has_sped_up_before == True:\n",
        "        # based on Ruiyi's formula, using the distance of the closest ff in obs\n",
        "        #reward += math.exp(-((self.ff_current[1, 0]**2)*(25/1.5)**2)/2)\n",
        "        #self.has_sped_up_before = False\n",
        "      self.prev_action = self.action.clone()\n",
        "\n",
        "      return reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    self.time += self.dt\n",
        "    action=torch.tensor(action)\n",
        "    self.action = action.clone()\n",
        "    action[1] = action[1]/2+0.5\n",
        "    self.sys_vel=action.clone()\n",
        "    self.state_step(action)\n",
        "    self.obs = self.beliefs().numpy()\n",
        "    reward=self.calculate_reward()\n",
        "    self.episode_reward += reward\n",
        "    if self.time >= self.episode_len*self.dt:\n",
        "      self.end_episode = True\n",
        "      #self.current_episode += 1\n",
        "      print(\"Reward for the episode: \", self.episode_reward)\n",
        "    #print(\"action: \", torch.round(action, decimals = 3), \"obs: \", np.round(self.obs, decimals = 3), \"Reward: \", round(reward, 3))\n",
        "    return self.obs, reward, self.end_episode, {}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def state_step(self,action):\n",
        "    #vnoise=torch.distributions.Normal(0,torch.ones([1,1])).sample()*self.pro_noise_std \n",
        "    #wnoise=torch.distributions.Normal(0,torch.ones([1,1])).sample()*self.pro_noise_std\n",
        "    vnoise = 0\n",
        "    wnoise = 0\n",
        "    self.dw_normed = (action[0]+wnoise)\n",
        "    self.dv_normed = (action[1]+vnoise)\n",
        "    self.dw = (action[0]+wnoise)*self.wgain*self.dt\n",
        "    self.agentheading = self.agentheading + self.dw.item()\n",
        "    self.dv =  (action[1]+vnoise)*self.vgain*self.dt\n",
        "    self.dx= torch.cos(self.agentheading)*self.dv\n",
        "    self.dy= torch.sin(self.agentheading)*self.dv\n",
        "    self.agentx = self.agentx + self.dx.item()\n",
        "    self.agenty = self.agenty + self.dy.item()\n",
        "    self.agentxy = torch.cat((self.agentx, self.agenty))\n",
        "    self.agentr = vector_norm(self.agentxy)\n",
        "    self.agenttheta = torch.tensor(torch.atan2(self.agenty, self.agentx))  \n",
        "                               \n",
        "    if self.agentr >= self.arena_radius:\n",
        "      self.agentr = 2*self.arena_radius-self.agentr\n",
        "      self.agenttheta = self.agenttheta + pi\n",
        "      self.agentx = (self.agentr*torch.cos(self.agenttheta)).reshape(1,)\n",
        "      self.agenty = (self.agentr*torch.sin(self.agenttheta)).reshape(1,)\n",
        "      self.agentxy = torch.cat((self.agentx, self.agenty))\n",
        "      self.agentheading = self.agenttheta - pi\n",
        "    while self.agentheading >= 2*pi:\n",
        "      self.agentheading = self.agentheading - 2*pi\n",
        "    while self.agentheading < 0:\n",
        "      self.agentheading = self.agentheading + 2*pi   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def beliefs(self): \n",
        "\n",
        "\n",
        "    # Make a tensor containing the relative distance of all fireflies to the agent\n",
        "    self.ff_distance_all = vector_norm(self.ffxy - self.agentxy, dim=1)\n",
        "    #distance_noise = torch.distributions.Normal(0,self.ff_distance_all*self.distance_noise_factor+ self.distance_noise_constant).sample() \n",
        "    #self.ff_distance_all = self.ff_distance_all + distance_noise\n",
        "    # Make a tensor containing the relative (real) angle of all fireflies to the agent\n",
        "    ffradians = torch.atan2(self.ffy-self.agenty, self.ffx-self.agentx)\n",
        "    angle0 = ffradians - self.agentheading\n",
        "    angle0[angle0 > pi] = angle0[angle0 > pi] - 2*pi\n",
        "    angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2*pi\n",
        "\n",
        "    #angle_noise = torch.distributions.Normal(0,torch.abs(angle0)*self.angle_noise_factor+ self.angle_noise_constant).sample() \n",
        "    #angle0 = angle0+angle_noise\n",
        "    # Adjust the angle based on reward boundary\n",
        "    angle1 = torch.abs(angle0)-torch.abs(torch.arcsin(torch.div(self.reward_boundary, torch.clip( self.ff_distance_all, self.reward_boundary, 400) ))) # use torch clip to get valid arcsin input\n",
        "    angle2 = torch.clip(angle1,0,pi)\n",
        "    ff_angle_all = torch.sign(angle0)* angle2\n",
        "    # Update the tensor containing the uncertainties of all fireflies to the agent\n",
        "    visible_ff = torch.logical_and( self.ff_distance_all < self.invisible_distance, torch.abs(angle0) < self.invisible_angle)\n",
        "    self.visible_ff_indices0 = visible_ff.nonzero().reshape(-1)\n",
        "    for index in self.visible_ff_indices0:\n",
        "      ff = self.ff_flash[index]\n",
        "      if not torch.any(torch.logical_and(ff[:, 0] <= self.time, ff[:, 1] >= self.time)):\n",
        "        visible_ff[index] = False\n",
        "    self.visible_ff_indices = visible_ff.nonzero().reshape(-1)\n",
        "\n",
        "    # Consider the case where there are fewer than self.obs_ff fireflies that are in memory\n",
        "    if torch.numel(self.visible_ff_indices) >= self.obs_ff:\n",
        "      # Rank the ff whose \"memory\" is creater than 0 based on distance\n",
        "      sorted_indices =  torch.topk(-self.ff_distance_all[self.visible_ff_indices], self.obs_ff).indices\n",
        "      self.topk_indices = self.visible_ff_indices[sorted_indices]\n",
        "      self.ffxy_topk = self.ffxy[self.topk_indices]\n",
        "      #self.ff_distance_topk = vector_norm(self.ffxy_topk - self.agentxy, dim=1)\n",
        "      self.ff_distance_topk = self.ff_distance_all[self.topk_indices]\n",
        "      self.ff_angle_topk_2 = angle0[self.topk_indices]\n",
        "      ff_angle_topk_3 = ff_angle_all[self.topk_indices]\n",
        "\n",
        "      # # Calculate relative angles \n",
        "      # ffradians = torch.atan2(self.ffxy_topk[:,1]-self.agenty, self.ffxy_topk[:,0]-self.agentx)\n",
        "      # angle0 = ffradians - self.agentheading\n",
        "      # angle0[angle0 > pi] = angle0[angle0 > pi] - 2*pi\n",
        "      # angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2*pi\n",
        "      # self.ff_angle_topk_2 = angle0.clone()\n",
        "     \n",
        "      # # Calculate relative angles of all ffs based on reward boundaries\n",
        "      # # Adjust the angle based on reward boundary\n",
        "      # angle1 = torch.abs(angle0)-torch.abs(torch.arcsin(torch.div(self.reward_boundary, torch.clip(self.ff_distance_topk, self.reward_boundary, 400) ))) # use torch clip to get valid arcsin input\n",
        "      # angle2 = torch.clip(angle1,0,pi)\n",
        "      # ff_angle_topk_3 = torch.sign(angle0)* angle2\n",
        "      \n",
        "      \n",
        "      # Concatenate distance, angle, and memory\n",
        "      ff_array = torch.stack((self.ff_angle_topk_2, ff_angle_topk_3, self.ff_distance_topk), dim=0)\n",
        "    \n",
        "\n",
        "    elif torch.numel(self.visible_ff_indices) == 0:\n",
        "      ff_array = torch.tensor([[0], [0], [self.invisible_distance]]).repeat([1, self.obs_ff])\n",
        "      self.topk_indices = torch.tensor([])\n",
        "\n",
        "    else:\n",
        "      sorted_distance, sorted_indices = torch.sort(-self.ff_distance_all[self.visible_ff_indices])\n",
        "      self.topk_indices = self.visible_ff_indices[sorted_indices]\n",
        "      self.ffxy_topk = self.ffxy[self.topk_indices ]\n",
        "      \n",
        "      self.ff_distance_topk = self.ff_distance_all[self.topk_indices]\n",
        "      self.ff_angle_topk_2 = angle0[self.topk_indices]\n",
        "      ff_angle_topk_3 = ff_angle_all[self.topk_indices]\n",
        "      # self.ff_distance_topk = vector_norm(self.ffxy_topk - self.agentxy, dim=1)\n",
        "      # # Calculate relative angles \n",
        "      # ffradians = torch.atan2(self.ffxy_topk[:,1]-self.agenty, self.ffxy_topk[:,0]-self.agentx)\n",
        "      # angle0 = ffradians - self.agentheading\n",
        "      # angle0[angle0 > pi] = angle0[angle0 > pi] - 2*pi\n",
        "      # angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2*pi\n",
        "      # self.ff_angle_topk_2 = angle0.clone()\n",
        "      # # Calculate relative angles of all ffs based on reward boundaries\n",
        "      # # Adjust the angle based on reward boundary\n",
        "      # angle1 = torch.abs(angle0)-torch.abs(torch.arcsin(torch.div(self.reward_boundary, torch.clip(self.ff_distance_topk, self.reward_boundary, 400) ))) # use torch clip to get valid arcsin input\n",
        "      # angle2 = torch.clip(angle1,0,pi)\n",
        "      # ff_angle_topk_3 = torch.sign(angle0)* angle2\n",
        "      \n",
        "      # Concatenate distance, angle, and memory\n",
        "      ff_array0 = torch.stack((self.ff_angle_topk_2, ff_angle_topk_3, self.ff_distance_topk), dim=0)\n",
        "      needed_ff = self.obs_ff - torch.numel(self.visible_ff_indices)\n",
        "      ff_array = torch.stack([ff_array0.reshape([3,-1]), torch.tensor([[0], [0], [self.invisible_distance]]).repeat([1, needed_ff])], dim=1)\n",
        "      \n",
        "\n",
        "    # ff_array[0:2,:] = ff_array[0:2,:]/pi\n",
        "    # ff_array[2,:] = (ff_array[2,:]/self.invisible_distance-0.5)*2\n",
        "    ff_array[2,:] = ff_array[2,:]/self.invisible_distance\n",
        "    self.ff_array = ff_array.clone()\n",
        "    return torch.flatten(ff_array.transpose(0, 1))\n",
        "\n",
        "\n",
        "env = MultiFF() \n",
        "env.reset() \n",
        "\n",
        "sac_model = SAC(\"MlpPolicy\", \n",
        "            env,\n",
        "            buffer_size=int(1e6),\n",
        "            batch_size=1024,\n",
        "            device='auto',\n",
        "            verbose=False,\n",
        "            train_freq=100,\n",
        "            learning_starts = int(10),\n",
        "            target_update_interval=20,\n",
        "            learning_rate=1e-2,\n",
        "            gamma=0.9999,\n",
        "            policy_kwargs=dict(activation_fn=nn.ReLU, net_arch=[64, 64])\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CR0roGsdf-n"
      },
      "source": [
        "### load LSTM model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1doayHYltE8a"
      },
      "source": [
        "#### prep codes\n",
        "\n",
        "The cell below contains all the codes combined. This version is:decrease hidden layer; change to QNetworkLSTM2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efPA7oU9gfi8"
      },
      "source": [
        "Note that the codes are slightly different from those in the notebook for training the agent because this notebook does not use GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pg8l0PVrjCQJ"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def linear_weights_init(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        stdv = 1. / math.sqrt(m.weight.size(1))\n",
        "        m.weight.data.uniform_(-stdv, stdv)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "def conv_weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "!pip install pygame\n",
        "import pygame\n",
        "from gym.spaces.box import Box\n",
        "\n",
        "\n",
        "\n",
        "class ReplayBufferLSTM:\n",
        "    \"\"\" \n",
        "    Replay buffer for agent with LSTM network additionally using previous action, can be used \n",
        "    if the hidden states are not stored (arbitrary initialization of lstm for training).\n",
        "    And each sample contains the whole episode instead of a single step.\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, state, action, last_action, reward, next_state, done):\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(None)\n",
        "        self.buffer[self.position] = (state, action, last_action, reward, next_state, done)\n",
        "        self.position = int((self.position + 1) % self.capacity)  # as a ring buffer\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, last_action, reward, next_state, done = map(np.stack,\n",
        "                                                      zip(*batch))  # stack for each element\n",
        "        ''' \n",
        "        the * serves as unpack: sum(a,b) <=> batch=(a,b), sum(*batch) ;\n",
        "        zip: a=[1,2], b=[2,3], zip(a,b) => [(1, 2), (2, 3)] ;\n",
        "        the map serves as mapping the function on each list element: map(square, [2,3]) => [4,9] ;\n",
        "        np.stack((1,2)) => array([1, 2])\n",
        "        '''\n",
        "        return state, action, last_action, reward, next_state, done\n",
        "\n",
        "    def __len__(\n",
        "            self):  # cannot work in multiprocessing case, len(replay_buffer) is not available in proxy of manager!\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def get_length(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "class ReplayBufferLSTM2:\n",
        "    \"\"\" \n",
        "    Replay buffer for agent with LSTM network additionally storing previous action, \n",
        "    initial input hidden state and output hidden state of LSTM.\n",
        "    And each sample contains the whole episode instead of a single step.\n",
        "    'hidden_in' and 'hidden_out' are only the initial hidden state for each episode, for LSTM initialization.\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, hidden_in, hidden_out, state, action, last_action, reward, next_state, done):\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(None)\n",
        "        self.buffer[self.position] = (hidden_in, hidden_out, state, action, last_action, reward, next_state, done)\n",
        "        self.position = int((self.position + 1) % self.capacity)  # as a ring buffer\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        s_lst, a_lst, la_lst, r_lst, ns_lst, hi_lst, ci_lst, ho_lst, co_lst, d_lst=[],[],[],[],[],[],[],[],[],[]\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        for sample in batch:\n",
        "            (h_in, c_in), (h_out, c_out), state, action, last_action, reward, next_state, done = sample\n",
        "            s_lst.append(state) \n",
        "            a_lst.append(action)\n",
        "            la_lst.append(last_action)\n",
        "            r_lst.append(reward)\n",
        "            ns_lst.append(next_state)\n",
        "            d_lst.append(done)\n",
        "            hi_lst.append(h_in)  # h_in: (1, batch_size=1, hidden_size)\n",
        "            ci_lst.append(c_in)\n",
        "            ho_lst.append(h_out)\n",
        "            co_lst.append(c_out)\n",
        "        hi_lst = torch.cat(hi_lst, dim=-2).detach() # cat along the batch dim\n",
        "        ho_lst = torch.cat(ho_lst, dim=-2).detach()\n",
        "        ci_lst = torch.cat(ci_lst, dim=-2).detach()\n",
        "        co_lst = torch.cat(co_lst, dim=-2).detach()\n",
        "\n",
        "        hidden_in = (hi_lst, ci_lst)\n",
        "        hidden_out = (ho_lst, co_lst)\n",
        "\n",
        "        return hidden_in, hidden_out, s_lst, a_lst, la_lst, r_lst, ns_lst, d_lst\n",
        "\n",
        "    def __len__(\n",
        "            self):  # cannot work in multiprocessing case, len(replay_buffer) is not available in proxy of manager!\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def get_length(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import copy\n",
        "\n",
        "\n",
        "class ValueNetworkBase(nn.Module):\n",
        "    \"\"\" Base network class for value function approximation \"\"\"\n",
        "    def __init__(self, state_space, activation):\n",
        "        super(ValueNetworkBase, self).__init__()\n",
        "        self._state_space = state_space\n",
        "        self._state_shape = state_space.shape\n",
        "        if len(self._state_shape) == 1:\n",
        "            self._state_dim = self._state_shape[0]\n",
        "        else:  # high-dim state\n",
        "            pass  \n",
        "\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self):\n",
        "        pass\n",
        "\n",
        "class QNetworkBase(ValueNetworkBase):\n",
        "    def __init__(self, state_space, action_space, activation ):\n",
        "        super().__init__( state_space, activation)\n",
        "        self._action_space = action_space\n",
        "        self._action_shape = action_space.shape\n",
        "        self._action_dim = self._action_shape[0]\n",
        "\n",
        "\n",
        "class ValueNetwork(ValueNetworkBase):\n",
        "    def __init__(self, state_space, hidden_dim, activation=F.relu, output_activation=None):\n",
        "        super().__init__(state_space, activation)\n",
        "\n",
        "        self.linear1 = nn.Linear(self._state_dim, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.linear4 = nn.Linear(hidden_dim, 1)\n",
        "        # weights initialization\n",
        "        self.linear4.apply(linear_weights_init)\n",
        "        \n",
        "    def forward(self, state):\n",
        "        x = self.activation(self.linear1(state))\n",
        "        x = self.activation(self.linear2(x))\n",
        "        x = self.activation(self.linear3(x))\n",
        "        x = self.linear4(x)\n",
        "        return x        \n",
        "\n",
        "\n",
        "class QNetwork(QNetworkBase):\n",
        "    def __init__(self, state_space, action_space, hidden_dim, activation=F.relu, output_activation=None):\n",
        "        super().__init__(state_space, action_space, activation)\n",
        "\n",
        "        self.linear1 = nn.Linear(self._state_dim+self._action_dim, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.linear4 = nn.Linear(hidden_dim, 1)\n",
        "        # weights initialization\n",
        "        self.linear4.apply(linear_weights_init)\n",
        "        \n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state, action], 1) # the dim 0 is number of samples\n",
        "        x = self.activation(self.linear1(x))\n",
        "        x = self.activation(self.linear2(x))\n",
        "        x = self.activation(self.linear3(x))\n",
        "        x = self.linear4(x)\n",
        "        return x        \n",
        "\n",
        "class QNetworkLSTM(QNetworkBase):\n",
        "    \"\"\"\n",
        "    Q network with LSTM structure.\n",
        "    The network follows two-branch structure as in paper: \n",
        "    Sim-to-Real Transfer of Robotic Control with Dynamics Randomization\n",
        "    \"\"\"\n",
        "    def __init__(self, state_space, action_space, hidden_dim, activation=F.relu, output_activation=None):\n",
        "        super().__init__(state_space, action_space, activation)\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.linear1 = nn.Linear(self._state_dim+self._action_dim, hidden_dim)\n",
        "        self.linear2 = nn.Linear(self._state_dim+self._action_dim, hidden_dim)\n",
        "        self.lstm1 = nn.LSTM(hidden_dim, hidden_dim, dropout = 0.2)\n",
        "        self.linear3 = nn.Linear(2*hidden_dim, hidden_dim)\n",
        "        self.linear4 = nn.Linear(hidden_dim, 1)\n",
        "        # weights initialization\n",
        "        self.linear4.apply(linear_weights_init)\n",
        "        \n",
        "    def forward(self, state, action, last_action, hidden_in):\n",
        "        \"\"\" \n",
        "        state shape: (batch_size, sequence_length, state_dim)\n",
        "        output shape: (batch_size, sequence_length, 1)\n",
        "        for lstm needs to be permuted as: (sequence_length, batch_size, state_dim)\n",
        "        \"\"\"\n",
        "        state = state.permute(1,0,2)\n",
        "        action = action.permute(1,0,2)\n",
        "        last_action = last_action.permute(1,0,2)\n",
        "        # branch 1\n",
        "        fc_branch = torch.cat([state, action], -1) \n",
        "        fc_branch = self.activation(self.linear1(fc_branch))\n",
        "        # branch 2\n",
        "        lstm_branch = torch.cat([state, last_action], -1) \n",
        "        lstm_branch = self.activation(self.linear2(lstm_branch))  # linear layer for 3d input only applied on the last dim\n",
        "        lstm_branch, lstm_hidden = self.lstm1(lstm_branch, hidden_in)  # no activation after lstm\n",
        "        # merged\n",
        "        merged_branch=torch.cat([fc_branch, lstm_branch], -1) \n",
        "\n",
        "        x = self.activation(self.linear3(merged_branch))\n",
        "        x = self.linear4(x)\n",
        "        x = x.permute(1,0,2)  # back to same axes as input    \n",
        "        return x, lstm_hidden    # lstm_hidden is actually tuple: (hidden, cell)   \n",
        "\n",
        "class QNetworkLSTM2(QNetworkBase):\n",
        "    \"\"\"\n",
        "    Q network with LSTM structure.\n",
        "    The network follows single-branch structure as in paper: \n",
        "    Memory-based control with recurrent neural networks\n",
        "    \"\"\"\n",
        "    def __init__(self, state_space, action_space, hidden_dim, activation=F.relu, output_activation=None):\n",
        "        super().__init__(state_space, action_space, activation)\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.linear1 = nn.Linear(self._state_dim+2*self._action_dim, hidden_dim)\n",
        "        self.lstm1 = nn.LSTM(hidden_dim, hidden_dim, dropout = 0.2)\n",
        "        self.linear2 = nn.Linear(hidden_dim, 1)\n",
        "        # weights initialization\n",
        "        self.linear2.apply(linear_weights_init)\n",
        "        \n",
        "    def forward(self, state, action, last_action, hidden_in):\n",
        "        \"\"\" \n",
        "        state shape: (batch_size, sequence_length, state_dim)\n",
        "        output shape: (batch_size, sequence_length, 1)\n",
        "        for lstm needs to be permuted as: (sequence_length, batch_size, state_dim)\n",
        "        \"\"\"\n",
        "        state = state.permute(1,0,2)\n",
        "        action = action.permute(1,0,2)\n",
        "        last_action = last_action.permute(1,0,2)\n",
        "        # single branch\n",
        "        x = torch.cat([state, action, last_action], -1) \n",
        "        x = self.activation(self.linear1(x))\n",
        "        x, lstm_hidden = self.lstm1(x, hidden_in)  # no activation after lstm\n",
        "        x = self.linear2(x)\n",
        "        x = x.permute(1,0,2)  # back to same axes as input    \n",
        "        return x, lstm_hidden    # lstm_hidden is actually tuple: (hidden, cell)   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PolicyNetworkBase(nn.Module):\n",
        "    \"\"\" Base network class for policy function \"\"\"\n",
        "    def __init__(self, state_space, action_space, action_range):\n",
        "        super(PolicyNetworkBase, self).__init__()\n",
        "        self._state_space = state_space\n",
        "        self._state_shape = state_space.shape\n",
        "        if len(self._state_shape) == 1:\n",
        "            self._state_dim = self._state_shape[0]\n",
        "        else:  # high-dim state\n",
        "            pass  \n",
        "        self._action_space = action_space\n",
        "        self._action_shape = action_space.shape\n",
        "        if len(self._action_shape) < 1:  # Discrete space\n",
        "            self._action_dim = action_space.n\n",
        "        else:\n",
        "            self._action_dim = self._action_shape[0]\n",
        "        self.action_range = action_range\n",
        "\n",
        "    def forward(self):\n",
        "        pass\n",
        "    \n",
        "    def evaluate(self):\n",
        "        pass \n",
        "    \n",
        "    def get_action(self):\n",
        "        pass\n",
        "\n",
        "    def sample_action(self,):\n",
        "        a=torch.FloatTensor(self._action_dim).uniform_(-1, 1)\n",
        "        return self.action_range*a.numpy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SAC_PolicyNetworkLSTM(PolicyNetworkBase):\n",
        "    def __init__(self, state_space, action_space, hidden_size, action_range=1., init_w=3e-3, log_std_min=-20, log_std_max=2):\n",
        "        super().__init__(state_space, action_space, action_range=action_range)\n",
        "        \n",
        "        self.log_std_min = log_std_min\n",
        "        self.log_std_max = log_std_max\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.linear1 = nn.Linear(self._state_dim, hidden_size)\n",
        "        self.linear2 = nn.Linear(self._state_dim+self._action_dim, hidden_size)\n",
        "        self.lstm1 = nn.LSTM(hidden_size, hidden_size, dropout = 0.2)\n",
        "        self.linear3 = nn.Linear(2*hidden_size, hidden_size)\n",
        "\n",
        "        self.mean_linear = nn.Linear(hidden_size, self._action_dim)\n",
        "        self.mean_linear.weight.data.uniform_(-init_w, init_w)\n",
        "        self.mean_linear.bias.data.uniform_(-init_w, init_w)\n",
        "        \n",
        "        self.log_std_linear = nn.Linear(hidden_size, self._action_dim)\n",
        "        self.log_std_linear.weight.data.uniform_(-init_w, init_w)\n",
        "        self.log_std_linear.bias.data.uniform_(-init_w, init_w)\n",
        "\n",
        "\n",
        "    def forward(self, state, last_action, hidden_in):\n",
        "        \"\"\" \n",
        "        state shape: (batch_size, sequence_length, state_dim)\n",
        "        output shape: (batch_size, sequence_length, action_dim)\n",
        "        for lstm needs to be permuted as: (sequence_length, batch_size, -1)\n",
        "        \"\"\"\n",
        "        state = state.permute(1,0,2)\n",
        "        last_action = last_action.permute(1,0,2)\n",
        "        # branch 1\n",
        "        fc_branch = F.relu(self.linear1(state))\n",
        "        # branch 2\n",
        "        lstm_branch = torch.cat([state, last_action], -1)\n",
        "        lstm_branch = F.relu(self.linear2(lstm_branch))\n",
        "        lstm_branch, lstm_hidden = self.lstm1(lstm_branch, hidden_in)  # no activation after lstm\n",
        "        # merged\n",
        "        merged_branch=torch.cat([fc_branch, lstm_branch], -1) \n",
        "        x = torch.tanh(self.linear3(merged_branch))\n",
        "        x = x.permute(1,0,2)  # permute back\n",
        "\n",
        "        mean    = self.mean_linear(x)\n",
        "        # mean    = F.leaky_relu(self.mean_linear(x))\n",
        "        log_std = self.log_std_linear(x)\n",
        "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
        "        \n",
        "        return mean, log_std, lstm_hidden\n",
        "    \n",
        "    def evaluate(self, state, last_action, hidden_in, epsilon=1e-6):\n",
        "        '''\n",
        "        generate sampled action with state as input wrt the policy network;\n",
        "        '''\n",
        "        mean, log_std, hidden_out = self.forward(state, last_action, hidden_in)\n",
        "        std = log_std.exp() # no clip in evaluation, clip affects gradients flow\n",
        "        \n",
        "        normal = Normal(0, 1)\n",
        "        z = normal.sample(mean.shape)\n",
        "        action_0 = torch.tanh(mean + std * z)  # TanhNormal distribution as actions; reparameterization trick\n",
        "        action = self.action_range * action_0\n",
        "        log_prob = Normal(mean, std).log_prob(mean + std * z) - torch.log(\n",
        "            1. - action_0.pow(2) + epsilon) - np.log(self.action_range)\n",
        "        # both dims of normal.log_prob and -log(1-a**2) are (N,dim_of_action);\n",
        "        # the Normal.log_prob outputs the same dim of input features instead of 1 dim probability,\n",
        "        # needs sum up across the features dim to get 1 dim prob; or else use Multivariate Normal.\n",
        "        log_prob = log_prob.sum(dim=-1, keepdim=True)\n",
        "        return action, log_prob, z, mean, log_std, hidden_out\n",
        "\n",
        "    def get_action(self, state, last_action, hidden_in, deterministic=True):\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).unsqueeze(0) # increase 2 dims to match with training data\n",
        "        last_action = torch.FloatTensor(last_action).unsqueeze(0).unsqueeze(0)\n",
        "        mean, log_std, hidden_out = self.forward(state, last_action, hidden_in)\n",
        "        std = log_std.exp()\n",
        "        \n",
        "        normal = Normal(0, 1)\n",
        "        z = normal.sample(mean.shape)\n",
        "        action = self.action_range * torch.tanh(mean + std * z)\n",
        "\n",
        "        action = self.action_range * torch.tanh(mean).detach().cpu().numpy() if deterministic else \\\n",
        "        action.detach().cpu().numpy()\n",
        "        return action[0][0], hidden_out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Soft Actor-Critic version 2\n",
        "using target Q instead of V net: 2 Q net, 2 target Q net, 1 policy net\n",
        "add alpha loss compared with version 1\n",
        "paper: https://arxiv.org/pdf/1812.05905.pdf\n",
        "'''\n",
        "\n",
        "\n",
        "from torch.distributions import Normal\n",
        "from IPython.display import clear_output\n",
        "from matplotlib import animation\n",
        "from IPython.display import display\n",
        "\n",
        "GPU = False\n",
        "device_idx = 0\n",
        "if GPU:\n",
        "    device = torch.device(\"cuda:\" + str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)\n",
        "\n",
        "\n",
        "\n",
        "class SAC_Trainer():\n",
        "    def __init__(self, replay_buffer, state_space, action_space, hidden_dim, action_range, gamma, soft_q_lr, policy_lr, alpha_lr, batch_size, update_itr, reward_scale, target_entropy, soft_tau, train_freq):\n",
        "\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.update_itr = update_itr\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.reward_scale = reward_scale\n",
        "        self.target_entropy = target_entropy\n",
        "        self.soft_tau = soft_tau\n",
        "        self.train_freq = train_freq\n",
        "\n",
        "        self.replay_buffer = replay_buffer\n",
        "        self.soft_q_net1 = QNetworkLSTM2(state_space, action_space, hidden_dim).to(device)\n",
        "        self.soft_q_net2 = QNetworkLSTM2(state_space, action_space, hidden_dim).to(device)\n",
        "        self.target_soft_q_net1 = QNetworkLSTM2(state_space, action_space, hidden_dim).to(device)\n",
        "        self.target_soft_q_net2 = QNetworkLSTM2(state_space, action_space, hidden_dim).to(device)\n",
        "        self.policy_net = SAC_PolicyNetworkLSTM(state_space, action_space, hidden_dim, action_range).to(device)\n",
        "        self.log_alpha = torch.zeros(1, dtype=torch.float32, requires_grad=True, device=device)\n",
        "        print('Soft Q Network (1,2): ', self.soft_q_net1)\n",
        "        print('Policy Network: ', self.policy_net)\n",
        "\n",
        "        for target_param, param in zip(self.target_soft_q_net1.parameters(), self.soft_q_net1.parameters()):\n",
        "            target_param.data.copy_(param.data)\n",
        "        for target_param, param in zip(self.target_soft_q_net2.parameters(), self.soft_q_net2.parameters()):\n",
        "            target_param.data.copy_(param.data)\n",
        "\n",
        "        self.soft_q_criterion1 = nn.MSELoss()\n",
        "        self.soft_q_criterion2 = nn.MSELoss()\n",
        "\n",
        "        # soft_q_lr = 0.0015\n",
        "        # policy_lr = 0.0015\n",
        "        # alpha_lr  = 0.0015\n",
        "\n",
        "        self.soft_q_optimizer1 = optim.Adam(self.soft_q_net1.parameters(), lr=soft_q_lr)\n",
        "        self.soft_q_optimizer2 = optim.Adam(self.soft_q_net2.parameters(), lr=soft_q_lr)\n",
        "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=policy_lr)\n",
        "        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=alpha_lr)\n",
        "\n",
        "    \n",
        "    def update(self, batch_size, reward_scale=10., auto_entropy=True, target_entropy=-2, gamma=0.975,soft_tau=1e-2):\n",
        "        hidden_in, hidden_out, state, action, last_action, reward, next_state, done = self.replay_buffer.sample(batch_size)\n",
        "        # print('sample:', state, action,  reward, done)\n",
        "\n",
        "        batch_size = self.batch_size\n",
        "        reward_scale = self.reward_scale\n",
        "        gamma = self.gamma\n",
        "        target_entropy = self.target_entropy\n",
        "        soft_tau = self.soft_tau\n",
        "\n",
        "\n",
        "        state      = torch.FloatTensor(np.array(state)).to(device)\n",
        "        next_state = torch.FloatTensor(np.array(next_state)).to(device)\n",
        "        action     = torch.FloatTensor(np.array(action)).to(device)\n",
        "        last_action     = torch.FloatTensor(np.array(last_action)).to(device)\n",
        "        reward     = torch.FloatTensor(np.array(reward)).unsqueeze(-1).to(device)  # reward is single value, unsqueeze() to add one dim to be [reward] at the sample dim;\n",
        "        done       = torch.FloatTensor(np.float32(done)).unsqueeze(-1).to(device)\n",
        "\n",
        "\n",
        "        # state      = torch.FloatTensor(state).to(device)\n",
        "        # next_state = torch.FloatTensor(next_state).to(device)\n",
        "        # action     = torch.FloatTensor(action).to(device)\n",
        "        # last_action     = torch.FloatTensor(last_action).to(device)\n",
        "        # reward     = torch.FloatTensor(reward).unsqueeze(-1).to(device)  # reward is single value, unsqueeze() to add one dim to be [reward] at the sample dim;\n",
        "        # done       = torch.FloatTensor(np.float32(done)).unsqueeze(-1).to(device)\n",
        "\n",
        "        predicted_q_value1, _ = self.soft_q_net1(state, action, last_action, hidden_in)\n",
        "        predicted_q_value2, _ = self.soft_q_net2(state, action, last_action, hidden_in)\n",
        "        new_action, log_prob, z, mean, log_std, _ = self.policy_net.evaluate(state, last_action, hidden_in)\n",
        "        new_next_action, next_log_prob, _, _, _, _ = self.policy_net.evaluate(next_state, action, hidden_out)\n",
        "        reward = reward_scale * (reward - reward.mean(dim=0)) / (reward.std(dim=0) + 1e-6) # normalize with batch mean and std; plus a small number to prevent numerical problem\n",
        "    # Updating alpha wrt entropy\n",
        "        # alpha = 0.0  # trade-off between exploration (max entropy) and exploitation (max Q) \n",
        "        if auto_entropy is True:\n",
        "            alpha_loss = -(self.log_alpha * (log_prob + target_entropy).detach()).mean()\n",
        "            # print('alpha loss: ',alpha_loss)\n",
        "            self.alpha_optimizer.zero_grad()\n",
        "            alpha_loss.backward()\n",
        "            self.alpha_optimizer.step()\n",
        "            self.alpha = self.log_alpha.exp()\n",
        "        else:\n",
        "            self.alpha = 1.\n",
        "            alpha_loss = 0\n",
        "\n",
        "    # Training Q Function\n",
        "        predict_target_q1, _ = self.target_soft_q_net1(next_state, new_next_action, action, hidden_out)\n",
        "        predict_target_q2, _ = self.target_soft_q_net2(next_state, new_next_action, action, hidden_out)\n",
        "        target_q_min = torch.min(predict_target_q1, predict_target_q2) - self.alpha * next_log_prob\n",
        "        target_q_value = reward + (1 - done) * gamma * target_q_min # if done==1, only reward\n",
        "        q_value_loss1 = self.soft_q_criterion1(predicted_q_value1, target_q_value.detach())  # detach: no gradients for the variable\n",
        "        q_value_loss2 = self.soft_q_criterion2(predicted_q_value2, target_q_value.detach())\n",
        "\n",
        "\n",
        "        self.soft_q_optimizer1.zero_grad()\n",
        "        q_value_loss1.backward()\n",
        "        self.soft_q_optimizer1.step()\n",
        "        self.soft_q_optimizer2.zero_grad()\n",
        "        q_value_loss2.backward()\n",
        "        self.soft_q_optimizer2.step()  \n",
        "\n",
        "    # Training Policy Function\n",
        "        predict_q1, _= self.soft_q_net1(state, new_action, last_action, hidden_in)\n",
        "        predict_q2, _ = self.soft_q_net2(state, new_action, last_action, hidden_in)\n",
        "        predicted_new_q_value = torch.min(predict_q1, predict_q2)\n",
        "        policy_loss = (self.alpha * log_prob - predicted_new_q_value).mean()\n",
        "\n",
        "        self.policy_optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        self.policy_optimizer.step()\n",
        "        \n",
        "        # print('q loss: ', q_value_loss1, q_value_loss2)\n",
        "        # print('policy loss: ', policy_loss )\n",
        "\n",
        "\n",
        "    # Soft update the target value net\n",
        "        for target_param, param in zip(self.target_soft_q_net1.parameters(), self.soft_q_net1.parameters()):\n",
        "            target_param.data.copy_(  # copy data value into target parameters\n",
        "                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
        "            )\n",
        "        for target_param, param in zip(self.target_soft_q_net2.parameters(), self.soft_q_net2.parameters()):\n",
        "            target_param.data.copy_(  # copy data value into target parameters\n",
        "                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
        "            )\n",
        "        return predicted_new_q_value.mean()\n",
        "\n",
        "    def save_model(self, path):\n",
        "        torch.save(self.soft_q_net1.state_dict(), path+'/lstm_q1')\n",
        "        torch.save(self.soft_q_net2.state_dict(), path+'/lstm_q2')\n",
        "        torch.save(self.policy_net.state_dict(), path+'/lstm_policy')\n",
        "\n",
        "    def load_model(self, path):\n",
        "        self.soft_q_net1.load_state_dict(torch.load(path+'/lstm_q1', map_location=torch.device('cpu')))\n",
        "        self.soft_q_net2.load_state_dict(torch.load(path+'/lstm_q2', map_location=torch.device('cpu')))\n",
        "        self.policy_net.load_state_dict(torch.load(path+'/lstm_policy', map_location=torch.device('cpu')))\n",
        "\n",
        "        self.soft_q_net1.eval()\n",
        "        self.soft_q_net2.eval()\n",
        "        self.policy_net.eval()\n",
        "\n",
        "\n",
        "def plot(rewards):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.plot(rewards)\n",
        "    plt.savefig('sac_v2_lstm.png')\n",
        "    # plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUPPiuw8e_qz"
      },
      "source": [
        "#### make agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-SgBmKMuFwG"
      },
      "outputs": [],
      "source": [
        "replay_buffer_size = 100\n",
        "batch_size = 10\n",
        "replay_buffer = ReplayBufferLSTM2(replay_buffer_size)\n",
        "\n",
        "# choose env\n",
        "action_space = env.action_space\n",
        "state_space  = env.observation_space\n",
        "action_range=1.\n",
        "\n",
        "action_dim = action_space.shape[0]\n",
        "\n",
        "# hyper-parameters for RL training\n",
        "max_episodes  = 1000\n",
        "max_steps   = 1024\n",
        "frame_idx   = 0\n",
        "AUTO_ENTROPY=True\n",
        "DETERMINISTIC=False\n",
        "rewards     = []\n",
        "train_freq = 100\n",
        "\n",
        "\n",
        "# # env includes memory\n",
        "# gamma=1-0.017155091291799374\n",
        "# soft_q_lr=0.0010223596792273903\n",
        "# policy_lr=1.670414519639193e-05\n",
        "# alpha_lr=0.0005012216457204279\n",
        "# batch_size=512\n",
        "# update_itr=4\n",
        "# hidden_dim=256\n",
        "# reward_scale=10\n",
        "# target_entropy=-2\n",
        "# soft_tau=0.004341682252329602\n",
        "# train_freq = 10\n",
        "\n",
        "# # env does not include memory\n",
        "# gamma = 1-0.0008767920887906205\n",
        "# soft_q_lr = 3.45249248634654e-05\n",
        "# policy_lr = 0.003925404720612826\n",
        "# alpha_lr = 0.03434388145436511\n",
        "# batch_size = 16\n",
        "# update_itr = 1\n",
        "# hidden_dim = 256\n",
        "# reward_scale = 100\n",
        "# target_entropy = -2\n",
        "# soft_tau = 1.2326412869674132e-06\n",
        "# train_freq = 10\n",
        "\n",
        "\n",
        "#sac_trainer=SAC_Trainer(replay_buffer, state_space, action_space, hidden_dim=hidden_dim, action_range=action_range  )\n",
        "sac_trainer=SAC_Trainer(replay_buffer, state_space, action_space, hidden_dim=hidden_dim, action_range=action_range,\\\n",
        "                          gamma = gamma, soft_q_lr = soft_q_lr, policy_lr = policy_lr, alpha_lr = alpha_lr, \\\n",
        "                          batch_size = batch_size, update_itr = update_itr, reward_scale = reward_scale, \\\n",
        "                        target_entropy = target_entropy, soft_tau = soft_tau, train_freq = train_freq)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClwgDbJzqTl0"
      },
      "source": [
        "#### LOAD (not simulated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGtmyHHMqTl8"
      },
      "outputs": [],
      "source": [
        "class CollectInformation(MultiFF):  # Note when using this wrapper, the number of steps cannot exceed one episode\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "      self.ff_information = np.ones([self.num_ff, 8])*(-9999)   #[index, x, y, time_start, time_captured, mx(when_captured), my(when_captured), index_in_flash]\n",
        "      self.episode_len = 15000\n",
        "      self.num_intervals = 25000\n",
        "  def reset(self):\n",
        "      self.obs = super().reset()\n",
        "      self.ff_information = np.ones([self.num_ff, 8])*(-9999)   #[index, x, y, time_start, time_captured, mx(when_captured), my(when_captured), index_in_flash]\n",
        "      self.ff_information[:,0] = np.arange(self.num_ff)\n",
        "      self.ff_information[:,7] = np.arange(self.num_ff)\n",
        "      self.ff_information[:,1] = self.ffx.clone().numpy()\n",
        "      self.ff_information[:,2] = self.ffy.clone().numpy()\n",
        "      self.ff_information[:,3] = 0\n",
        "      return self.obs\n",
        "\n",
        "  def calculate_reward(self):\n",
        "      reward = super().calculate_reward()\n",
        "      if self.num_targets > 0: \n",
        "        for index in self.captured_ff_index:\n",
        "          overall_index = int(self.ff_information[:,0][np.where(self.ff_information[:,-1]==index)[0][-1]])\n",
        "          self.ff_information[overall_index, 4] = self.time\n",
        "          self.ff_information[overall_index, 5] = self.agentx.item()\n",
        "          self.ff_information[overall_index, 6] = self.agenty.item()\n",
        "        ## The following codes are not needed because this is for simulation\n",
        "        self.new_ff_info = np.ones([self.num_targets, 8])*(-9999)\n",
        "        self.new_ff_info[:,0] = np.arange(len(self.ff_information), len(self.ff_information)+self.num_targets)\n",
        "        self.new_ff_info[:,7] = np.array(self.captured_ff_index)\n",
        "        self.new_ff_info[:,1] = self.ffx[self.captured_ff_index].numpy()\n",
        "        self.new_ff_info[:,2] = self.ffy[self.captured_ff_index].numpy()\n",
        "        self.new_ff_info[:,3] = self.time + self.dt\n",
        "        self.ff_information = np.concatenate([self.ff_information, self.new_ff_info], axis = 0)\n",
        "      return(reward)\n",
        "\n",
        "\n",
        "env = CollectInformation()\n",
        "\n",
        "env.flash_on_interval = 0.3\n",
        "env.distance2center_cost = 0\n",
        "\n",
        "sac_trainer.load_model(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bw54TJsIdJV"
      },
      "source": [
        "### test (LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HA81DP2mhPcB"
      },
      "outputs": [],
      "source": [
        "env = CollectInformation()\n",
        "\n",
        "env.flash_on_interval = 0.3\n",
        "env.distance2center_cost = 0\n",
        "sac_trainer.load_model(model_path)\n",
        "\n",
        "\n",
        "# env.flash_on_interval = 0.3\n",
        "\n",
        "# model_path = \"/content/gdrive/MyDrive/fireflies_agent/lstm/July_17\"\n",
        "# sac_trainer.load_model(model_path)\n",
        "\n",
        "# Test the trained agent\n",
        "n_steps = 10000\n",
        "\n",
        "obs = env.reset()\n",
        "\n",
        "state = obs\n",
        "\n",
        "last_action = env.action_space.sample()\n",
        "hidden_out = (torch.zeros([1, 1, hidden_dim], dtype=torch.float), \\\n",
        "        torch.zeros([1, 1, hidden_dim], dtype=torch.float))  # initialize hidden state for lstm, (hidden, cell), each is (layer, batch, dim)             \n",
        "episode_reward = 0\n",
        "hidden_out = (torch.zeros([1, 1, hidden_dim], dtype=torch.float), \\\n",
        "    torch.zeros([1, 1, hidden_dim], dtype=torch.float))  # initialize hidden state for lstm, (hidden, cell), each is (layer, batch, dim)\n",
        "mx = []\n",
        "my = []\n",
        "monkey_t = []\n",
        "monkey_speed_info = []\n",
        "monkey_angle = [] # in radians\n",
        "ffxy_all = []\n",
        "ffxy_visible = []\n",
        "ffxy2_all = []\n",
        "time_rl = []\n",
        "reward_log = []\n",
        "mx_rewarded = []\n",
        "my_rewarded = []\n",
        "mheading = []\n",
        "captured_ff = []\n",
        "current_target = []\n",
        "action_reward = []\n",
        "num_targets = []\n",
        "env_obs = []\n",
        "#captured_ff_cum_x = []\n",
        "#captured_ff_cum_y = []\n",
        "all_captured_ff_x = []\n",
        "all_captured_ff_y = []\n",
        "visible_ff_indices_all = []\n",
        "memory_ff_indices_all = []\n",
        "obs_ff_indices_all = []\n",
        "obs_ff_overall_indices_all = []\n",
        "memory_ff_indices_all = []\n",
        "obs_ff_indices_all = []\n",
        "ff_angles2 = []\n",
        "ff_distances2 = []\n",
        "for step in range(n_steps):\n",
        "  hidden_in = hidden_out\n",
        "  action, hidden_out = sac_trainer.policy_net.get_action(state, last_action, hidden_in, deterministic = DETERMINISTIC)\n",
        "  previous_ffxy = env.ffxy\n",
        "  prev_ff_information = env.ff_information.copy()\n",
        "  next_state, reward, done, _ = env.step(action)  \n",
        "  #print(step, \"Action: \", action)\n",
        "  #print(hidden_out)\n",
        "  last_action = action\n",
        "  episode_reward += reward\n",
        "  state=next_state\n",
        "  reward_log.append(reward)\n",
        "  num_targets.append(env.num_targets)\n",
        "  if env.num_targets > 0:\n",
        "    #print(step, \"num_targets:\",  env.num_targets)\n",
        "    captured_ff.append(env.captured_ff_index)\n",
        "    all_captured_ff_x = all_captured_ff_x + previous_ffxy[env.captured_ff_index][:,0].tolist()\n",
        "    all_captured_ff_y = all_captured_ff_y + previous_ffxy[env.captured_ff_index][:,1].tolist()\n",
        "  else:\n",
        "    captured_ff.append(0)\n",
        "  #captured_ff_cum_x.append(all_captured_ff_x)\n",
        "  #captured_ff_cum_y.append(all_captured_ff_y)\n",
        "  mx.append(env.agentx.item())\n",
        "  my.append(env.agenty.item())\n",
        "  monkey_t.append(env.time)\n",
        "  monkey_speed_info.append(env.dv.item())\n",
        "  mheading.append(env.agentheading.item())\n",
        "  time_rl.append(env.time)\n",
        "  ffxy_all.append(env.ffxy.clone())\n",
        "  ffxy2_all.append(env.ffxy2.clone())\n",
        "  ffxy_visible.append(env.ffxy[env.visible_ff_indices].clone())\n",
        "  env_obs.append(next_state.copy())\n",
        "  visible_ff_indices_all.append(env.visible_ff_indices)\n",
        "  obs_ff_indices_all.append(env.topk_indices.numpy())\n",
        "  real_indices = []\n",
        "  for index in env.topk_indices:\n",
        "    real_indices.append(int(prev_ff_information[:,0][np.where(prev_ff_information[:,7]==index.item())[0][-1]].copy()))\n",
        "  obs_ff_overall_indices_all.append(real_indices)\n",
        "  if len(env.topk_indices) > 0:\n",
        "    ff_angles2.append(env.ff_angle_topk_2)\n",
        "    ff_distances2.append(env.ff_distance_topk)\n",
        "  else:\n",
        "    ff_angles2.append(torch.tensor([]))\n",
        "    ff_distances2.append(torch.tensor([]))\n",
        "  if done:\n",
        "    obs = env.reset()\n",
        "  #print(step, ffxy_visible[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp_SSPHzYog4"
      },
      "source": [
        "### monkey & ff information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhR-SSHwYog5"
      },
      "outputs": [],
      "source": [
        "monkey_speed = np.array(monkey_speed_info)\n",
        "\n",
        "monkey_information = {\n",
        "'monkey_x': np.array(mx),\n",
        "'monkey_y': np.array(my),\n",
        "'monkey_t': np.array(monkey_t),\n",
        "'monkey_speed': monkey_speed,\n",
        "'monkey_angle': np.array(mheading),\n",
        "}\n",
        "\n",
        "\n",
        "monkey_information['monkey_speeddummy'] = (monkey_speed>200*0.01*env.dt).astype(int) \n",
        "monkey_speeddummy = (monkey_speed>200*0.01*env.dt).astype(int) \n",
        "delta_time = np.delete(monkey_information['monkey_t'], 0) - np.delete(monkey_information['monkey_t'], -1)\n",
        "monkey_dw = np.diff(np.array(mheading), prepend=mheading[0])/env.dt\n",
        "monkey_information['monkey_dw'] = monkey_dw\n",
        "\n",
        "\n",
        "#ff: \n",
        "#[index, x, y, time_start, time_captured, mx(when_captured), my(when_captured), index_in_flash]\n",
        "# Note: when collecting firefly data, no more than one episode can be run. Otherwise the data might be messed up.\n",
        "# A solution is to elongate an episode\n",
        "\n",
        "\n",
        "# sort the time of capture for ff (if they have been captured)\n",
        "ff_information = env.ff_information.copy()\n",
        "ff_time_captured_all = ff_information[:,4]\n",
        "captured_ff_indices = np.where(ff_time_captured_all != -9999)[0]\n",
        "not_captured_ff_indices = np.where(ff_time_captured_all == -9999)[0]\n",
        "num_captured_ff = len(captured_ff_indices)\n",
        "sorted_indices_captured = captured_ff_indices[np.argsort(ff_time_captured_all[captured_ff_indices])]\n",
        "sort_indices_all = np.concatenate([sorted_indices_captured, not_captured_ff_indices])\n",
        "ff_information_sorted = ff_information[sort_indices_all]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# make ff_flash_sorted\n",
        "ff_flash_sorted = []\n",
        "env_end_time = env.time\n",
        "for ff in ff_information[sorted_indices_captured]:\n",
        "  flash = env.ff_flash[int(ff[7])].numpy()\n",
        "  replace_start = False\n",
        "  replace_end = False\n",
        "\n",
        "  # first flatten flash, and then find the elements that are before ff start time\n",
        "  # if the number of elements is even, then the flashing_start_time is the start time of the next interval\n",
        "  before_start = np.where(flash.flatten() <= ff[3])[0]\n",
        "  if len(before_start) > 0:\n",
        "    if len(before_start)%2 == 0:\n",
        "      start_flash_index = int(len(before_start)/2)\n",
        "    else:\n",
        "      start_flash_index = int((len(before_start)-1)/2)\n",
        "      replace_start = True\n",
        "  else:\n",
        "      start_flash_index = 0\n",
        "\n",
        "\n",
        "  after_finish = np.where(flash.flatten() >= ff[4])[0]\n",
        "  if len(after_finish) > 0:\n",
        "    num_indices_before_finish = after_finish[0]\n",
        "    if num_indices_before_finish%2 == 0:\n",
        "      end_flash_index = int(num_indices_before_finish/2) - 1\n",
        "    else: \n",
        "      end_flash_index = int((num_indices_before_finish+1)/2) - 1\n",
        "      replace_end = True\n",
        "  else:\n",
        "    end_flash_index = len(flash)-1\n",
        "\n",
        "\n",
        "  \n",
        "  ff_flash = flash[start_flash_index:(end_flash_index+1)]\n",
        "\n",
        "  if len(ff_flash) < 2:\n",
        "    ff_flash = flash[end_flash_index-1: end_flash_index+1]\n",
        "    if len(ff_flash) < 2:\n",
        "      ff_flash = np.array([[-1,-1]])\n",
        "\n",
        "\n",
        "  if replace_start == True:\n",
        "    ff_flash[0, 0] = ff[3]\n",
        "  if replace_end == True:\n",
        "    ff_flash[-1, 1] = ff[4]\n",
        "\n",
        "  if len(ff_flash) == 0:\n",
        "    ff_flash = np.array([[-1,-1]])\n",
        "\n",
        "  ff_flash_sorted.append(ff_flash)\n",
        "\n",
        "\n",
        "\n",
        "# For the ffs that have never been captured, the end_flash_time is evaluated not in relation to the \n",
        "# time of captrue, but to the time that the env ends\n",
        "for ff in ff_information[not_captured_ff_indices]:\n",
        "  flash = env.ff_flash[int(ff[7])].numpy()\n",
        "  replace_start = False\n",
        "\n",
        "\n",
        "  # first flatten flash, and then find the elements that are before ff start time\n",
        "  # if the number of elements is even, then the flashing_start_time is the start time of the next interval\n",
        "  before_start = np.where(flash.flatten() <= ff[3])[0]\n",
        "  if len(before_start)%2 == 0:\n",
        "    start_flash_index = int(len(before_start)/2)\n",
        "  else:\n",
        "    start_flash_index = int((len(before_start)-1)/2)\n",
        "    replace_start = True\n",
        "\n",
        "\n",
        "# # if we only want the part before the testing ends\n",
        "#   after_finish = np.where(flash.flatten() >= env_end_time)[0] # differing from captured ffs\n",
        "#   num_indices_before_finish = after_finish[0]\n",
        "#   if num_indices_before_finish%2 == 0:\n",
        "#     end_flash_index = int(num_indices_before_finish/2) - 1\n",
        "#   else: \n",
        "#     end_flash_index = int((num_indices_before_finish+1)/2) - 1\n",
        "#     replace_end = True\n",
        "#   ff_flash = flash[start_flash_index:(end_flash_index+1)]\n",
        "\n",
        "\n",
        "  ff_flash = flash[start_flash_index:]\n",
        "\n",
        "  if len(ff_flash) < 2:\n",
        "    ff_flash = flash[end_flash_index-1: end_flash_index+1]\n",
        "    if len(ff_flash) < 2:\n",
        "      ff_flash = np.array([[-1,-1]])\n",
        "\n",
        "\n",
        "  if replace_start == True:\n",
        "    ff_flash[0, 0] = ff[3]\n",
        "\n",
        "  # # if we only want the part before the testing ends\n",
        "  # if replace_end == True:\n",
        "  #   ff_flash[-1, 1] = ff[4]\n",
        "  if len(ff_flash) == 0:\n",
        "    ff_flash = np.array([[-1,-1]])\n",
        "\n",
        "  ff_flash_sorted.append(ff_flash)\n",
        "\n",
        "\n",
        "ff_catched_T_sorted = ff_time_captured_all[sorted_indices_captured] # Note that these two will be shorter than the other arrays\n",
        "ff_believed_position_sorted = ff_information[:,5:7][sorted_indices_captured]\n",
        "\n",
        "ff_real_position_sorted = ff_information[:,1:3][sort_indices_all]\n",
        "ff_life_sorted = ff_information[:,3:5][sort_indices_all]\n",
        "ff_life_sorted[:,1][np.where(ff_life_sorted[:,1]==-9999)[0]] = env.time\n",
        "ff_flash_end_sorted = [flash[-1,1] if len(flash) > 0 else env.time for flash in ff_flash_sorted]\n",
        "ff_flash_end_sorted = np.array(ff_flash_end_sorted)\n",
        "\n",
        "catched_ff_num = len(ff_catched_T_sorted) - 200\n",
        "total_ff_num = len(ff_life_sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7VLu6YNY72e"
      },
      "source": [
        "## Organize data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z6lEBAhxRck"
      },
      "source": [
        "#### ff_dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0av0F7vFPwVI"
      },
      "source": [
        "Note: here angels are in radians, whereas in previous stages, angles are in degrees. In addition, here angle_boundary is added. It replaces angle as a standard to select valid points. And because of that, more points are selected into the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD0JWjoCDQC-"
      },
      "outputs": [],
      "source": [
        "if NEW_DATASET == False:\n",
        "  filepath = 'gdrive/MyDrive/fireflies_data/' + data_folder_name + '/ff_dataframe.csv'\n",
        "  ff_dataframe = pd.read_csv(filepath)\n",
        "  if len(ff_catched_T_sorted) > 400:\n",
        "    ff_dataframe = ff_dataframe[ff_dataframe['time'] < ff_catched_T_sorted[-200]]\n",
        "  ff_dataframe = ff_dataframe.drop([\"Unnamed: 0\"], axis=1)\n",
        "  ff_dataframe.memory = ff_dataframe.memory.astype('int')\n",
        "  \n",
        "  # Let's use data from monkey_information. But we shall cut off portion that is before the time of capturing the first target\n",
        "  monkey_t_array0 = np.array(monkey_information['monkey_t'])\n",
        "  monkey_x_array0 = np.array(monkey_information['monkey_x'])\n",
        "  monkey_y_array0 = np.array(monkey_information['monkey_y'])\n",
        "  monkey_angle_array0 = np.array(monkey_information['monkey_angle'])\n",
        "  valid_index = np.where(monkey_t_array0 > ff_catched_T_sorted[0])[0]\n",
        "  num_missed_index = valid_index[0]\n",
        "  monkey_t_array = monkey_t_array0[valid_index]\n",
        "  monkey_x_array = monkey_x_array0[valid_index]\n",
        "  monkey_y_array = monkey_y_array0[valid_index]\n",
        "  monkey_angle_array = monkey_angle_array0[valid_index]\n",
        "  index_array = np.array(range(len(monkey_t_array)))\n",
        "  ff_dataframe['target_x'] = ff_real_position_sorted[np.array(ff_dataframe['target_index'])][:,0]\n",
        "  ff_dataframe['target_y'] = ff_real_position_sorted[np.array(ff_dataframe['target_index'])][:,1]\n",
        "  ff_dataframe['ffdistance2target'] = LA.norm(np.array(ff_dataframe[['ff_x', 'ff_y']])-np.array(ff_dataframe[['target_x', 'target_y']]), axis = 1)\n",
        "  min_point_index = np.min(np.array(ff_dataframe['point_index']))\n",
        "  max_point_index = np.max(np.array(ff_dataframe['point_index']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMUH0sXl76_x"
      },
      "source": [
        "##### for agent\n",
        "(only include those in obs space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkhIG-IGlQHO"
      },
      "outputs": [],
      "source": [
        "if NEW_DATASET == True:\n",
        "  \n",
        "  # Let's use data from monkey_information. But we shall cut off portion that is before the time of capturing the first target\n",
        "  monkey_t_array0 = np.array(monkey_information['monkey_t'])\n",
        "  monkey_x_array0 = np.array(monkey_information['monkey_x'])\n",
        "  monkey_y_array0 = np.array(monkey_information['monkey_y'])\n",
        "  monkey_angle_array0 = np.array(monkey_information['monkey_angle'])\n",
        "  valid_index = np.where(monkey_t_array0 > ff_catched_T_sorted[0])[0]\n",
        "  num_missed_index = valid_index[0]\n",
        "  monkey_t_array = monkey_t_array0[valid_index]\n",
        "  monkey_x_array = monkey_x_array0[valid_index]\n",
        "  monkey_y_array = monkey_y_array0[valid_index]\n",
        "  monkey_angle_array = monkey_angle_array0[valid_index]\n",
        "  #index_array = np.array(range(len(monkey_t_array))) this overlaps with another variable later\n",
        "\n",
        "\n",
        "  max_distance = 400\n",
        "  reward_boundary = 25\n",
        "  #max_time = 400\n",
        "  #max_time = ff_catched_T_sorted[1251] #replace with ff_catched_T_sorted[-1]\n",
        "  ff_index = []\n",
        "  point_index = []\n",
        "  time = []\n",
        "  target_index = []\n",
        "  ff_x = []\n",
        "  ff_y = []\n",
        "  monkey_x = []\n",
        "  monkey_y = []\n",
        "  visible = []\n",
        "  memory = []\n",
        "  ff_distance = []\n",
        "  ff_angle = []\n",
        "  ff_angle_boundary = []\n",
        "  left_right = []\n",
        "  num_captures = []\n",
        "  distance_closest_ff =[]\n",
        "  distance_2ndclosest_ff = []\n",
        "  num_ff_within = []\n",
        "  reward_distance = []\n",
        "  reward_angle = []\n",
        "  catched_ff_num = len(ff_catched_T_sorted) - 200 \n",
        "  total_ff_num = len(ff_life_sorted)\n",
        "\n",
        "\n",
        "  for i in range(total_ff_num):\n",
        "\n",
        "    if i % 100 == 0:\n",
        "      print(i,\" out of \", total_ff_num)\n",
        "\n",
        "\n",
        "    # Go through every visible duration of the same ff\n",
        "    ff_flash = ff_flash_sorted[i]\n",
        "    whether_in_obs = []\n",
        "    original_index = sort_indices_all[i]\n",
        "    for index in valid_index:\n",
        "      obs_ff_indices = obs_ff_overall_indices_all[index]\n",
        "      if original_index in obs_ff_indices:\n",
        "        whether_in_obs.append(True) \n",
        "      else:\n",
        "        whether_in_obs.append(False)\n",
        "\n",
        "    \n",
        "    cum_indices = np.array(whether_in_obs).nonzero()[0]\n",
        "    if len(cum_indices) > 0:\n",
        "\n",
        "      updated_t_array = monkey_t_array[cum_indices]\n",
        "      visible_indices = cum_indices\n",
        "      index_array = cum_indices\n",
        "      in_memory_length = len(index_array)\n",
        "        \n",
        "      cum_t, cum_angle = monkey_t_array[cum_indices], monkey_angle_array[cum_indices]\n",
        "      cum_mx, cum_my = monkey_x_array[cum_indices], monkey_y_array[cum_indices]\n",
        "      distances_to_monkey = LA.norm(np.stack([cum_mx, cum_my], axis=1)-ff_real_position_sorted[i], axis = 1)\n",
        "      \n",
        "      angles_to_monkey = np.arctan2(ff_real_position_sorted[i,1]-cum_my, ff_real_position_sorted[i,0]-cum_mx)-cum_angle\n",
        "      angles_to_monkey[angles_to_monkey > pi] = angles_to_monkey[angles_to_monkey > pi] - 2*pi\n",
        "      angles_to_monkey[angles_to_monkey < -pi] = angles_to_monkey[angles_to_monkey < -pi] + 2*pi\n",
        "      angles_adjusted = np.absolute(angles_to_monkey)-np.abs(np.arcsin(np.divide(reward_boundary, np.maximum(distances_to_monkey, reward_boundary) ))) # use torch clip to get valid arcsin input\n",
        "      angles_adjusted = np.clip(angles_adjusted, 0, pi)\n",
        "      angles_to_monkey = np.sign(angles_to_monkey)* angles_adjusted\n",
        "\n",
        "\n",
        "\n",
        "      # Make a numpy array of points to denote memory, with 0 means being invisible. We also append 99 extra points after the last point    \n",
        "      memory_indices0 = np.zeros(visible_indices[-1]+100, dtype=int)\n",
        "      memory_indices0[visible_indices] = 100\n",
        "\n",
        "      if len(ff_catched_T_sorted)-1 >= i:\n",
        "        # Find the index of the time at which the ff is captured\n",
        "        last_live_time_array = np.where(monkey_t_array <= ff_catched_T_sorted[i])[0]\n",
        "        if len(last_live_time_array) > 0:\n",
        "          last_live_time = last_live_time_array[-1]\n",
        "        # Truncate memory_indices0 based on that \n",
        "          memory_indices0 = memory_indices0[:last_live_time+1]\n",
        "\n",
        "      \n",
        "      # Iterate through memory_indices0 to make a new list to denote memory (replacing some 0s with other numbers based on time)\n",
        "      # We preserve the first element of memory_indices0. We also separate memory_indices and point_indices (denoted as final_indices)\n",
        "      memory_indices = [memory_indices0[0]]\n",
        "\n",
        "      for k in range(1, len(memory_indices0)):\n",
        "        if memory_indices0[k] == 0:\n",
        "          memory_indices.append(memory_indices[k-1]-1)\n",
        "        else: # Else, preserve the current value\n",
        "          memory_indices.append(memory_indices0[k])\n",
        "      memory_indices_array = np.array(memory_indices)\n",
        "\n",
        "      index_array0 = np.arange(len(memory_indices0))\n",
        "      if len(index_array0) > len(monkey_t_array):\n",
        "        max_index = len(monkey_t_array)\n",
        "        index_array0 = index_array0[:max_index]\n",
        "        memory_indices_array = memory_indices_array[index_array0]\n",
        "\n",
        "      in_memory_indices = np.where(memory_indices_array > 0)[0]\n",
        "      memory_indices_array = memory_indices_array[in_memory_indices]\n",
        "      index_array = index_array0[in_memory_indices]\n",
        "      in_memory_length = len(memory_indices_array)\n",
        "      memory_indices = memory_indices_array.tolist()\n",
        "      final_indices = index_array.tolist()\n",
        "\n",
        "      ff_index = ff_index + [i]*in_memory_length\n",
        "      point_index = point_index+ [point + num_missed_index for point in final_indices]\n",
        "      relevant_time = monkey_t_array[index_array]\n",
        "      time = time+relevant_time.tolist()\n",
        "      target_index = target_index + np.digitize(relevant_time, ff_catched_T_sorted).tolist()\n",
        "      ff_x = ff_x + [(ff_real_position_sorted[i][0])]*in_memory_length\n",
        "      ff_y = ff_y + [(ff_real_position_sorted[i][1])]*in_memory_length\n",
        "      monkey_x = monkey_x + monkey_x_array[index_array].tolist()\n",
        "      monkey_y = monkey_y + monkey_y_array[index_array].tolist()\n",
        "      visible = visible + [1 if point ==100 else 0 for point in memory_indices]\n",
        "      memory = memory + memory_indices\n",
        "      monkey_xy_relevant = np.stack([monkey_x_array[index_array], monkey_y_array[index_array]],axis=1)\n",
        "      monkey_angle_relevant = monkey_angle_array[index_array]\n",
        "      ff_distance_relevant = LA.norm(monkey_xy_relevant-ff_real_position_sorted[i], axis=1)\n",
        "      ff_distance = ff_distance + ff_distance_relevant.tolist()\n",
        "      angles_to_monkey = np.arctan2(ff_real_position_sorted[i,1]-monkey_xy_relevant[:,1], ff_real_position_sorted[i,0]-monkey_xy_relevant[:,0]) - monkey_angle_relevant\n",
        "      angles_to_monkey[angles_to_monkey > pi] = angles_to_monkey[angles_to_monkey > pi] - 2*pi\n",
        "      angles_to_monkey[angles_to_monkey < -pi] = angles_to_monkey[angles_to_monkey < -pi] + 2*pi\n",
        "      angles_adjusted = np.absolute(angles_to_monkey)-np.abs(np.arcsin(np.divide(reward_boundary, np.maximum(ff_distance_relevant, reward_boundary) ))) # use torch clip to get valid arcsin input\n",
        "      angles_adjusted = np.clip(angles_adjusted, 0, pi)\n",
        "      angles_adjusted = np.sign(angles_to_monkey)* angles_adjusted\n",
        "      ff_angle = ff_angle + angles_to_monkey.tolist()\n",
        "      ff_angle_boundary = ff_angle_boundary + angles_adjusted.tolist()\n",
        "      left_right = left_right + (np.array(angles_to_monkey) > 0).astype(int).tolist()\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "  # Now let's create a dictionary of the lists\n",
        "  ff_dict = {'ff_index':ff_index, 'point_index':point_index, 'time':time, 'target_index':target_index,\n",
        "              'ff_x':ff_x, 'ff_y':ff_y, 'monkey_x':monkey_x, 'monkey_y':monkey_y, 'visible':visible,\n",
        "              'memory':memory, 'ff_distance':ff_distance, 'ff_angle':ff_angle, 'ff_angle_boundary': ff_angle_boundary, 'left_right':left_right}\n",
        "  ff_dataframe = pd.DataFrame(ff_dict)\n",
        "\n",
        "  filepath = 'gdrive/MyDrive/fireflies_data/' + data_folder_name + '/ff_dataframe.csv'\n",
        "  os.makedirs('gdrive/MyDrive/fireflies_data/' + data_folder_name, exist_ok = True)\n",
        "  ff_dataframe.to_csv(filepath) \n",
        "\n",
        "  ff_dataframe['target_x'] = ff_real_position_sorted[np.array(ff_dataframe['target_index'])][:,0]\n",
        "  ff_dataframe['target_y'] = ff_real_position_sorted[np.array(ff_dataframe['target_index'])][:,1]\n",
        "  ff_dataframe['ffdistance2target'] = LA.norm(np.array(ff_dataframe[['ff_x', 'ff_y']])-np.array(ff_dataframe[['target_x', 'target_y']]), axis = 1)\n",
        "\n",
        "  #ff_dataframe = ff_dataframe[ff_dataframe['time'] < ff_catched_T_sorted[-200]]\n",
        "\n",
        "  min_point_index = np.min(np.array(ff_dataframe['point_index']))\n",
        "  max_point_index = np.max(np.array(ff_dataframe['point_index']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6TdojHoU2zU"
      },
      "source": [
        "# Categorize all trials\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEqcdidEVCtq"
      },
      "source": [
        "## Capture more than 1 in a cluster\n",
        "\n",
        "A change: instead of using ff_real_position_sorted, I now use ff_believed_position_sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWRaVamlVCtr"
      },
      "outputs": [],
      "source": [
        "n_ff_in_a_row = [1]\n",
        "distance_between_ff = 50\n",
        "count = 1\n",
        "# n_ff_in_a_row[k] will denote the number of ff that the monkey has captured in a row at trial k\n",
        "for i in range(1, catched_ff_num):\n",
        "  if LA.norm(ff_believed_position_sorted[i]-ff_believed_position_sorted[i-1]) < distance_between_ff:\n",
        "    count += 1\n",
        "  else:\n",
        "    count = 1\n",
        "  n_ff_in_a_row.append(count)\n",
        "\n",
        "\n",
        "n_ff_in_a_row = np.array(n_ff_in_a_row)\n",
        "\n",
        "\n",
        "two_in_a_row= np.where(n_ff_in_a_row==2)[0]\n",
        "dif_time = ff_catched_T_sorted[two_in_a_row] - ff_catched_T_sorted[two_in_a_row-1]\n",
        "two_in_a_row_simul = two_in_a_row[np.where(dif_time <= 0.1)[0]]\n",
        "two_in_a_row_non_simul = two_in_a_row[np.where(dif_time > 0.1)[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ptf9II2Ew47"
      },
      "outputs": [],
      "source": [
        "two_in_a_row_non_simul"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEgwYZy-VE8s"
      },
      "source": [
        "## on before last one\n",
        "\n",
        "In the future, a sub-category can be created where the target is far from any other ff that has been visible for a while"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMYMpC0nVE8t"
      },
      "outputs": [],
      "source": [
        "on_before_last_one_trials = [] # Note, the formula has been changed from the previous stages\n",
        "for i in range(1, catched_ff_num):\n",
        "  # Evaluate whether the last flash of the current ff finishes before the capture of the previous ff\n",
        "  if ff_flash_end_sorted[i] < ff_catched_T_sorted[i-1]:\n",
        "    # Then we consider special situations\n",
        "    # Make sure that the index is not out of bound\n",
        "    if i > 1: \n",
        "      # If the monkey captures 2 fireflies at the same time, then the trial does not count as \"on_before_last_one\"\n",
        "      if ff_catched_T_sorted[i] == ff_catched_T_sorted[i-1]:\n",
        "        continue\n",
        "    # Append the index into the list\n",
        "    on_before_last_one_trials.append(i)\n",
        "\n",
        "on_before_last_one_trials = np.array(on_before_last_one_trials)\n",
        "\n",
        "\n",
        "dif_time2 = ff_catched_T_sorted[on_before_last_one_trials] - ff_catched_T_sorted[on_before_last_one_trials-1]\n",
        "on_before_last_one_simul = on_before_last_one_trials[np.where(dif_time2 <= 0.1)[0]]\n",
        "on_before_last_one_non_simul = on_before_last_one_trials[np.where(dif_time2 > 0.1)[0]]\n",
        "print(len(on_before_last_one_non_simul))\n",
        "len(on_before_last_one_simul)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L89GUbTkeRKY"
      },
      "source": [
        "## visible before last one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6YtMVFeeQr8"
      },
      "outputs": [],
      "source": [
        "temp_dataframe = ff_dataframe[(ff_dataframe['target_index']==ff_dataframe['ff_index']) & (ff_dataframe['visible'] == 1)]\n",
        "trials_not_to_select = np.unique(np.array(temp_dataframe['target_index']))\n",
        "all_trials = np.unique(np.array(ff_dataframe['target_index']))\n",
        "visible_before_last_one_trials = np.setdiff1d(all_trials, trials_not_to_select)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucd_xX0puAgO"
      },
      "source": [
        "## used clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DTV5buEuFCF"
      },
      "outputs": [],
      "source": [
        "used_cluster = np.intersect1d(two_in_a_row_non_simul, on_before_last_one_trials)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GG3EL7XVH0T"
      },
      "source": [
        "## disappear_latest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hC0EIxUuVH0U"
      },
      "outputs": [],
      "source": [
        "#By trial\n",
        "ff_dataframe_visible = ff_dataframe[(ff_dataframe['visible']==1)]\n",
        "# For each trial, find out the point index where the monkey last sees a ff\n",
        "last_visible_index = ff_dataframe_visible[['point_index','target_index']].groupby('target_index').max()\n",
        "# Take out all the rows corresponding to these points\n",
        "last_visible_ffs = pd.merge(last_visible_index, ff_dataframe_visible, how=\"left\")\n",
        "# Select trials where the target disappears the latest\n",
        "disappear_latest_trials = np.array(last_visible_ffs[last_visible_ffs['target_index']==last_visible_ffs['ff_index']]['target_index'])\n",
        "not_disappear_latest_trials = np.unique(np.array(last_visible_ffs[last_visible_ffs['target_index']!=last_visible_ffs['ff_index']]['target_index']))\n",
        "'''\n",
        "#By points (problem: the time for annotation is too short, and can be distracting)\n",
        "## When did the targets start to be visible for the last time?\n",
        "# For each trial, find the rows from ff_dataframe about the target\n",
        "disappear_latest_points = []\n",
        "for i in disappear_latest_trials:\n",
        "  rows = ff_dataframe[ff_dataframe['ff_index'] == i]\n",
        "  # Find the index of the point when the target changes from invisible to visible. Keep the last point of such change.\n",
        "  # If not such point exists, then just keep the first point\n",
        "  turning_points = np.where(np.ediff1d(np.array(rows['visible'])) == 1)[0]\n",
        "  if len(turning_points) > 0:\n",
        "    starting_index = turning_points[-1]+1\n",
        "    disappear_latest_points = disappear_latest_points + list(range(np.array(rows['point_index'])[starting_index], np.array(rows['point_index'])[-1]+1))\n",
        "# Subtract 50 points from all points\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(ff_dataframe['ff_distance']<250)"
      ],
      "metadata": {
        "id": "dvp5ugnU85S8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(disappear_latest_trials)"
      ],
      "metadata": {
        "id": "rYc5UUry86Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-Uf0qN8VKq7"
      },
      "source": [
        "## Cluster around targets\n",
        "\n",
        "The method is changed from the previous stage so that the visibility of ffs (distance & angle) is also considered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMhJTeE1MY6v"
      },
      "outputs": [],
      "source": [
        "max_time_apart = 1.25\n",
        "# See if the target is close to any ff that has been visible in the past 5s\n",
        "ffs_around_target = []\n",
        "ffs_around_target_indices = []\n",
        "ffs_around_target_positions = [] # Stores the positions of the ffs around the target\n",
        "temp_frame = ff_dataframe[['ff_index', 'target_index', 'ff_distance', 'visible', 'time']]\n",
        "for i in range(catched_ff_num):\n",
        "  time = ff_catched_T_sorted[i]\n",
        "  duration = [time-max_time_apart, time+max_time_apart]\n",
        "  target_nums = np.arange(i-1, i+2)\n",
        "  temp_frame2 = temp_frame[(temp_frame['time']>duration[0])&(temp_frame['time']<duration[1])]\n",
        "  #temp_frame2 = temp_frame[temp_frame['target_index'].isin(target_nums)]\n",
        "  temp_frame2 = temp_frame2[~temp_frame2['ff_index'].isin(target_nums)]\n",
        "  temp_frame2 = temp_frame2[(temp_frame2['visible'] ==1)]\n",
        "  temp_frame2 = temp_frame2[temp_frame2['ff_distance'] < 250]\n",
        "  past_visible_ff_indices = np.unique(np.array(temp_frame2.ff_index))\n",
        "  # Get positions of these ffs\n",
        "  past_visible_ff_positions = ff_real_position_sorted[past_visible_ff_indices]\n",
        "  # See if any one of it is within 50 cm of the target\n",
        "  distance2target = LA.norm(past_visible_ff_positions - ff_real_position_sorted[i], axis=1)\n",
        "  close_ff_indices = np.where(distance2target < 50)[0]\n",
        "  num_ff = len(close_ff_indices)\n",
        "  ffs_around_target.append(num_ff)\n",
        "  if len(close_ff_indices) > 0:\n",
        "    ffs_around_target_positions.append(past_visible_ff_positions[close_ff_indices])\n",
        "    ffs_around_target_indices.append(past_visible_ff_indices[close_ff_indices])\n",
        "  else:\n",
        "    ffs_around_target_positions.append(np.array([]))\n",
        "    ffs_around_target_indices.append(np.array([]))\n",
        "ffs_around_target = np.array(ffs_around_target)\n",
        "ffs_around_target_trials = np.where(ffs_around_target > 0)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXpJOfgcVPSB"
      },
      "source": [
        "## Waste cluster around last target\n",
        "\n",
        "The method is changed from the previous stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHvnHrpqVPSC"
      },
      "outputs": [],
      "source": [
        "# And then look at \"capture # ffs in a row\"\n",
        "# If in the previous trial, the target has at least one ff in the vicinity, and yet the current trial is not categorized as capturing more than 1 ff in a row. \n",
        "# Then we categorize such a trial as \"waste cluster of last target\"\n",
        "# This means that while the previous target is in a cluster, the monkey didn't capture the ffs in its vicinity.\n",
        "\n",
        "waste_cluster_last_target_trials = np.intersect1d(ffs_around_target_trials+1, np.where(n_ff_in_a_row == 1)[0] )\n",
        "# Note, if one wants to see the target around which there is a cluster, use waste_cluster_last_target_trials-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNfDjMxLVYLV"
      },
      "source": [
        "## Clusters of ffs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Db4cIyir-rCJ"
      },
      "outputs": [],
      "source": [
        "# temp_dataframe1 = pd.DataFrame(point_vs_cluster, columns=['point_index', 'ff_index', 'cluster_label'])\n",
        "# u, c = np.unique(point_vs_cluster[:,0], return_counts=True)\n",
        "# temp_dataframe2 = pd.DataFrame(np.concatenate([u.reshape(-1,1), c.reshape(-1,1)], axis=1), columns = ['point_index','num_ff_at_point'])\n",
        "# temp_dataframe3 = temp_dataframe1.merge(temp_dataframe2, how = \"left\", on = \"point_index\")\n",
        "# corresponding_t =  monkey_information['monkey_t'][np.array(temp_dataframe3['point_index'])]\n",
        "# temp_dataframe3['time'] = corresponding_t \n",
        "# temp_dataframe3['target_index'] = np.digitize(corresponding_t, ff_catched_T_sorted)\n",
        "# temp_dataframe3 = temp_dataframe3[temp_dataframe3['target_index']<len(ff_catched_T_sorted)]\n",
        "# cluster_dataframe_point = temp_dataframe3\n",
        "# cluster_dataframe_trial = cluster_dataframe_point[['target_index', 'num_ff_at_point']].groupby('target_index', as_index=True).agg({'num_ff_at_point':['max', 'count']})\n",
        "# cluster_dataframe_trial.columns = [\"max_ff_in_cluster\", \"num_points_w_cluster\"]\n",
        "# cluster_exist_trials = cluster_dataframe_point.target_index.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsseZjllVdVE"
      },
      "source": [
        "## Waste cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_34kRGtgVdVF"
      },
      "outputs": [],
      "source": [
        "# # If in the trial before the previous trial, there has been at least one cluster, but the monkey does not capture two ffs in a row\n",
        "# cluster2 = cluster_exist_trials+1\n",
        "# capture2 = np.where(n_ff_in_a_row == 1)[0]\n",
        "# waste_cluster_trials = np.intersect1d(capture2, cluster2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_dwcy8XV2Bd"
      },
      "source": [
        "## ignore sudden flash\n",
        "\n",
        "Not choose a closeby ff (closer than the target) that suddenly becomes visible"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CPIwVQRv6Jn"
      },
      "source": [
        "One change I made: Before, I selected \"The indices and trials of ff_dataframe where the suddenly visible ff is the target.\" Now I select \"The indices and trials of ff_dataframe where the suddenly visible ff is the target or next target\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wm10bds5CFIj"
      },
      "outputs": [],
      "source": [
        "from math import pi\n",
        "df_ffdistance = np.array(ff_dataframe['ff_distance'])\n",
        "max_ff_index = max(np.array(ff_dataframe['ff_index']))\n",
        "\n",
        "\n",
        "\n",
        "# These are the indices of points where a ff changes from being invisible to become visible\n",
        "start_index1 = np.where(np.ediff1d(np.array(ff_dataframe['visible'])) == 1)[0]+1\n",
        "# These are the indices of points where the ff_index has changed\n",
        "start_index2 = np.where(np.ediff1d(np.array(ff_dataframe['ff_index']))!= 0)[0]+1\n",
        "# Combine the two\n",
        "start_index3 = np.concatenate((start_index1, start_index2))\n",
        "start_index = np.unique(start_index3)\n",
        "\n",
        "# Find the indices of ff_dataframe where the monkey encounters a closeby ff that suddenly becomes visible\n",
        "suddent_flash_index = start_index[np.where(df_ffdistance[start_index] < 50)]\n",
        "\n",
        "# The indices and trials of ff_dataframe where the suddenly visible ff is the target or next target\n",
        "#sudden_flash_capture = suddent_flash_index[np.where(np.array(ff_dataframe['ff_index'])[suddent_flash_index] == np.array(ff_dataframe['target_index'])[suddent_flash_index])[0]]\n",
        "condition = np.logical_or((np.array(ff_dataframe['ff_index'])[suddent_flash_index] == np.array(ff_dataframe['target_index'])[suddent_flash_index]), (np.array(ff_dataframe['ff_index'])[suddent_flash_index] == np.array(ff_dataframe['target_index']+1)[suddent_flash_index]))\n",
        "sudden_flash_capture = suddent_flash_index[condition]\n",
        "sudden_flash_capture_trials = np.array(ff_dataframe['target_index'])[sudden_flash_capture]\n",
        "sudden_flash_capture_trials = np.unique(sudden_flash_capture_trials)\n",
        "\n",
        "# The indices and trials of ff_dataframe where the suddenly visible ff is not the target\n",
        "#sudden_flash_ignore = suddent_flash_index[np.where(np.array(ff_dataframe['ff_index'])[suddent_flash_index] != np.array(ff_dataframe['target_index'])[suddent_flash_index])[0]]\n",
        "sudden_flash_ignore = suddent_flash_index[~condition]\n",
        "\n",
        "# Find the distance to targets at these points\n",
        "cum_x = np.array(ff_dataframe.monkey_x)[sudden_flash_ignore]\n",
        "cum_y = np.array(ff_dataframe.monkey_y)[sudden_flash_ignore]\n",
        "cum_target_indices = np.array(ff_dataframe.target_index)[sudden_flash_ignore]\n",
        "cum_target_distances = LA.norm(np.stack([cum_x, cum_y], axis=1)-ff_real_position_sorted[cum_target_indices], axis=1)\n",
        "cum_ff_distances = df_ffdistance[sudden_flash_ignore]\n",
        "valid_indices = np.where(cum_target_distances > cum_ff_distances)\n",
        "sudden_flash_ignore_trials_non_unique = cum_target_indices[valid_indices]\n",
        "sudden_flash_ignore_trials = np.unique(sudden_flash_ignore_trials_non_unique)\n",
        "\n",
        "\n",
        "## By points\n",
        "# Find the points corresponding to such a sudden flash\n",
        "sudden_flash_ignore_indices = np.array(ff_dataframe['point_index'])[sudden_flash_ignore[valid_indices]]\n",
        "sudden_flash_ignore_ff = np.array(ff_dataframe['ff_index'])[sudden_flash_ignore[valid_indices]]\n",
        "# Append each point into a list and the following n points so that the message can be visible for 2 seconds\n",
        "sudden_flash_ignore_points = []\n",
        "for i in sudden_flash_ignore_indices:\n",
        "  sudden_flash_ignore_points = sudden_flash_ignore_points+list(range(i, i+121))\n",
        "\n",
        "sudden_flash_ignore_time = monkey_information['monkey_t'][sudden_flash_ignore_indices]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbyGXa5Eys8O"
      },
      "outputs": [],
      "source": [
        "len(sudden_flash_ignore_trials)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAhtJERBV5Cc"
      },
      "source": [
        "## try a few times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iz8Qq8qjV5Ce"
      },
      "outputs": [],
      "source": [
        "# Show the trials where the last cluster has more than 2 stops\n",
        "try_a_few_times_trials = []\n",
        "try_a_few_times_indices = []\n",
        "try_a_few_times_info = [] # for each incident, I'll get [trial, min_stop_index_overall, max_stop_index_overall]\n",
        "for i in range(catched_ff_num): \n",
        "  # Find clusters based on a distance of 50\n",
        "  clusters = find_clusters(i,50)\n",
        "  # if clusters is not empty:\n",
        "  if len(clusters) > 0:\n",
        "    num_for_last_cluster = clusters[-1]\n",
        "    # If the last cluster has more than 2 stops\n",
        "    if clusters.count(num_for_last_cluster) > 1:\n",
        "      distinct_stops = num_of_stops(i)\n",
        "      distinct_stops_indices = num_of_stops_indices(i)\n",
        "      # If the last stop is close enough to the believed position of the target\n",
        "      if LA.norm(distinct_stops[-1]-ff_believed_position_sorted[i]) < 50:\n",
        "        try_a_few_times_trials.append(i)\n",
        "# By points \n",
        "        min_index = distinct_stops_indices[-clusters.count(num_for_last_cluster)]\n",
        "        max_index = distinct_stops_indices[-1]\n",
        "        try_a_few_times_indices = try_a_few_times_indices + \\\n",
        "        list(range(min_index-20, min(max_index+20, max_point_index)))\n",
        "        try_a_few_times_info.append([i, min_index, max_index])\n",
        "try_a_few_times_info = np.array(try_a_few_times_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtnAiAPUV-sk"
      },
      "source": [
        "## give up after trying"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nGJCfqIV-sm"
      },
      "outputs": [],
      "source": [
        "# By trials\n",
        "# Show the trials where there at least one cluster has more than 2 stops, and this cluster is neither at the beginning nor at the end\n",
        "give_up_after_trying_trials = []\n",
        "for i in range(catched_ff_num):\n",
        "  # Find clusters based on a distance of 50\n",
        "  clusters = find_clusters(i,50)\n",
        "  # if clusters is not empty:\n",
        "  if len(clusters) > 0:\n",
        "    clusters_counts = collections.Counter(clusters)\n",
        "    distinct_stop_positions = num_of_stops(i)\n",
        "    for k in range(1, clusters[-1]+1):  # for each cluster\n",
        "      # if it has more than one element:\n",
        "      if clusters_counts[k] > 1:\n",
        "        # Get positions of these points\n",
        "        stop_positions = [distinct_stop_positions[index] for index, value in enumerate(clusters) if value == k]\n",
        "        # If the first stop is not close to beginning, and the last stop is not too close to the end:\n",
        "        if LA.norm(stop_positions[0]-ff_believed_position_sorted[i-1]) > 50 and LA.norm(stop_positions[-1]-ff_believed_position_sorted[i]) > 50:\n",
        "          give_up_after_trying_trials.append(i) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hh9NZSasGkmL"
      },
      "outputs": [],
      "source": [
        "# By points\n",
        "\n",
        "# Show the trials where there at least one cluster has more than 2 stops, and this cluster is neither at the beginning nor at the end\n",
        "\n",
        "give_up_after_trying_indices = []\n",
        "give_up_after_trying_info = [] # for each incident, I'll get [trial, min_stop_index_overall, max_stop_index_overall]\n",
        "for i in range(catched_ff_num):\n",
        "  # Find clusters based on a distance of 50\n",
        "  clusters = find_clusters(i,50)\n",
        "  # if clusters is not empty:\n",
        "  if len(clusters) > 0:\n",
        "    clusters_counts = collections.Counter(clusters)\n",
        "    distinct_stop_positions = num_of_stops(i)\n",
        "    distinct_stops_indices = num_of_stops_indices(i)\n",
        "    for k in range(1, clusters[-1]+1):  # for each cluster\n",
        "      # if it has more than one element:\n",
        "      if clusters_counts[k] > 1:\n",
        "        # Get positions of these points\n",
        "        stop_positions = [distinct_stop_positions[index] for index, value in enumerate(clusters) if value == k]\n",
        "        # If the first stop is not close to beginning, and the last stop is not too close to the end:\n",
        "        if LA.norm(stop_positions[0]-ff_believed_position_sorted[i-1]) > 50 and LA.norm(stop_positions[-1]-ff_believed_position_sorted[i]) > 50:\n",
        "          # Get indices of these points\n",
        "          stop_positions_indices = [distinct_stops_indices[index] for index, value in enumerate(clusters) if value == k]\n",
        "          give_up_after_trying_indices = give_up_after_trying_indices + list(range(min(stop_positions_indices)-20, max(stop_positions_indices)+21)) \n",
        "          give_up_after_trying_info.append([i, min(stop_positions_indices), max(stop_positions_indices)])\n",
        "give_up_after_trying_info = np.array(give_up_after_trying_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0T8vdHdZZiB"
      },
      "source": [
        "# trials_char\n",
        "\n",
        "Problem: for all the last_visible...I should consider the cluster that the target is in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMA-LGYTZZiK"
      },
      "outputs": [],
      "source": [
        "relevant_df0 = ff_dataframe[ff_dataframe['visible'] == 1]\n",
        "last_trial = catched_ff_num-200\n",
        "trial_array = [i for i in range(1, last_trial+1)]\n",
        "# Trial number is named after the index of the target. \n",
        "# Trial number starts at 1\n",
        "\n",
        "t_array = ff_catched_T_sorted[1:last_trial+1] - ff_catched_T_sorted[:last_trial]\n",
        "t_array = t_array.tolist()\n",
        "\n",
        "## How long can the monkey remember a target?\n",
        "## time elapses between the target last being visible and its capture\n",
        "t_last_visible = []\n",
        "d_last_visible = []\n",
        "abs_angle_last_visible = []\n",
        "for i in range(1, last_trial+1):\n",
        "  relevant_df = relevant_df0[((relevant_df0['target_index']==i) & (relevant_df0['ffdistance2target']<25))| (relevant_df0['ff_index']==i)]\n",
        "  if len(relevant_df) > 0:\n",
        "    t_last_visible.append(ff_catched_T_sorted[i] - max(np.array(relevant_df.time)))\n",
        "    d_last_visible.append(max(np.array(relevant_df.ff_distance)))\n",
        "    abs_angle_last_visible.append(max(np.absolute(np.array(relevant_df.ff_angle_boundary))))\n",
        "  else:\n",
        "    t_last_visible.append(9999)\n",
        "    d_last_visible.append(9999)\n",
        "    abs_angle_last_visible.append(9999)\n",
        "\n",
        "\n",
        "hit_boundary = []\n",
        "for i in range(1, last_trial+1):\n",
        "  duration = [ff_catched_T_sorted[i-1], ff_catched_T_sorted[i]]\n",
        "  cum_indices = np.where((monkey_information['monkey_t'] >= duration[0]) & (monkey_information['monkey_t'] <= duration[1]))[0]\n",
        "  if len(cum_indices) > 1:\n",
        "    cum_t, cum_angle = monkey_information['monkey_t'][cum_indices],  monkey_information['monkey_angle'][cum_indices]\n",
        "    cum_mx, cum_my = monkey_information['monkey_x'][cum_indices], monkey_information['monkey_y'][cum_indices] \n",
        "    if np.any(cum_mx[1:]-cum_mx[:-1] > 55) or np.any(cum_my[1:]-cum_my[:-1] > 55):\n",
        "      hit_boundary.append(False)\n",
        "    else:\n",
        "      hit_boundary.append(False)\n",
        "  else:\n",
        "    hit_boundary.append(False)\n",
        "\n",
        "# num_stops\n",
        "num_stops_array = [len(num_of_stops(i)) for i in range(1, last_trial+1)]\n",
        "\n",
        "num_stops_since_last_seen = [len(num_of_stops_since_target_last_seen(i)) for i in range(1, last_trial+1)]\n",
        "\n",
        "\n",
        "num_stops_near_target = []\n",
        "for i in range(1, last_trial+1):\n",
        "  clusters = find_clusters(i,50)\n",
        "  # for each trial, append the trial number and the number of stop in the last cluster, if the cluster is close enough to the target\n",
        "  distinct_stops = num_of_stops(i)\n",
        "  if len(distinct_stops) > 0:\n",
        "    # If the last stop is close enough to the believed position of the target\n",
        "    if LA.norm(distinct_stops[-1]-ff_believed_position_sorted[i]) < 50:\n",
        "      num_stops_near_target.append(clusters.count(clusters[-1]))\n",
        "    else: \n",
        "      num_stops_near_target.append(0)\n",
        "  else: \n",
        "    num_stops_near_target.append(0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# trials_char\n",
        "trials_dict = {'trial':trial_array, \n",
        "               't':t_array,  \n",
        "               't_last_visible':t_last_visible,\n",
        "               'd_last_visible':d_last_visible,\n",
        "               'abs_angle_last_visible': abs_angle_last_visible,\n",
        "               'hit_boundary': hit_boundary,  \n",
        "               'num_stops': num_stops_array, \n",
        "               'num_stops_near_target': num_stops_near_target                            \n",
        "                      }\n",
        "trials_char = pd.DataFrame(trials_dict)\n",
        "\n",
        "trials_char['n_ff_in_a_row'] = n_ff_in_a_row[:len(trials_char)]\n",
        "\n",
        "\n",
        "valid_trials = trials_char[(trials_char['t_last_visible']<50) & (trials_char['hit_boundary']==False)].reset_index()\n",
        "median_values = valid_trials.median(axis=0)\n",
        "\n",
        "\n",
        "len(valid_trials)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjftM0YCur9f"
      },
      "outputs": [],
      "source": [
        "data_folder_name = \"LSTM_July25\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KAAhMtWul_x"
      },
      "outputs": [],
      "source": [
        "# # # save\n",
        "# data_folder_name = '0219'\n",
        "# filepath = 'gdrive/MyDrive/fireflies_data/' + data_folder_name + '/trials_char.csv'\n",
        "# os.makedirs('gdrive/MyDrive/fireflies_data/' + data_folder_name, exist_ok = True)\n",
        "# trials_char.to_csv(filepath) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lb2MmiXbzTeF"
      },
      "outputs": [],
      "source": [
        "# # # save\n",
        "# data_folder_name = 'LSTM_July_26_2'\n",
        "# filepath = 'gdrive/MyDrive/fireflies_data/' + data_folder_name + '/trials_char.csv'\n",
        "# os.makedirs('gdrive/MyDrive/fireflies_data/' + data_folder_name, exist_ok = True)\n",
        "# trials_char.to_csv(filepath) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLjoJT99vRC1"
      },
      "outputs": [],
      "source": [
        "# retrieve\n",
        "filepath = 'gdrive/MyDrive/fireflies_data/' + '0219' + '/trials_char.csv'\n",
        "#filepath = 'gdrive/MyDrive/fireflies_data/' + data_folder_name + '/trials_char.csv'\n",
        "trials_char_m = pd.read_csv(filepath)\n",
        "valid_trials_m = trials_char_m[(trials_char_m['t_last_visible']<50) & (trials_char_m['hit_boundary']==False)].reset_index()\n",
        "median_values_m = valid_trials_m.median(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scZlSCHn59IG"
      },
      "outputs": [],
      "source": [
        "filepath = 'gdrive/MyDrive/fireflies_data/' + 'LSTM_July_26_2' + '/trials_char.csv'\n",
        "#filepath = 'gdrive/MyDrive/fireflies_data/' + data_folder_name + '/trials_char.csv'\n",
        "trials_char_lstm = pd.read_csv(filepath)\n",
        "valid_trials_lstm = trials_char_lstm[(trials_char_lstm['t_last_visible']<50) & (trials_char_lstm['hit_boundary']==False)].reset_index()\n",
        "median_values_lstm = valid_trials_lstm.median(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqwZe02U1fdT"
      },
      "outputs": [],
      "source": [
        "trials_char"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-1E6QI0avwK"
      },
      "source": [
        "# Stats dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQtV60Qi3R8a"
      },
      "source": [
        "## agent (new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-REA2ttobYjs"
      },
      "outputs": [],
      "source": [
        "max_time = 2000\n",
        "catched_T_bounded = ff_catched_T_sorted[np.where(ff_catched_T_sorted < max_time)[0]]\n",
        "max_trial = 650\n",
        "num_stops_array = [len(num_of_stops(i)) for i in range(1, max_trial+1)]\n",
        "stop_success_rate = max_trial/sum(num_stops_array)\n",
        "\n",
        "temp_dataframe = ff_dataframe[(ff_dataframe['target_index']==ff_dataframe['ff_index']) & (ff_dataframe['visible'] == 1)]\n",
        "trials_not_to_select = np.unique(np.array(temp_dataframe['target_index']))\n",
        "all_trials = np.unique(np.array(ff_dataframe['target_index']))\n",
        "visible_before_last_one_trials = np.setdiff1d(all_trials, trials_not_to_select)\n",
        "\n",
        "n_trial = max_trial\n",
        "n_points = np.where(monkey_information['monkey_t'] <= ff_catched_T_sorted[n_trial])[0][-1]\n",
        "\n",
        "stats_dict = {\n",
        "\"Two in a row\" : len(np.where(n_ff_in_a_row[:n_trial]>=2)[0])/(n_trial-2),\n",
        "\"Visible before last capture\" : len(np.where(visible_before_last_one_trials < n_trial)[0])/(n_trial-2),\n",
        "\"Target disappears latest\" : len(np.where(disappear_latest_trials < n_trial)[0])/(n_trial-1),\n",
        "\"Waste cluster around last target\": len(np.where(waste_cluster_last_target_trials < n_trial)[0])/(n_trial-2),\n",
        "\"Ignore sudden flash\": len(np.where(sudden_flash_ignore_trials < n_trial)[0])/(n_trial-1),\n",
        "\"Try a few times\": len(np.where(np.array(try_a_few_times_trials) < n_trial)[0])/(n_trial-1),\n",
        "\"Give up after trying\": len(np.where(np.array(give_up_after_trying_trials) < n_trial)[0])/(n_trial-1),\n",
        "\"ff capture rate\": (len(catched_T_bounded)-1)/(catched_T_bounded[-1]-catched_T_bounded[0]),\n",
        "\"Stop success rate\": max_trial/sum(num_stops_array),\n",
        "}\n",
        "stats_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSjRw9u8T6i5"
      },
      "outputs": [],
      "source": [
        "n_points = np.where(monkey_information['monkey_t'] <= ff_catched_T_sorted[n_trial])[0][-1]\n",
        "\n",
        "stats_dict_all = {\n",
        "\"Two in a row\" : len(np.where(n_ff_in_a_row[:n_trial]>=2)[0])/(n_trial-2),\n",
        "\"Visible before last capture\" : len(np.where(visible_before_last_one_trials < n_trial)[0])/(n_trial-2),\n",
        "\"Target disappears latest\" : len(np.where(disappear_latest_trials < n_trial)[0])/(n_trial-1),\n",
        "\"Waste cluster around last target\": len(np.where(waste_cluster_last_target_trials < n_trial)[0])/(n_trial-2),\n",
        "\"Ignore sudden flash\": len(np.where(sudden_flash_ignore_trials < n_trial)[0])/(n_trial-1),\n",
        "\"Try a few times\": len(np.where(np.array(try_a_few_times_trials) < n_trial)[0])/(n_trial-1),\n",
        "\"Give up after trying\": len(np.where(np.array(give_up_after_trying_trials) < n_trial)[0])/(n_trial-1),\n",
        "\"ff capture rate\": (len(catched_T_bounded)-1)/(catched_T_bounded[-1]-catched_T_bounded[0]),\n",
        "\"Stop success rate\": max_trial/sum(num_stops_array),\n",
        "\"Median time\": median_values['t'],\n",
        "\"Median time target last seen\": median_values['t_last_visible'],\n",
        "#\"Median distance target last seen\": median_values['d_last_visible'],\n",
        "\"Median abs angle target last seen \": median_values['abs_angle_last_visible'],\n",
        "\"Median num stops\": median_values['num_stops'],\n",
        "\"Median num stops near target\": median_values['num_stops_near_target'],\n",
        "}\n",
        "stats_dict_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIs9-R6Jjrl7"
      },
      "outputs": [],
      "source": [
        "stats_dict_median = {\"Median time\": median_values['t'],\n",
        "\"Median time target last seen\": median_values['t_last_visible'],\n",
        "#\"Median distance target last seen\": median_values['d_last_visible'],\n",
        "\"Median abs angle target last seen \": median_values['abs_angle_last_visible'],\n",
        "\"Median num stops\": median_values['num_stops'],\n",
        "\"Median num stops near target\": median_values['num_stops_near_target'],\n",
        "}\n",
        "stats_dict_median"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## agent (stored)"
      ],
      "metadata": {
        "id": "jprRAPoCvho4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# c10.2: dt = 0.25, noise = 1, memory = 3\n",
        "stats_dict = {'Give up after trying': 0.20955315870570107,\n",
        " 'Ignore sudden flash': 0.032357473035439135,\n",
        " 'Stop success rate': 0.3551912568306011,\n",
        " 'Target disappears latest': 0.5978428351309707,\n",
        " 'Try a few times': 0.10015408320493066,\n",
        " 'Two in a row': 0.17746913580246915,\n",
        " 'Visible before last capture': 0.1111111111111111,\n",
        " 'Waste cluster around last target': 0.08333333333333333,\n",
        " 'ff capture rate': 0.3093119438526131}\n",
        "\n",
        "\n",
        "stats_dict_median ={'Median abs angle target last seen ': 0.8535855351984136,\n",
        " 'Median num stops': 3.0,\n",
        " 'Median num stops near target': 1.0,\n",
        " 'Median time': 2.25,\n",
        " 'Median time target last seen': 0.25}"
      ],
      "metadata": {
        "id": "nlILoeXA1asI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats_dict2 = {'Give up after trying': 0.21879815100154082,\n",
        " 'Ignore sudden flash': 0.04776579352850539,\n",
        " 'Stop success rate': 0.3504043126684636,\n",
        " 'Target disappears latest': 0.6579352850539292,\n",
        " 'Try a few times': 0.1386748844375963,\n",
        " 'Two in a row': 0.1712962962962963,\n",
        " 'Visible before last capture': 0.13117283950617284,\n",
        " 'Waste cluster around last target': 0.09259259259259259,\n",
        " 'ff capture rate': 0.34771700953336676}\n",
        "\n",
        "\n",
        "stats_dict_median2 ={'Median abs angle target last seen ': 0.9426093204677228,\n",
        " 'Median num stops': 3.0,\n",
        " 'Median num stops near target': 1.0,\n",
        " 'Median time': 2.0,\n",
        " 'Median time target last seen': 0.25}"
      ],
      "metadata": {
        "id": "okuqVk11a-r2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIjk7sm83QLZ"
      },
      "source": [
        "## monkey"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmkhMJdKjiu_"
      },
      "outputs": [],
      "source": [
        "stats_dict_m = {'Give up after trying': 0.1263482280431433,\n",
        " 'Ignore sudden flash': 0.1325115562403698,\n",
        " 'Stop success rate': 0.33095723014256617,\n",
        " 'Target disappears latest': 0.17873651771956856,\n",
        " 'Try a few times': 0.3728813559322034,\n",
        " 'Two in a row': 0.09876543209876543,\n",
        " 'Visible before last capture': 0.13580246913580246,\n",
        " 'Waste cluster around last target': 0.12037037037037036,\n",
        " 'ff capture rate': 0.35397913078919646}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPPbTk1qY3AU"
      },
      "outputs": [],
      "source": [
        "stats_dict_all_m = {'Give up after trying': 0.1263482280431433,\n",
        " 'Ignore sudden flash': 0.1325115562403698,\n",
        " 'Median abs angle target last seen ': 0.6661714343096776,\n",
        " 'Median num stops': 3.0,\n",
        " 'Median num stops near target': 1.0,\n",
        " 'Median time': 2.4152945000000727,\n",
        " 'Median time target last seen': 0.7636549999999716,\n",
        " 'Stop success rate': 0.2128356254092993,\n",
        " 'Target disappears latest': 0.17873651771956856,\n",
        " 'Try a few times': 0.3728813559322034,\n",
        " 'Two in a row': 0.09876543209876543,\n",
        " 'Visible before last capture': 0.13580246913580246,\n",
        " 'Waste cluster around last target': 0.12037037037037036,\n",
        " 'ff capture rate': 0.35397913078919646}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9arH59DGXvYs"
      },
      "outputs": [],
      "source": [
        "stats_dict_median_m = {'Median abs angle target last seen ': 0.6661714343096776,\n",
        " 'Median num stops': 3.0,\n",
        " 'Median num stops near target': 1.0,\n",
        " 'Median time': 2.4152945000000727,\n",
        " 'Median time target last seen': 0.7636549999999716}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NfeNc-13Tf5"
      },
      "source": [
        "## big graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### compare 2"
      ],
      "metadata": {
        "id": "jXubBjXou24e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVJ7xeNFYAAA"
      },
      "outputs": [],
      "source": [
        "#filename = f\"noise = {noise}\"\n",
        "\n",
        "agent_stats_df = pd.DataFrame.from_dict(stats_dict, orient='index', columns=['Value'])\n",
        "agent_stats_df['Player'] = \"Agent\"\n",
        "monkey_stats_df = pd.DataFrame.from_dict(stats_dict_m, orient='index', columns=['Value'])\n",
        "monkey_stats_df['Player'] = 'Monkey'\n",
        "merged_stats_df = pd.concat([agent_stats_df, monkey_stats_df], axis=0)\n",
        "merged_stats_df = merged_stats_df.reset_index()\n",
        "merged_stats_df.rename(columns = {'index':'Category'}, inplace = True)\n",
        "\n",
        "\n",
        "# set plot style: grey grid in the background:\n",
        "#sns.set(style=\"darkgrid\")\n",
        "\n",
        "# load dataset\n",
        "tips = sns.load_dataset(\"tips\")\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "# grouped barplot\n",
        "ax = sns.barplot(x=\"Category\", y=\"Value\", hue=\"Player\", data=merged_stats_df, ci=None);\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "\n",
        "images_dir = '/content/gdrive/MyDrive/fireflies_analysis/modify_env'\n",
        "\n",
        "\n",
        "# CHECK_FOLDER = os.path.isdir(images_dir)\n",
        "# # If folder doesn't exist, then create it.\n",
        "# if not CHECK_FOLDER:\n",
        "#     os.makedirs(images_dir)\n",
        "# #os.makedirs(images_dir)\n",
        "# ax.set_title(title, fontsize = 22)\n",
        "# #plt.savefig(f\"{images_dir}/{filename}.png\")\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meJsHGBwj3k7"
      },
      "outputs": [],
      "source": [
        "#filename = f\"noise = {noise}\"\n",
        "\n",
        "agent_stats_df = pd.DataFrame.from_dict(stats_dict_median, orient='index', columns=['Value'])\n",
        "agent_stats_df['Player'] = \"Agent\"\n",
        "monkey_stats_df = pd.DataFrame.from_dict(stats_dict_median_m, orient='index', columns=['Value'])\n",
        "monkey_stats_df['Player'] = 'Monkey'\n",
        "merged_stats_df = pd.concat([agent_stats_df, monkey_stats_df], axis=0)\n",
        "merged_stats_df = merged_stats_df.reset_index()\n",
        "merged_stats_df.rename(columns = {'index':'Category'}, inplace = True)\n",
        "\n",
        "\n",
        "# set plot style: grey grid in the background:\n",
        "sns.set(style=\"darkgrid\")\n",
        "\n",
        "# load dataset\n",
        "tips = sns.load_dataset(\"tips\")\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "# grouped barplot\n",
        "ax = sns.barplot(x=\"Category\", y=\"Value\", hue=\"Player\", data=merged_stats_df, ci=None);\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "\n",
        "images_dir = '/content/gdrive/MyDrive/fireflies_analysis/modify_env'\n",
        "\n",
        "\n",
        "# CHECK_FOLDER = os.path.isdir(images_dir)\n",
        "# # If folder doesn't exist, then create it.\n",
        "# if not CHECK_FOLDER:\n",
        "#     os.makedirs(images_dir)\n",
        "# #os.makedirs(images_dir)\n",
        "# ax.set_title(title, fontsize = 22)\n",
        "# #plt.savefig(f\"{images_dir}/{filename}.png\")\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### compare 3"
      ],
      "metadata": {
        "id": "EP8oQdELvCuf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k706CZF0vJ8m"
      },
      "outputs": [],
      "source": [
        "#filename = f\"noise = {noise}\"\n",
        "\n",
        "agent_stats_df = pd.DataFrame.from_dict(stats_dict, orient='index', columns=['Value'])\n",
        "agent_stats_df['Player'] = \"Agent (noise factor = 1)\"\n",
        "agent_stats_df2 = pd.DataFrame.from_dict(stats_dict2, orient='index', columns=['Value'])\n",
        "agent_stats_df2['Player'] = \"Agent (noise factor = 2)\"\n",
        "monkey_stats_df = pd.DataFrame.from_dict(stats_dict_m, orient='index', columns=['Value'])\n",
        "monkey_stats_df['Player'] = 'Monkey'\n",
        "merged_stats_df = pd.concat([monkey_stats_df, agent_stats_df, agent_stats_df2], axis=0)\n",
        "merged_stats_df = merged_stats_df.reset_index()\n",
        "merged_stats_df.rename(columns = {'index':'Category'}, inplace = True)\n",
        "\n",
        "\n",
        "# set plot style: grey grid in the background:\n",
        "#sns.set(style=\"darkgrid\")\n",
        "\n",
        "# load dataset\n",
        "tips = sns.load_dataset(\"tips\")\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "# grouped barplot\n",
        "ax = sns.barplot(x=\"Category\", y=\"Value\", hue=\"Player\", data=merged_stats_df, ci=None);\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "\n",
        "images_dir = '/content/gdrive/MyDrive/fireflies_analysis/modify_env'\n",
        "\n",
        "\n",
        "# CHECK_FOLDER = os.path.isdir(images_dir)\n",
        "# # If folder doesn't exist, then create it.\n",
        "# if not CHECK_FOLDER:\n",
        "#     os.makedirs(images_dir)\n",
        "# #os.makedirs(images_dir)\n",
        "# ax.set_title(title, fontsize = 22)\n",
        "# #plt.savefig(f\"{images_dir}/{filename}.png\")\n",
        "plt.ylabel(\"Percentage of Trials\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NuxA1tAvJ8n"
      },
      "outputs": [],
      "source": [
        "#filename = f\"noise = {noise}\"\n",
        "\n",
        "agent_stats_df = pd.DataFrame.from_dict(stats_dict_median, orient='index', columns=['Value'])\n",
        "agent_stats_df['Player'] = \"Agent\"\n",
        "agent_stats_df2 = pd.DataFrame.from_dict(stats_dict_median2, orient='index', columns=['Value'])\n",
        "agent_stats_df2['Player'] = \"Agent2\"\n",
        "monkey_stats_df = pd.DataFrame.from_dict(stats_dict_median_m, orient='index', columns=['Value'])\n",
        "monkey_stats_df['Player'] = 'Monkey'\n",
        "merged_stats_df = pd.concat([agent_stats_df, agent_stats_df2, monkey_stats_df], axis=0)\n",
        "merged_stats_df = merged_stats_df.reset_index()\n",
        "merged_stats_df.rename(columns = {'index':'Category'}, inplace = True)\n",
        "\n",
        "\n",
        "# set plot style: grey grid in the background:\n",
        "sns.set(style=\"darkgrid\")\n",
        "\n",
        "# load dataset\n",
        "tips = sns.load_dataset(\"tips\")\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "# grouped barplot\n",
        "ax = sns.barplot(x=\"Category\", y=\"Value\", hue=\"Player\", data=merged_stats_df, ci=None);\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "\n",
        "images_dir = '/content/gdrive/MyDrive/fireflies_analysis/modify_env'\n",
        "\n",
        "\n",
        "# CHECK_FOLDER = os.path.isdir(images_dir)\n",
        "# # If folder doesn't exist, then create it.\n",
        "# if not CHECK_FOLDER:\n",
        "#     os.makedirs(images_dir)\n",
        "# #os.makedirs(images_dir)\n",
        "# ax.set_title(title, fontsize = 22)\n",
        "# #plt.savefig(f\"{images_dir}/{filename}.png\")\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCxqTg8dlplF"
      },
      "source": [
        "## graphs by categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6fsCeej5Oc5"
      },
      "source": [
        "### compare 2\n",
        "\n",
        "compare current agent and monkey"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGtsi8ff6OTX"
      },
      "outputs": [],
      "source": [
        "agent_stats_df = pd.DataFrame.from_dict(stats_dict, orient='index', columns=['Value'])\n",
        "agent_stats_df['Player'] = \"Agent\"\n",
        "monkey_stats_df = pd.DataFrame.from_dict(stats_dict_m, orient='index', columns=['Value'])\n",
        "monkey_stats_df['Player'] = 'Monkey'\n",
        "merged_stats_df = pd.concat([agent_stats_df, monkey_stats_df], axis=0)\n",
        "merged_stats_df = merged_stats_df.reset_index()\n",
        "merged_stats_df.rename(columns = {'index':'Category'}, inplace = True)\n",
        "merged_stats_df['Value%'] = merged_stats_df['Value']*100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tArpdgKwlxcN"
      },
      "outputs": [],
      "source": [
        "merged_stats_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26zGRp3jlplG"
      },
      "outputs": [],
      "source": [
        "for cate in merged_stats_df.Category:\n",
        "  print(cate)\n",
        "  category_df = merged_stats_df[merged_stats_df['Category']==cate]\n",
        "\n",
        "  # set plot style: grey grid in the background:\n",
        "  sns.set(style=\"darkgrid\")\n",
        "\n",
        "  # load dataset\n",
        "  tips = sns.load_dataset(\"tips\")\n",
        "\n",
        "  # Set the figure size\n",
        "  plt.figure(figsize=(4, 8))\n",
        "\n",
        "  # grouped barplot\n",
        "  ##ax = sns.barplot(x=\"Player\", y=\"Value\", hue=\"Player\", data=category_df, ci=None);\n",
        "  ax = sns.barplot(x=\"Player\", y=\"Value\", data=category_df, ci=None);\n",
        "  ##ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
        "  #ax.set_ylabel(\"Percentage of captured fireflies\", fontsize = 22)\n",
        "  ax.set_ylabel(\"\")\n",
        "  plt.xticks(fontsize= 22) \n",
        "  plt.yticks(fontsize= 15) \n",
        "  ##ax.set_xticklabels(\"\")\n",
        "  ax.set_xlabel(\"\")\n",
        "  ax.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
        "  plt.tight_layout()\n",
        "\n",
        "\n",
        "  plt.show()\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_vYdDVN66pR"
      },
      "source": [
        "### compare 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0jWFA-f5lDO"
      },
      "outputs": [],
      "source": [
        "# agent from env d22\n",
        "stats_dict_lstm={'Give up after trying': 0.040061633281972264,\n",
        " 'Ignore sudden flash': 0.007704160246533128,\n",
        " 'Stop success rate': 0.7072905331882481,\n",
        " 'Target disappears latest': 0.2711864406779661,\n",
        " 'Try a few times': 0.4869029275808937,\n",
        " 'Two in a row': 0.2052469135802469,\n",
        " 'Visible before last capture': 0.29012345679012347,\n",
        " 'Waste cluster around last target': 0.013888888888888888,\n",
        " 'ff capture rate': 0.48360450563204005}\n",
        "\n",
        "stats_dict_median_lstm={'Median abs angle target last seen ': 0.3873275738322851,\n",
        " 'Median num stops': 2.0,\n",
        " 'Median num stops near target': 1.0,\n",
        " 'Median time': 1.75,\n",
        " 'Median time target last seen': 1.5}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSAzKHdO6B7u"
      },
      "outputs": [],
      "source": [
        "# agent from env c12\n",
        "stats_dict_sb3 = {'Give up after trying': 0.07087827426810478,\n",
        " 'Ignore sudden flash': 0.02157164869029276,\n",
        " 'Stop success rate': 0.5191693290734825,\n",
        " 'Target disappears latest': 0.7673343605546995,\n",
        " 'Try a few times': 0.06933744221879815,\n",
        " 'Two in a row': 0.19290123456790123,\n",
        " 'Visible before last capture': 0.08024691358024691,\n",
        " 'Waste cluster around last target': 0.08796296296296297,\n",
        " 'ff capture rate': 0.4580152671755725}\n",
        " \n",
        "stats_dict_median_sb3 = {'Median abs angle target last seen ': 0.9224812934369755,\n",
        " 'Median num stops': 2.0,\n",
        " 'Median num stops near target': 1.0,\n",
        " 'Median time': 1.75,\n",
        " 'Median time target last seen': 0.0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS1sQ1_4lw2s"
      },
      "outputs": [],
      "source": [
        "agent_stats_df = pd.DataFrame.from_dict(stats_dict_sb3, orient='index', columns=['Value'])\n",
        "agent_stats_df['Player'] = \"Agent\"\n",
        "monkey_stats_df = pd.DataFrame.from_dict(stats_dict_m, orient='index', columns=['Value'])\n",
        "monkey_stats_df['Player'] = 'Monkey'\n",
        "lstm_stats_df = pd.DataFrame.from_dict(stats_dict_lstm, orient='index', columns=['Value'])\n",
        "lstm_stats_df['Player'] = 'Agent(LSTM)'\n",
        "merged_stats_df = pd.concat([monkey_stats_df, agent_stats_df, lstm_stats_df], axis=0)\n",
        "merged_stats_df = merged_stats_df.reset_index()\n",
        "merged_stats_df.rename(columns = {'index':'Category'}, inplace = True)\n",
        "merged_stats_df['Value%'] = merged_stats_df['Value']*100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAOATU3X73qz"
      },
      "outputs": [],
      "source": [
        "merged_stats_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LQ4TUnZ8Bpz"
      },
      "outputs": [],
      "source": [
        "for cate in merged_stats_df.Category:\n",
        "  print(cate)\n",
        "  category_df = merged_stats_df[merged_stats_df['Category']==cate]\n",
        "\n",
        "  # set plot style: grey grid in the background:\n",
        "  sns.set(style=\"darkgrid\")\n",
        "\n",
        "  # load dataset\n",
        "  tips = sns.load_dataset(\"tips\")\n",
        "\n",
        "  # Set the figure size\n",
        "  plt.figure(figsize=(6, 8))\n",
        "\n",
        "  # grouped barplot\n",
        "  ##ax = sns.barplot(x=\"Player\", y=\"Value\", hue=\"Player\", data=category_df, ci=None);\n",
        "  ax = sns.barplot(x=\"Player\", y=\"Value%\", data=category_df, ci=None);\n",
        "  ##ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
        "  #ax.set_ylabel(\"Percentage of captured fireflies\", fontsize = 22)\n",
        "  ax.set_ylabel(\"\")\n",
        "  plt.xticks(fontsize= 22\n",
        "             ) \n",
        "  plt.yticks(fontsize= 22) \n",
        "  ##ax.set_xticklabels(\"\")\n",
        "  ax.set_xlabel(\"\")\n",
        "  ax.yaxis.set_major_formatter(mtick.PercentFormatter(decimals=0))\n",
        "  plt.tight_layout()\n",
        "\n",
        "\n",
        "  plt.show()\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht4IuNGSFETE"
      },
      "outputs": [],
      "source": [
        "merged_stats_df.Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5-dtEevGFvz"
      },
      "outputs": [],
      "source": [
        "from matplotlib.ticker import FormatStrFormatter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VNsawYaFCTm"
      },
      "outputs": [],
      "source": [
        "\n",
        "cate = \"ff capture rate\"\n",
        "category_df = merged_stats_df[merged_stats_df['Category']==cate]\n",
        "\n",
        "# set plot style: grey grid in the background:\n",
        "sns.set(style=\"darkgrid\")\n",
        "\n",
        "# load dataset\n",
        "tips = sns.load_dataset(\"tips\")\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(6, 8))\n",
        "\n",
        "# grouped barplot\n",
        "##ax = sns.barplot(x=\"Player\", y=\"Value\", hue=\"Player\", data=category_df, ci=None);\n",
        "ax = sns.barplot(x=\"Player\", y=\"Value\", data=category_df, ci=None);\n",
        "##ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
        "#ax.set_ylabel(\"Percentage of captured fireflies\", fontsize = 22)\n",
        "ax.set_ylabel(\"\")\n",
        "plt.xticks(fontsize= 22\n",
        "            ) \n",
        "plt.yticks(fontsize= 22) \n",
        "\n",
        "current_values = plt.gca().get_yticks()\n",
        "# using format string '{:.0f}' here but you can choose others\n",
        "plt.gca().set_yticklabels(['{:.1f}/s'.format(x) for x in current_values])\n",
        "\n",
        "##ax.set_xticklabels(\"\")\n",
        "ax.set_xlabel(\"\")\n",
        "#ax.yaxis.set_major_formatter(mtick.PercentFormatter(decimals=0))\n",
        "plt.tight_layout()\n",
        "\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yPgy8WwizQ5"
      },
      "source": [
        "# histograms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overall"
      ],
      "metadata": {
        "id": "KUEpr7AzuMkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_trials"
      ],
      "metadata": {
        "id": "N6TCkkPE21xT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtQgy6_BraH_"
      },
      "outputs": [],
      "source": [
        "variable_of_interest_list = ['t', 't_last_visible', 'd_last_visible',\n",
        "    'abs_angle_last_visible', 'num_stops','num_stops_near_target']\n",
        "subplot_list = [(0,0), (0,1), (0,2), (1,0), (1,1), (1,2)]\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "for i in range(6):\n",
        "  variable_of_interest = variable_of_interest_list[i]\n",
        "  sns.histplot(ax=axes[subplot_list[i]], data = valid_trials[variable_of_interest], kde = True, alpha = 0.4, color = \"blue\")\n",
        "  sns.histplot(ax=axes[subplot_list[i]], data = valid_trials_m[variable_of_interest], kde = True, alpha = 0.4, color = \"orange\")\n",
        "  axes[subplot_list[i]].legend(labels=[\"Agent\",\"Monkey\"])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gGtLnbOqibD"
      },
      "source": [
        "Not in subplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Enp-JuWUqhy6"
      },
      "outputs": [],
      "source": [
        "variable_of_interest_list = ['t', 't_last_visible', 'd_last_visible',\n",
        "    'abs_angle_last_visible', 'num_stops','num_stops_near_target']\n",
        "\n",
        "for i in range(6):\n",
        "  variable_of_interest = variable_of_interest_list[i]\n",
        "  fig, axes = plt.subplots(figsize=(6, 5))\n",
        "  sns.histplot(data = valid_trials[variable_of_interest], kde = True, alpha = 0.4, color = \"blue\")\n",
        "  sns.histplot(data = valid_trials_m[variable_of_interest], kde = True, alpha = 0.4, color = \"orange\")\n",
        "  axes.legend(labels=[\"Agent\",\"Monkey\"])\n",
        "  axes.set_ylabel(\"\")\n",
        "  axes.set_yticklabels(\"\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WARUUIH4UcK4"
      },
      "source": [
        "## By category"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib as mpl\n",
        "mpl.rcParams.update(mpl.rcParamsDefault)\n",
        "mstyle.use('seaborn-white')"
      ],
      "metadata": {
        "id": "qNOIQPgRRIe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9A-7ReduXiOa"
      },
      "outputs": [],
      "source": [
        "valid_trials = valid_trials_lstm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nau0U81zsq8J"
      },
      "outputs": [],
      "source": [
        "variable_of_interest = \"t_last_visible\"\n",
        "fig, axes = plt.subplots(figsize=(4, 5))\n",
        "sns.histplot(data = valid_trials[variable_of_interest], kde = True, alpha = 0.4, color = \"green\", binwidth = 0.25)\n",
        "sns.histplot(data = valid_trials_m[variable_of_interest], kde = True, alpha = 0.4, color = \"blue\")\n",
        "axes.legend(labels=[\"Agent(LSTM)\",\"Monkey\"], fontsize = 14)\n",
        "#axes.set_ylabel(\"\")\n",
        "#axes.set_yticklabels(\"\")\n",
        "plt.xticks(fontsize = 18)\n",
        "plt.title(\"Time Since Target Last Visible\", fontsize=18)\n",
        "plt.xlabel(\"Time (s)\", fontsize = 18)\n",
        "plt.xlim([0, 6])\n",
        "\n",
        "#axes.xaxis.set_major_locator(ticker.NullLocator())\n",
        "axes.yaxis.set_major_locator(ticker.NullLocator())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6Ie_SwuCdTd"
      },
      "outputs": [],
      "source": [
        "variable_of_interest = \"num_stops_near_target\"\n",
        "fig, axes = plt.subplots(figsize=(4, 5))\n",
        "sns.histplot(data = valid_trials[variable_of_interest], kde = False, alpha = 0.4, color = \"green\", binwidth=0.5, stat=\"probability\")\n",
        "sns.histplot(data = valid_trials_m[variable_of_interest], kde = False, alpha = 0.4, color = \"blue\", binwidth=0.5, stat=\"probability\")\n",
        "axes.legend(labels=[\"Agent(LSTM)\",\"Monkey\"], fontsize = 14)\n",
        "sns.kdeplot(data = valid_trials, x = variable_of_interest, bw=1, color = \"green\")\n",
        "sns.kdeplot(data = valid_trials_m, x = variable_of_interest, bw=1, color = \"blue\")\n",
        "axes.set_ylabel(\"Probability\",fontsize=15)\n",
        "#axes.set_yticklabels(\"\")\n",
        "plt.xlim(0,5)\n",
        "plt.title(\"Number of Stops Near Targets\", fontsize=17)\n",
        "plt.xlabel(\"Number of Stops\", fontsize=15)\n",
        "axes.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "# axes.xaxis.set_major_locator(ticker.NullLocator())\n",
        "#axes.yaxis.set_major_locator(ticker.NullLocator())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgiIkYOSxaLe"
      },
      "outputs": [],
      "source": [
        "variable_of_interest = \"n_ff_in_a_row\"\n",
        "fig, axes = plt.subplots(figsize=(4, 5))\n",
        "sns.histplot(data = valid_trials[variable_of_interest], kde = False, alpha = 0.4, color = \"green\", binwidth=0.5, stat=\"probability\")\n",
        "sns.histplot(data = valid_trials_m[variable_of_interest], kde = False, alpha = 0.4, color = \"blue\", binwidth=0.5, stat=\"probability\")\n",
        "axes.legend(labels=[\"Agent(LSTM)\",\"Monkey\"], fontsize = 14)\n",
        "sns.kdeplot(data = valid_trials, x = variable_of_interest, bw=1, color = \"green\")\n",
        "sns.kdeplot(data = valid_trials_m, x = variable_of_interest, bw=1, color = \"blue\")\n",
        "axes.set_ylabel(\"Probability\",fontsize=15)\n",
        "#axes.set_yticklabels(\"\")\n",
        "plt.xlim(1,5)\n",
        "plt.title(\"Number of fireflies caught in a cluster\", fontsize=17)\n",
        "plt.xlabel(\"Number of Fireflies\", fontsize=15)\n",
        "axes.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "# axes.xaxis.set_major_locator(ticker.NullLocator())\n",
        "#axes.yaxis.set_major_locator(ticker.NullLocator())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDl8ZXjAu-BD"
      },
      "outputs": [],
      "source": [
        "variable_of_interest = \"d_last_visible\"\n",
        "fig, axes = plt.subplots(figsize=(4, 5))\n",
        "sns.histplot(data = valid_trials[variable_of_interest], kde = True, alpha = 0.3, binwidth=40, binrange=(0,400), color = \"green\", stat=\"probability\")\n",
        "sns.histplot(data = valid_trials_m[variable_of_interest], kde = True, alpha = 0.3, binwidth=40, binrange=(0,400), color = \"blue\", stat=\"probability\")\n",
        "axes.legend(labels=[\"Agent(LSTM)\",\"Monkey\"], fontsize = 14)\n",
        "# sns.kdeplot(data = valid_trials, x = variable_of_interest, color = \"green\", bw=1)\n",
        "# sns.kdeplot(data = valid_trials_m, x = variable_of_interest, color = \"blue\", bw=1)\n",
        "axes.set_ylabel(\"Probability\",fontsize=15)\n",
        "#axes.set_yticklabels(\"\")\n",
        "plt.xlim(0, 400)\n",
        "plt.title(\"Distance of Target Last Visible\", fontsize=17)\n",
        "plt.xlabel(\"Distance (cm)\", fontsize=15)\n",
        "axes.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "# axes.xaxis.set_major_locator(ticker.NullLocator())\n",
        "#axes.yaxis.set_major_locator(ticker.NullLocator())\n",
        "\n",
        "\n",
        "xticklabels=axes.get_xticks().tolist()\n",
        "xticklabels = [str(int(label)) for label in xticklabels]\n",
        "xticklabels[-1]='400+'\n",
        "axes.set_xticklabels(xticklabels)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQj7Kl_xzmKQ"
      },
      "outputs": [],
      "source": [
        "variable_of_interest = \"abs_angle_last_visible\"\n",
        "fig, axes = plt.subplots(figsize=(4, 5))\n",
        "sns.histplot(data = valid_trials[variable_of_interest], kde = True, binwidth=0.1, binrange=(0,0.8), alpha = 0.3, color = \"green\", stat=\"probability\")\n",
        "sns.histplot(data = valid_trials_m[variable_of_interest], kde = True, binwidth=0.1, binrange=(0,0.8), alpha = 0.3, color = \"blue\", stat=\"probability\")\n",
        "axes.legend(labels=[\"Agent(LSTM)\",\"Monkey\"], fontsize = 14)\n",
        "# sns.kdeplot(data = valid_trials, x = variable_of_interest, color = \"green\", bw=1)\n",
        "# sns.kdeplot(data = valid_trials_m, x = variable_of_interest, color = \"blue\", bw=1)\n",
        "axes.set_ylabel(\"Probability\",fontsize=15)\n",
        "#axes.set_yticklabels(\"\")\n",
        "plt.title(\"Abs Angle of Target Last Visible\", fontsize=17)\n",
        "plt.xlabel(\"Angle (rad)\", fontsize=15)\n",
        "axes.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "# axes.xaxis.set_major_locator(ticker.NullLocator())\n",
        "#axes.yaxis.set_major_locator(ticker.NullLocator())\n",
        "\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIs5bah51wUu"
      },
      "outputs": [],
      "source": [
        "variable_of_interest = \"t\"\n",
        "fig, axes = plt.subplots(figsize=(4, 5))\n",
        "sns.histplot(data = valid_trials[variable_of_interest], kde = True, binwidth=1, binrange=(0,20), alpha = 0.3, color = \"green\", stat=\"probability\")\n",
        "sns.histplot(data = valid_trials_m[variable_of_interest], kde = True, binwidth=1, binrange=(0,20), alpha = 0.3, color = \"blue\", stat=\"probability\")\n",
        "axes.legend(labels=[\"Agent(LSTM)\",\"Monkey\"], fontsize = 14)\n",
        "# sns.kdeplot(data = valid_trials, x = variable_of_interest, color = \"green\", bw=1)\n",
        "# sns.kdeplot(data = valid_trials_m, x = variable_of_interest, color = \"blue\", bw=1)\n",
        "axes.set_ylabel(\"Probability\",fontsize=15)\n",
        "#axes.set_yticklabels(\"\")\n",
        "plt.title(\"Trial Duration\", fontsize=17)\n",
        "plt.xlabel(\"Duration (s)\", fontsize=15)\n",
        "axes.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "# axes.xaxis.set_major_locator(ticker.NullLocator())\n",
        "#axes.yaxis.set_major_locator(ticker.NullLocator())\n",
        "\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_trials[\"num_stops\"].loc[np.where(np.array(valid_trials[\"num_stops\"]==0))] = 1\n",
        "valid_trials_m[\"num_stops\"].loc[np.where(np.array(valid_trials_m[\"num_stops\"]==0))] = 1"
      ],
      "metadata": {
        "id": "GmT7jn4s4WJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcOvAcb33A5C"
      },
      "outputs": [],
      "source": [
        "variable_of_interest = \"num_stops\"\n",
        "fig, axes = plt.subplots(figsize=(4, 5))\n",
        "sns.histplot(data = valid_trials[variable_of_interest], binwidth=1, binrange=(1,12), alpha = 0.3, color = \"green\", stat=\"probability\")\n",
        "sns.histplot(data = valid_trials_m[variable_of_interest], binwidth=1, alpha = 0.3, binrange=(1,12), color = \"blue\", stat=\"probability\")\n",
        "axes.legend(labels=[\"Agent(LSTM)\",\"Monkey\"], fontsize = 14)\n",
        "sns.kdeplot(data = valid_trials, x = variable_of_interest,  bw=2, color = \"green\")\n",
        "sns.kdeplot(data = valid_trials_m, x = variable_of_interest, bw=2, color = \"blue\")\n",
        "axes.set_ylabel(\"Probability\",fontsize=15)\n",
        "plt.xlim(0,12)\n",
        "#axes.set_yticklabels(\"\")\n",
        "plt.title(\"Number of Stops During Trials\", fontsize=17)\n",
        "plt.xlabel(\"Number of Stops\", fontsize=15)\n",
        "axes.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "# axes.xaxis.set_major_locator(ticker.NullLocator())\n",
        "#axes.yaxis.set_major_locator(ticker.NullLocator())\n",
        "\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0S-lODkEpNN"
      },
      "source": [
        "# Monkey animation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRaHsCAjFz8k"
      },
      "source": [
        "## Animation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWh985ooF82h"
      },
      "outputs": [],
      "source": [
        "# New version\n",
        "currentTrial = 500\n",
        "num_trials = 7\n",
        "print(f\"Trials {currentTrial}-{currentTrial+num_trials-1}\")\n",
        "ff_position_during_this_trial = np.array([ff_real_position_sorted[i] for i,value in \n",
        "                                enumerate(ff_life_sorted) if (value[-1]>=ff_catched_T_sorted[currentTrial-1]) and (value[0]<=ff_catched_T_sorted[currentTrial+num_trials-1])])\n",
        "duration = [ff_catched_T_sorted[currentTrial-1], ff_catched_T_sorted[currentTrial+num_trials-1]]\n",
        "index_temp = np.where((monkey_information['monkey_t'] > ff_catched_T_sorted[currentTrial-1]) & \n",
        "                        (monkey_information['monkey_t'] <= ff_catched_T_sorted[currentTrial+num_trials-1]))\n",
        "if len(index_temp) > 0:\n",
        "  flash_index = flashing_ff(ff_flash_sorted, duration)\n",
        "  flash_fireflies = ff_real_position_sorted[flash_index]\n",
        "\n",
        "\n",
        "  cum_t = monkey_information['monkey_t'][index_temp]\n",
        "  cum_mx = monkey_information['monkey_x'][index_temp]\n",
        "  cum_my = monkey_information['monkey_y'][index_temp]\n",
        "  cum_believedffp = ff_believed_position_sorted[currentTrial: currentTrial+num_trials]\n",
        "  cum_angle = monkey_information['monkey_angle'][index_temp]\n",
        "\n",
        "# figure\n",
        "global fig; fig = plt.figure(dpi=100)\n",
        "plt.rcParams['figure.figsize'] = (10, 10)\n",
        "plt.rcParams['font.size'] = 15\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Animation\n",
        "filename = f\"Trial {currentTrial}\"\n",
        "k = 5\n",
        "anim_index = index_temp[0][0:-1:k].copy()\n",
        "anim_t = cum_t[0:-1:k].copy()\n",
        "anim_mx = cum_mx[0:-1:k].copy()\n",
        "anim_my = cum_my[0:-1:k].copy()\n",
        "anim_angle = cum_angle[0:-1:k].copy()\n",
        "flash_on_ff_dict = flash_on_ff(anim_t, currentTrial, num_trials, ff_flash_sorted)\n",
        "believed_ff_dict = believed_ff(anim_t, currentTrial, num_trials, ff_believed_position_sorted, ff_catched_T_sorted)\n",
        "circle_theta = np.arange(0, 2*pi, 0.01)\n",
        "circle_x = np.cos(circle_theta)*1000\n",
        "circle_y = np.sin(circle_theta)*1000\n",
        "ff_position_during_this_trial = np.array([ff_real_position_sorted[i] for i,value in \n",
        "                            enumerate(ff_life_sorted) if (value[-1]>=ff_catched_T_sorted[currentTrial-1]) and (value[0]<ff_catched_T_sorted[currentTrial+num_trials-1])])\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(ff_position_during_this_trial[:,0] , ff_position_during_this_trial[:,1] , alpha=0.9, c=\"gray\", s=20)\n",
        "\n",
        "# boundary\n",
        "margin = 400\n",
        "xmin, xmax = np.min(anim_mx), np.max(anim_mx)\n",
        "ymin, ymax = np.min(cum_my), np.max(cum_my)\n",
        "ax.set_xlim((xmin-margin, xmax+margin))\n",
        "ax.set_ylim((ymin-margin, ymax+margin))\n",
        "ax.set_aspect('equal')\n",
        "\n",
        "\n",
        "\n",
        "def animate(i):\n",
        "    ax.cla()\n",
        "    ax.axis('off')\n",
        "    ax.set_xlim((xmin-margin, xmax+margin))\n",
        "    ax.set_ylim((ymin-margin, ymax+margin))\n",
        "    ax.set_aspect('equal')\n",
        "    index = anim_index[i]\n",
        "    time = anim_t[i]\n",
        "    trial_num = np.where(ff_catched_T_sorted > time)[0][0]\n",
        "    flashing_on_ff = ff_real_position_sorted[flash_on_ff_dict[anim_t[i]]]\n",
        "    relevant_ff = ff_dataframe[['point_index', 'visible', 'ff_index', 'ff_x', 'ff_y']]\n",
        "    relevant_ff = relevant_ff[relevant_ff['point_index']==index]\n",
        "    in_memory_ffs = relevant_ff[relevant_ff['visible']==0]\n",
        "    visible_ffs = relevant_ff[relevant_ff['visible']==1]\n",
        "    ax.plot(circle_x, circle_y)\n",
        "\n",
        "    ax.scatter(ff_position_during_this_trial[:,0] , ff_position_during_this_trial[:,1] , alpha=0.7, c=\"gray\", s=20)\n",
        "    #ax.scatter(ff_real_position_sorted[trial_num][0], ff_real_position_sorted[trial_num][1], marker='*', c='blue', s = 130, alpha = 0.5)\n",
        "    ax.scatter(flashing_on_ff[:,0],flashing_on_ff[:,1] , alpha=1, c=\"red\", s=30)\n",
        "    #ax.scatter(in_memory_ffs.ff_x , in_memory_ffs.ff_y , alpha=1, c=\"green\", s=30)\n",
        "    ax.scatter(anim_mx[:i+1],anim_my[:i+1], s=15, c='royalblue')\n",
        "    \n",
        "\n",
        "    # for j in range(len(in_memory_ffs)):\n",
        "    #   circle = plt.Circle((in_memory_ffs.ff_x.iloc[j], in_memory_ffs.ff_y.iloc[j]), 25, facecolor='grey', edgecolor='orange', alpha=0.3, zorder=1)\n",
        "    #   ax.add_patch(circle)\n",
        "\n",
        "    for k in range(len(visible_ffs)):\n",
        "      circle = plt.Circle((visible_ffs.ff_x.iloc[k], visible_ffs.ff_y.iloc[k]), 25, facecolor='yellow', edgecolor='gray', alpha=0.5, zorder=1)\n",
        "      ax.add_patch(circle)\n",
        "      \n",
        "    if len(believed_ff_dict[i]) > 0:\n",
        "      for z in believed_ff_dict[i]:\n",
        "        ax.scatter(z[0],z[1], color=\"purple\", s=30)\n",
        "\n",
        "    ax.plot(np.array([anim_mx[i], anim_mx[i] + 30*np.cos(anim_angle[i]+2*pi/9)]), np.array([anim_my[i] , anim_my[i] + 30*np.sin(anim_angle[i]+2*pi/9)]), linewidth = 2)\n",
        "    ax.plot(np.array([anim_mx[i], anim_mx[i] + 30*np.cos(anim_angle[i]-2*pi/9)]), np.array([anim_my[i] , anim_my[i] + 30*np.sin(anim_angle[i]-2*pi/9)]), linewidth = 2)\n",
        "  \n",
        "\n",
        "\n",
        "num_frame = anim_t.size\n",
        "anim = animation.FuncAnimation(fig, animate, \n",
        "                frames=num_frame, interval=100, repeat=True) \n",
        "filename = \"0219_\" + str(currentTrial) + \"to\" + str(currentTrial+num_trials) \n",
        "gif_dir = '/content/gdrive/MyDrive/fireflies_anim/monkey'\n",
        "os.makedirs(gif_dir, exist_ok=True)\n",
        "writervideo = animation.FFMpegWriter(fps=4*3) # use fps = 208.3 if u want the time to be realistic\n",
        "anim.save(f\"{gif_dir}/{filename}.mp4\", writer=writervideo)\n",
        "#anim.save(f\"{gif_dir}/{filename}.gif\", writer='pillow', fps=60)\n",
        "anim\n",
        "HTML(anim.to_html5_video())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJpVizSFFw8p"
      },
      "source": [
        "## Animation with anotation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Organize data"
      ],
      "metadata": {
        "id": "2MjX2JicsKeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(give_up_after_trying_dummy)"
      ],
      "metadata": {
        "id": "8Ldw1hUh4hhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "give_up_after_trying_indices[-1]"
      ],
      "metadata": {
        "id": "-XhgVucW4nhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max(sudden_flash_ignore_points)"
      ],
      "metadata": {
        "id": "7_iBuZZ-8G3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sudden_flash_ignore_dummy)"
      ],
      "metadata": {
        "id": "BhPY1Moh8KRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnJSkeZgn6kQ"
      },
      "outputs": [],
      "source": [
        "sudden_flash_ignore_points = np.array(sudden_flash_ignore_points)[np.where(sudden_flash_ignore_points <= max_point_index)]\n",
        "\n",
        "\n",
        "give_up_after_trying_dummy = np.zeros(max_point_index+1, dtype=int)\n",
        "give_up_after_trying_dummy[give_up_after_trying_indices] = 1\n",
        "try_a_few_times_dummy = np.zeros(max_point_index+1, dtype=int)\n",
        "try_a_few_times_dummy[try_a_few_times_indices] = 1\n",
        "sudden_flash_ignore_dummy = np.zeros(max_point_index+1, dtype=int)\n",
        "sudden_flash_ignore_dummy[sudden_flash_ignore_points] = 1\n",
        "\n",
        "\n",
        "\n",
        "zero_array = np.zeros(catched_ff_num, dtype=int)\n",
        "\n",
        "multiple_in_a_row = np.where(n_ff_in_a_row>=2)[0]\n",
        "multiple_in_a_row_all = np.union1d(multiple_in_a_row, multiple_in_a_row-1)\n",
        "multiple_in_a_row2 = zero_array.copy()\n",
        "multiple_in_a_row_all2 = zero_array.copy()\n",
        "multiple_in_a_row2[multiple_in_a_row] = 1\n",
        "multiple_in_a_row_all2[multiple_in_a_row_all] = 1\n",
        "\n",
        "two_in_a_row2 = zero_array.copy()\n",
        "two_in_a_row2[two_in_a_row] = 1 \n",
        "\n",
        "three_in_a_row2 = zero_array.copy()\n",
        "three_in_a_row= np.where(n_ff_in_a_row==3)[0]\n",
        "three_in_a_row2[three_in_a_row] = 1\n",
        "\n",
        "four_in_a_row2 = zero_array.copy()\n",
        "four_in_a_row= np.where(n_ff_in_a_row==4)[0]\n",
        "four_in_a_row2[four_in_a_row] = 1\n",
        "\n",
        "none_in_a_row = np.where(n_ff_in_a_row<2)[0]\n",
        "none_in_a_row2 = zero_array.copy()\n",
        "none_in_a_row2[none_in_a_row] = 1\n",
        "\n",
        "visible_before_last_one2 = zero_array.copy()\n",
        "visible_before_last_one2[visible_before_last_one_trials] = 1\n",
        "\n",
        "disappear_latest2 = zero_array.copy()\n",
        "disappear_latest2[disappear_latest_trials] = 1 \n",
        "\n",
        "sudden_flash_ignore2 = zero_array.copy()\n",
        "sudden_flash_ignore2[sudden_flash_ignore_trials] = 1\n",
        "\n",
        "try_a_few_times2 = zero_array.copy()\n",
        "try_a_few_times2[try_a_few_times_trials] = 1\n",
        "\n",
        "give_up_after_trying2 = zero_array.copy()\n",
        "give_up_after_trying2[give_up_after_trying_trials] = 1\n",
        "\n",
        "# cluster_exists2 = zero_array.copy()\n",
        "# cluster_exists2[cluster_exist_trials] = 1 \n",
        "  \n",
        "\n",
        "# cluster_around_target2 = zero_array.copy()\n",
        "# cluster_around_target2[ffs_around_target_trials] = 1 \n",
        "\n",
        "# waste_cluster2 = zero_array.copy()\n",
        "# waste_cluster2[waste_cluster_trials] = 1\n",
        "\n",
        "waste_cluster_last_target2 = zero_array.copy()\n",
        "waste_cluster_last_target2[waste_cluster_last_target_trials] = 1\n",
        "\n",
        "# cluster_max_ff2 = zero_array.copy()\n",
        "# cluster_max_ff2[np.array(cluster_dataframe_trial.index)] = np.array(cluster_dataframe_trial.max_ff_in_cluster)\n",
        "\n",
        "# time_interval = monkey_information['monkey_t'][100] - monkey_information['monkey_t'][99]\n",
        "# cluster_exists_duration2 = np.zeros(catched_ff_num)\n",
        "# cluster_exists_duration2[np.array(cluster_dataframe_trial.index)] = np.array(cluster_dataframe_trial.num_points_w_cluster)*time_interval\n",
        "\n",
        "ffs_around_target2 = ffs_around_target\n",
        "\n",
        "\n",
        "anno_trial_dict = {\n",
        "# bool\n",
        "'two_in_a_row': two_in_a_row2,\n",
        "'three_in_a_row': three_in_a_row2,\n",
        "'four_in_a_row': four_in_a_row2,\n",
        "'none_in_a_row': none_in_a_row2,\n",
        "'multiple_in_a_row': multiple_in_a_row2,\n",
        "'multiple_in_a_row_all': multiple_in_a_row_all2,\n",
        "'visible_before_last_one': visible_before_last_one2,\n",
        "'disappear_latest': disappear_latest2,\n",
        "'sudden_flash_ignore': sudden_flash_ignore2,\n",
        "'try_a_few_times': try_a_few_times2,\n",
        "'give_up_after_trying': give_up_after_trying2,\n",
        "# 'cluster_exists': cluster_exists2,\n",
        "# 'cluster_around_target': cluster_around_target2,\n",
        "# 'waste_cluster': waste_cluster2,\n",
        "'waste_cluster_last_target': waste_cluster_last_target2,\n",
        "## num\n",
        "# 'cluster_max_ff' : cluster_max_ff2,  # The maximum number of ffs in a cluster during the trial\n",
        "# 'cluster_exists_duration': cluster_exists_duration2,\n",
        "# 'ffs_around_target': ffs_around_target2\n",
        "                                  }\n",
        "anno_trial = pd.DataFrame(anno_trial_dict) \n",
        "\n",
        "anno_trial[:3]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Animation"
      ],
      "metadata": {
        "id": "ReyQrZ4osDmK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7pr05IruG1z"
      },
      "outputs": [],
      "source": [
        "# New version\n",
        "currentTrial = 360\n",
        "num_trials = 10\n",
        "print(f\"Trials {currentTrial}-{currentTrial+num_trials-1}\")\n",
        "ff_position_during_this_trial = np.array([ff_real_position_sorted[i] for i,value in \n",
        "                                enumerate(ff_life_sorted) if (value[-1]>=ff_catched_T_sorted[currentTrial-1]) and (value[0]<=ff_catched_T_sorted[currentTrial+num_trials-1])])\n",
        "duration = [ff_catched_T_sorted[currentTrial-1], ff_catched_T_sorted[currentTrial+num_trials-1]]\n",
        "index_temp = np.where((monkey_information['monkey_t'] > ff_catched_T_sorted[currentTrial-1]) & \n",
        "                        (monkey_information['monkey_t'] <= ff_catched_T_sorted[currentTrial+num_trials-1]))\n",
        "if len(index_temp) > 0:\n",
        "  flash_index = flashing_ff(ff_flash_sorted, duration)\n",
        "  flash_fireflies = ff_real_position_sorted[flash_index]\n",
        "\n",
        "\n",
        "  cum_t = monkey_information['monkey_t'][index_temp]\n",
        "  cum_mx = monkey_information['monkey_x'][index_temp]\n",
        "  cum_my = monkey_information['monkey_y'][index_temp]\n",
        "  cum_believedffp = ff_believed_position_sorted[currentTrial: currentTrial+num_trials]\n",
        "  cum_angle = monkey_information['monkey_angle'][index_temp]\n",
        "# figure\n",
        "global fig; fig = plt.figure(dpi=100)\n",
        "plt.rcParams['figure.figsize'] = (10, 10)\n",
        "plt.rcParams['font.size'] = 15\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Animation\n",
        "filename = f\"Trial {currentTrial}\"\n",
        "k = 15\n",
        "anim_index = index_temp[0][0:-1:k].copy()\n",
        "anim_t = cum_t[0:-1:k].copy()\n",
        "anim_mx = cum_mx[0:-1:k].copy()\n",
        "anim_my = cum_my[0:-1:k].copy()\n",
        "anim_angle = cum_angle[0:-1:k].copy()\n",
        "flash_on_ff_dict = flash_on_ff(anim_t, currentTrial, num_trials, ff_flash_sorted)\n",
        "believed_ff_dict = believed_ff(anim_t, currentTrial, num_trials, ff_believed_position_sorted, ff_catched_T_sorted)\n",
        "circle_theta = np.arange(0, 2*pi, 0.01)\n",
        "circle_x = np.cos(circle_theta)*1000\n",
        "circle_y = np.sin(circle_theta)*1000\n",
        "ff_position_during_this_trial = np.array([ff_real_position_sorted[i] for i,value in \n",
        "                            enumerate(ff_life_sorted) if (value[-1]>=ff_catched_T_sorted[currentTrial-1]) and (value[0]<ff_catched_T_sorted[currentTrial+num_trials-1])])\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(ff_position_during_this_trial[:,0] , ff_position_during_this_trial[:,1] , alpha=0.9, c=\"gray\", s=20)\n",
        "\n",
        "# boundary\n",
        "margin = 400\n",
        "xmin, xmax = np.min(anim_mx), np.max(anim_mx)\n",
        "ymin, ymax = np.min(cum_my), np.max(cum_my)\n",
        "ax.set_xlim((xmin-margin, xmax+margin))\n",
        "ax.set_ylim((ymin-margin, ymax+margin))\n",
        "ax.set_aspect('equal')\n",
        "\n",
        "\n",
        "\n",
        "def animate(i):\n",
        "    ax.cla()\n",
        "    ax.axis('off')\n",
        "    ax.set_xlim((xmin-margin, xmax+margin))\n",
        "    ax.set_ylim((ymin-margin, ymax+margin))\n",
        "    ax.set_aspect('equal')\n",
        "    index = anim_index[i]\n",
        "    time = anim_t[i]\n",
        "    trial_num = np.where(ff_catched_T_sorted > time)[0][0]\n",
        "    flashing_on_ff = ff_real_position_sorted[flash_on_ff_dict[anim_t[i]]]\n",
        "    relevant_ff = ff_dataframe[['point_index', 'visible', 'ff_index', 'ff_x', 'ff_y']]\n",
        "    relevant_ff = relevant_ff[relevant_ff['point_index']==index]\n",
        "    in_memory_ffs = relevant_ff[relevant_ff['visible']==0]\n",
        "    visible_ffs = relevant_ff[relevant_ff['visible']==1]\n",
        "    ax.plot(circle_x, circle_y)\n",
        "\n",
        "    ax.scatter(ff_position_during_this_trial[:,0] , ff_position_during_this_trial[:,1] , alpha=0.7, c=\"gray\", s=20)\n",
        "    #ax.scatter(ff_real_position_sorted[trial_num][0], ff_real_position_sorted[trial_num][1], marker='*', c='purple', s = 130, alpha = 0.5)\n",
        "    ax.scatter(flashing_on_ff[:,0],flashing_on_ff[:,1] , alpha=1, c=\"red\", s=30)\n",
        "    #ax.scatter(in_memory_ffs.ff_x , in_memory_ffs.ff_y , alpha=1, c=\"green\", s=30)\n",
        "    ax.scatter(anim_mx[:i+1],anim_my[:i+1], s=15, c='royalblue')\n",
        "    \n",
        "\n",
        "    # for j in range(len(in_memory_ffs)):\n",
        "    #   circle = plt.Circle((in_memory_ffs.ff_x.iloc[j], in_memory_ffs.ff_y.iloc[j]), 25, facecolor='grey', edgecolor='orange', alpha=0.3, zorder=1)\n",
        "    #   ax.add_patch(circle)\n",
        "\n",
        "    for k in range(len(visible_ffs)):\n",
        "      circle = plt.Circle((visible_ffs.ff_x.iloc[k], visible_ffs.ff_y.iloc[k]), 25, facecolor='yellow', edgecolor='gray', alpha=0.5, zorder=1)\n",
        "      ax.add_patch(circle)\n",
        "      \n",
        "    if len(believed_ff_dict[i]) > 0:\n",
        "      for z in believed_ff_dict[i]:\n",
        "        ax.scatter(z[0],z[1], color=\"purple\", s=30)\n",
        "    # Annotation\n",
        "    annotation = \"\"\n",
        "    # If the monkey has captured more than one 1 ff in a cluster\n",
        "    if n_ff_in_a_row[trial_num] > 1:\n",
        "      annotation = annotation + f\"Captured {n_ff_in_a_row[trial_num]} ffs in a cluster\\n\"\n",
        "    # If the target stops being on before the monkey captures the previous firefly\n",
        "    if visible_before_last_one2[trial_num] == 1:\n",
        "      annotation = annotation + \"Target visible before last captre\\n\"\n",
        "    # If the target disappears the latest among visible ffs\n",
        "    if disappear_latest2[trial_num] == 1:   \n",
        "      annotation = annotation + \"Target disappears latest\\n\"\n",
        "    # If the monkey ignored a closeby ff that suddenly became visible\n",
        "    if sudden_flash_ignore_dummy[index] > 0:\n",
        "      annotation = annotation + \"Ignored sudden flash\\n\"\n",
        "    # If the monkey uses a few tries to capture a firefly\n",
        "    if try_a_few_times_dummy[index] > 0:\n",
        "      annotation = annotation + \"Try a few times to catch ff\\n\"\n",
        "    # If during the trial, the monkey fails to capture a firefly with a few tries and moves on to capture another one \n",
        "    if give_up_after_trying_dummy[index] > 0:\n",
        "      annotation = annotation + \"Give up after trying\\n\"\n",
        "    # If there is at least one cluster of visible ffs\n",
        "    # if all_point_vs_num_ff[index] > 0: \n",
        "    #   annotation = annotation + \"Clusters: max\" + str(all_point_vs_num_ff[index]) + \" ffs\" \n",
        "      #annotation = annotation + f\"Clusters: {point_vs_cluster[index]}\\n\"\n",
        "    # # Check whether the target is the closest ff among all that are visible or in memory (i.e. visible within 1.6s)\n",
        "    # if target_closest[index] == 2:\n",
        "    #   annotation = annotation +  \"Target is the closest\\n\"\n",
        "    # elif target_closest[index] ==1:\n",
        "    #   annotation = annotation +  \"Target is not the closest\\n\"\n",
        "    # # Check whether the target has the smallest angle among all ffs that are visible or in memory (i.e. visible within 1.6s)\n",
        "    # if target_angle_smallest[index] == 2:\n",
        "    #   annotation = annotation + \"Target has the smallest angle\"\n",
        "    # elif target_angle_smallest[index] ==1:\n",
        "    #   annotation = annotation +  \"Target does not have the smallest angle\"\n",
        "    # else:\n",
        "    #   annotation = annotation +  \"Target not visible in 1.6s\"\n",
        "    ax.text(0.5, 1.04, annotation, horizontalalignment='left', verticalalignment='top', transform=ax.transAxes, fontsize=12, color=\"black\", bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "    ax.plot(np.array([anim_mx[i], anim_mx[i] + 30*np.cos(anim_angle[i]+2*pi/9)]), np.array([anim_my[i] , anim_my[i] + 30*np.sin(anim_angle[i]+2*pi/9)]), linewidth = 2)\n",
        "    ax.plot(np.array([anim_mx[i], anim_mx[i] + 30*np.cos(anim_angle[i]-2*pi/9)]), np.array([anim_my[i] , anim_my[i] + 30*np.sin(anim_angle[i]-2*pi/9)]), linewidth = 2)\n",
        "  \n",
        "\n",
        "num_frame = anim_t.size\n",
        "\n",
        "\n",
        "\n",
        "anim = animation.FuncAnimation(fig, animate, \n",
        "                frames=num_frame, interval=100, repeat=True) \n",
        "filename = \"0219_\" + str(currentTrial) + \"to\" + str(currentTrial+num_trials) \n",
        "gif_dir = '/content/gdrive/MyDrive/fireflies_anim/monkey_annotated'\n",
        "os.makedirs(gif_dir, exist_ok=True)\n",
        "writervideo = animation.FFMpegWriter(fps=4*3) # use fps = 208.3 if u want the time to be realistic\n",
        "anim.save(f\"{gif_dir}/{filename}.mp4\", writer=writervideo)\n",
        "#anim.save(f\"{gif_dir}/{filename}.gif\", writer='pillow', fps=60)\n",
        "HTML(anim.to_html5_video())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other graphs"
      ],
      "metadata": {
        "id": "zxPFkgbzRHa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.style as mstyle"
      ],
      "metadata": {
        "id": "u6gXjz3ZT8sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mstyle.available"
      ],
      "metadata": {
        "id": "T12IxZt1ULaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMqQIO3EUvKK"
      },
      "outputs": [],
      "source": [
        "category = two_in_a_row_non_simul\n",
        "images_dir = '/content/gdrive/MyDrive/fireflies_analysis/algo_selected_summer_2022_updated/two_in_a_row_non_simul'\n",
        "num_trials = 2\n",
        "Show_Colorbar = True\n",
        "trail_color = \"orange\"\n",
        "trial_to_show_cluster = None # None, 0, or -1\n",
        "\n",
        "if len(category) > 0:\n",
        "  with initiate_plot(10,10,100):\n",
        "    #for currentTrial in category[:trial_total_num]:\n",
        "    for currentTrial in [50]:\n",
        "\n",
        "        duration = [ff_catched_T_sorted[currentTrial-num_trials], ff_catched_T_sorted[currentTrial]]\n",
        "        ff_position_during_this_trial = np.array([ff_real_position_sorted[i] for i,value in \n",
        "                                          enumerate(ff_life_sorted) if (value[-1]>=duration[0]) and (value[0]<duration[1])])\n",
        "\n",
        "        cum_indices = np.where((monkey_information['monkey_t'] > duration[0]) & (monkey_information['monkey_t'] <= duration[1]))[0]\n",
        "        cum_t, cum_angle = monkey_information['monkey_t'][cum_indices], monkey_information['monkey_angle'][cum_indices]\n",
        "        cum_mx, cum_my = monkey_information['monkey_x'][cum_indices], monkey_information['monkey_y'][cum_indices]\n",
        "        cum_speed, cum_speeddummy = monkey_information['monkey_speed'][cum_indices], monkey_information['monkey_speeddummy'][cum_indices]\n",
        "        cum_speed, cum_speeddummy = monkey_information['monkey_speed'][cum_indices], monkey_information['monkey_speeddummy'][cum_indices]\n",
        "\n",
        "        # Eliminate trials where the monkey has gone across the edge or if the trial is too short\n",
        "        cum_r = LA.norm(np.stack((cum_mx, cum_my)), axis = 0)\n",
        "        if (np.any(cum_r > 949)) or (len(cum_t) < 5):\n",
        "          continue\n",
        "\n",
        "        # Find the angle from the origin to the target\n",
        "        theta = pi/2-np.arctan2(cum_my[-1]-cum_my[0], cum_mx[-1]-cum_mx[0])     \n",
        "        c, s = np.cos(theta), np.sin(theta)\n",
        "        # Rotation matrix\n",
        "        R = np.array(((c, -s), (s, c)))\n",
        "\n",
        "        fig = plt.figure()\n",
        "        axes = fig.add_subplot(111)\n",
        "        axes.set_aspect(1)\n",
        "        PlotTrials(currentTrial = currentTrial,\n",
        "                      num_trials = num_trials, \n",
        "                      trail_color = trail_color, # \"orange\" or \"viridis\" or None\n",
        "                      show_reward_boundary = True,\n",
        "                      show_stops = True,\n",
        "                      show_colorbar = False, \n",
        "                      show_believed_target_positions = True,\n",
        "                      #show_connect_path_target = np.arange(currentTrial-num_trials+1, currentTrial+1),\n",
        "                      #show_connect_path_pre_target = np.arange(currentTrial-num_trials+1, currentTrial+1),\n",
        "                      #show_connect_path_ff = np.arange(currentTrial-num_trials+1, currentTrial+1),\n",
        "                      #trial_to_show_cluster = -1, \n",
        "                      show_scale_bar = True,\n",
        "                      )\n",
        "        \n",
        "\n",
        "        if images_dir != None:\n",
        "          filename = \"data\" + str(data_num) + \"_trial_\" + str(currentTrial)\n",
        "          CHECK_FOLDER = os.path.isdir(images_dir)\n",
        "          if not CHECK_FOLDER:\n",
        "              os.makedirs(images_dir)\n",
        "          #axes.set_title(f\"Trial {currentTrial}\", fontsize = 22)\n",
        "          #plt.savefig(f\"{images_dir}/{filename}.png\")\n",
        "        plt.show()  \n",
        "        plt.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "multifirefly_stage8_forPoster&PPT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMHAIeNZqu7CN87dLYOGhKM",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}